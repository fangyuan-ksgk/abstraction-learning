{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $0^{th}$ level policy model $\\pi^{(0)}(s_{t}|s_{t^{(1)}}^{(1)} \\circ a_{t}^{(1)})$\n",
    "2. $1^{th}$ level policy model $\\pi^{(1)}()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import sandwich_embedding as se \n",
    "import torch \n",
    "from model import GPTConfig\n",
    "\n",
    "config = GPTConfig(vocab_size=50304, n_layer=12, n_head=6, n_embd=768, K=3, L=2, device=\"cpu\", _compile=False)\n",
    "B, S, D = 3, 6, config.n_embd \n",
    "K = config.K\n",
    "L = config.L\n",
    "\n",
    "# Sandwich embedding ensemble (temporal predicted token embeddings ensemble)\n",
    "\n",
    "token_embeddings = torch.randn(B, S, D)\n",
    "high_level_embeddings = torch.randn(B, S//K, D)\n",
    "low_level_embeddings = torch.randn(B, S*K, D)\n",
    "\n",
    "# (I). Embedding Ensemble\n",
    "# v1. pure additive ensemble across abstraction levels\n",
    "se(low_level_embeddings, token_embeddings, high_level_embeddings, K)\n",
    "\n",
    "# (II). Conditional GPT\n",
    "from model import CondGPT, GPTConfig\n",
    "\n",
    "condgpt = CondGPT(config)\n",
    "idx = torch.randint(0, 50304, (B, S))\n",
    "condgpt.forward(idx, high_level_embeddings, low_level_embeddings)\n",
    "condgpt.inference(idx, high_level_embeddings, low_level_embeddings)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = token_embeddings.shape[1]\n",
    "L2 = low_level_embeddings.shape[1]\n",
    "assert seq_len*K <= L2, f\"Planning without grounding is not allowed: {seq_len} > {L2}\"\n",
    "token_embeddings += low_level_embeddings[:, 0:seq_len*K:K]\n",
    "\n",
    "if high_level_embeddings is not None: \n",
    "    L1 = high_level_embeddings.shape[1]\n",
    "    assert L1 * K <= seq_len < (L1 + 1) * K, f\"Execution without purpose or planning without grounding is not allowed: {L1 * K} < {seq_len} <= {(L1 + 1) * K}\"\n",
    "    cond_embeddings = high_level_embeddings.repeat_interleave(K, dim=1)\n",
    "    token_embeddings[:, :L1 * K] += cond_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Better to put a $<begin>$ token at each abstract level, just to avoid forward propagation without token at abstract level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
