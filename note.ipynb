{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Organizing Reinforcement Learning\n",
    "1. A more exciting question is how can I build a GAT module for a Snake game? It'll be the natural test-bed for SoRL -- a snake with inner-monologue. \n",
    "2. Decision Abstractive Transformer (DAT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Episode 0: Score=0, Reward=-11.0\n",
      "Episode 1: Score=0, Reward=-10.9\n",
      "Episode 2: Score=0, Reward=-10.9\n",
      "Episode 3: Score=1, Reward=-0.7999999999999989\n",
      "Episode 4: Score=0, Reward=-13.200000000000001\n",
      "Sanity check passed: total 153 0-th level tokens (state & action)\n",
      "Sanity check passed: 79 state tokens in data, 79 state tokens in trajectories\n",
      "Sanity check passed: 74 action tokens in data, 74 action tokens in trajectories\n",
      "Sanity check passed: 183 (action/state/abstract) tokens in data\n"
     ]
    }
   ],
   "source": [
    "from snake import SnakeGameEngine, collect_trajectories, RandomAgent\n",
    "from utils import HierTraj, data_sanity_check\n",
    "from constant import PLACE_HOLDER_STATE_TOK, PLACE_HOLDER_ACTION_TOK\n",
    "\n",
    "env = SnakeGameEngine(width=10, height=10)\n",
    "\n",
    "# This is a make-shift data collection pipeline, the abstract tokens are complete false. \n",
    "# The proper way to collect data, per SoRL, is to 'simulate' multiple actions on the environment and store the HierTraj state directly\n",
    "# in that sense, the code snippet in the middle is not useful...\n",
    "\n",
    "# A way to use 'expert trajectory' is to take in the 'trajectories', and have the agent 'learn / explore' on what abstract tokens can be \n",
    "# used to explain the trajectory. This is very similar to the 'intuitive-physics' based 'learning by surprise' or 'active learning from obs' \n",
    "# idea. \n",
    "\n",
    "# For what is worth, the mid-snippet is useless (besides serving as a toy-case on which we test on SSL training of GAT)\n",
    "\n",
    "# Collect trajectories\n",
    "trajectories = collect_trajectories(env, RandomAgent(env), num_episodes=5, device=\"cpu\")\n",
    "\n",
    "samples = []\n",
    "for trajectory in trajectories: \n",
    "\n",
    "    n_state = trajectory[0].size(0)\n",
    "    placeholder_tokens = [PLACE_HOLDER_STATE_TOK if i % 2 == 0 else PLACE_HOLDER_ACTION_TOK for i in range(2*n_state-1)]\n",
    "    sample = ([placeholder_tokens, [3, 9, 4, 2], [19, 14]], None)\n",
    "    samples.append(sample)\n",
    "\n",
    "batch_data = HierTraj.from_hierarchical_data(samples, K=3, L=3)\n",
    "data_sanity_check(batch_data, trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DAT, DATConfig\n",
    "\n",
    "# DAT model \n",
    "\n",
    "config = DATConfig(\n",
    "    n_layer=4,\n",
    "    n_head=2,\n",
    "    n_embd=32,\n",
    "    K=2,\n",
    "    L=3,\n",
    "    vocab_size_list=[64, 32],\n",
    "    device=\"cpu\",\n",
    "    _compile=True,\n",
    ")\n",
    "\n",
    "# Snake specific encoder & decoder for state & action\n",
    "from snake import StateEncoder, StateDecoder, ActionEncoder, ActionDecoder\n",
    "\n",
    "state_encoder = StateEncoder(height=10, width=10, feature_dim=config.n_embd)\n",
    "state_decoder = StateDecoder(height=10, width=10, feature_dim=config.n_embd)\n",
    "action_encoder = ActionEncoder(action_size=4, feature_dim=config.n_embd)\n",
    "action_decoder = ActionDecoder(action_size=4, feature_dim=config.n_embd)\n",
    "\n",
    "dat = DAT(config, state_encoder, state_decoder, action_encoder, action_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss\n",
    "loss = dat(batch_data, trajectories)\n",
    "\n",
    "# generate & update \n",
    "new_batch_data, new_trajectories = dat.generate(batch_data, trajectories)\n",
    "\n",
    "# act: produce action tokens (if there already exists action-tokens un-grounded with reward, skip it)\n",
    "pairs = dat.act(batch_data, trajectories) # list of (sample_idx, action_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "Hierarchical Trajectory K=2 L=3 (aligned by timestamp):\n",
      "-------------------------------------------------------\n",
      "Level 2:              [0]     \n",
      "Level 1:      [0]     [0]     \n",
      "L0-State: [s] [s] [s] [s]     \n",
      "L0-Action:    [a] [a] [a] [a] \n",
      "=======================================================\n",
      "Total tokens: 11, Max timestamp: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate on 'order of generation'\n",
    "from utils import test_dat_gen_order\n",
    "\n",
    "# Sanity check-up function (order of generation)\n",
    "test_dat_gen_order(dat, env, L=3, K=2, n_gen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x2 visualization for first 4 samples\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "from vis import visualize_backtrack\n",
    "from utils import * \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create 2x2 subplot figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Backtrack Visualization for First 4 Samples', fontsize=16)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "# Process first 4 samples\n",
    "num_samples = min(4, batch_data.indices.max().item() + 1)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample_idx = i\n",
    "    \n",
    "    # Get sample-level perplexity data\n",
    "    per_sample_ppt, per_sample_timestamps, max_abs_ts = get_sample_level_ppl(batch_data, ppt, level=0)\n",
    "    \n",
    "    # Extract data for current sample\n",
    "    sample_timestamps = per_sample_timestamps[sample_idx][:max_abs_ts + batch_data.K]\n",
    "    sample_ppt = per_sample_ppt[sample_idx][:max_abs_ts + batch_data.K]\n",
    "    \n",
    "    # Select current subplot\n",
    "    ax = axes_flat[i]\n",
    "    \n",
    "    # Plot the data\n",
    "    ax.plot(sample_timestamps.cpu().numpy(), sample_ppt.cpu().numpy(), \n",
    "            'b-', linewidth=1.5, label='Perplexity')\n",
    "    \n",
    "    # Add threshold line\n",
    "    ax.axhline(y=buffer.ppl_thres, color='r', linestyle='--', \n",
    "               linewidth=1, label=f'Threshold ({buffer.ppl_thres:.2f})')\n",
    "    \n",
    "    # Mark critical timestamps\n",
    "    critical_mask = sample_ppt > buffer.ppl_thres\n",
    "    if critical_mask.any():\n",
    "        critical_ts = sample_timestamps[critical_mask]\n",
    "        critical_ppt = sample_ppt[critical_mask]\n",
    "        ax.scatter(critical_ts.cpu().numpy(), critical_ppt.cpu().numpy(), \n",
    "                  color='red', s=50, zorder=5, label='Critical Points')\n",
    "    \n",
    "    # Mark backtrack points (where cts changed for this sample)\n",
    "    if sample_idx < len(cts):\n",
    "        backtrack_ts = cts[sample_idx] + 1\n",
    "        if backtrack_ts > 0:\n",
    "            ax.axvline(x=backtrack_ts, color='green', linestyle=':', \n",
    "                      linewidth=1.5, label=f'Backtrack (t={backtrack_ts})')\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlabel('Timestamp', fontsize=10)\n",
    "    ax.set_ylabel('Perplexity', fontsize=10)\n",
    "    ax.set_title(f'Sample {sample_idx}', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=8, loc='upper right')\n",
    "    \n",
    "    # Set y-axis limits for better visualization\n",
    "    ax.set_ylim([0, max(10, sample_ppt.max().item() * 1.1)])\n",
    "\n",
    "# Hide unused subplots if less than 4 samples\n",
    "for i in range(num_samples, 4):\n",
    "    axes_flat[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Visualized {num_samples} samples in 2x2 grid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new helper function for cleaner multi-sample visualization\n",
    "from vis import visualize_multi_sample_backtrack\n",
    "\n",
    "# Create 2x2 grid visualization for first 4 samples\n",
    "fig, axes = visualize_multi_sample_backtrack(batch_data, ppt, cts, buffer, num_samples=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Does DAT dream of playing snake game?\n",
    "\n",
    "# from agent import HiearchicalAgent\n",
    "from snake import SnakeGameEngine\n",
    "from utils import draw_gif\n",
    "from agent import collect_dat_game_play_frames\n",
    "from agent import HiearchicalAgent\n",
    "\n",
    "# Environment\n",
    "env = SnakeGameEngine(width=10, height=10)\n",
    "\n",
    "# DAT Agent (Snake specific agent)\n",
    "# agent = HiearchicalAgent(dat, env.reset(), \"cpu\")\n",
    "\n",
    "# DAT plays the snake game\n",
    "# --------------------------------------------------------------------------------------------------------------------------\n",
    "epsilon = 0.9\n",
    "frames = collect_dat_game_play_frames(dat, env, epsilon=epsilon)\n",
    "draw_gif(frames, txt=f\"DAT (randomized) play Snake (epsilon={epsilon})\", path=\"./visual/dat_snake.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Algorithm\n",
    "\n",
    "- Learn to Explain (GAT)\n",
    "Trajectory only data --> Sample --> Train\n",
    "\n",
    "- Learn to Explore (DAT)\n",
    "No data --> Sample --> Extend & Environment Interaction --> Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{SoRL (GAT)}$\n",
    "1. Group advantage computation \n",
    "2. Surrogate loss computation\n",
    "\n",
    "The key for learning from experience is learning from failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating circular with n=2, n_context=3...\n",
      "Saved 100 sequences to dataset/nbody/2body_100.bin\n"
     ]
    }
   ],
   "source": [
    "from dataset.nbody import NBodyDataset \n",
    "\n",
    "dataset = NBodyDataset(\n",
    "    n_bodies=2,\n",
    "    patterns=['circular'],\n",
    "    T=20,\n",
    "    filepath='dataset/nbody/2body_100.bin',\n",
    "    num_data=100,\n",
    "    K=3, \n",
    "    L=2\n",
    ").build()\n",
    "\n",
    "# dataset.sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 1085763.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 sequences to dataset/multiplication/2K-123.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataset.arithmetic import ArithmeticDataset\n",
    "\n",
    "\n",
    "dataset = ArithmeticDataset(\n",
    "    min_digit=1,\n",
    "    max_digit=3,\n",
    "    L=2,\n",
    "    K=3,\n",
    "    num_data=2000,\n",
    "    filepath=\"dataset/multiplication/2K-123.bin\"\n",
    ").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn-to-explain (GAT) | one batch ver.\n",
    "\n",
    "# (1). Initialize Data Buffer with trajectory-only data \n",
    "\n",
    "from model import GATConfig, GAT\n",
    "from dataset.arithmetic import ArithmeticDataset\n",
    "\n",
    "from dataclasses import asdict\n",
    "from search import SORLConfig \n",
    "import wandb\n",
    "\n",
    "gat_config = GATConfig(K=3, L=2, n_embd=128, n_head=4, n_layer=4, device=\"cpu\", _compile=False,\n",
    "                       vocab_size_list=[17, 8])\n",
    "gat = GAT(gat_config)\n",
    "\n",
    "\n",
    "config = SORLConfig(gat_config=gat_config, \n",
    "           n_generations=4, temperature=1.0, num_iterations=2, \n",
    "           joint_steps=10, context_length=1024, learning_rate=1e-3,\n",
    "           dataset_name=\"100K-123\", \n",
    "           dataset_path=\"dataset/multiplication/100K-123.bin\",\n",
    "           id_validate_dataset_path=\"dataset/multiplication/2k-123.bin\",\n",
    "           ood_validate_dataset_path=\"dataset/multiplication/2k-123.bin\")\n",
    "\n",
    "# load dataset\n",
    "dataset = ArithmeticDataset.from_file(config.id_validate_dataset_path)\n",
    "# id_val_dataset = ArithmeticDataset.from_file(config.id_validate_dataset_path)\n",
    "\n",
    "\n",
    "# gat.load_checkpoint(\"experiment/nbody/SoRL-GRPO-per-token-alternate-nbody.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters matching module device on cpu\n"
     ]
    }
   ],
   "source": [
    "from gat import GAT, GATConfig \n",
    "from sanity import check_model_device\n",
    "from search import get_batch\n",
    "\n",
    "\n",
    "context_length = 24 \n",
    "n = 1\n",
    "device = \"cpu\"\n",
    "\n",
    "# initialize GAT model \n",
    "gat_config = GATConfig(K=3, L=2, n_embd=128, n_head=4, n_layer=4, device=device, _compile=False,\n",
    "                       vocab_size_list=[17, 8])\n",
    "gat = GAT(gat_config)\n",
    "\n",
    "# move to device\n",
    "gat.to(device)\n",
    "check_model_device(gat)\n",
    "\n",
    "# get batch data\n",
    "batch_data = get_batch(dataset.sequences, dataset.lengths, context_length // n, gat.L, gat.K, device=device)\n",
    "\n",
    "# model inference \n",
    "gat(batch_data)\n",
    "\n",
    "gat.generate(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  5,  3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.level_mask_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m---> 27\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, config\u001b[38;5;241m.\u001b[39mvocab_size_list[\u001b[38;5;241m0\u001b[39m], (seq_len_per_sample,))\n\u001b[1;32m     28\u001b[0m     levels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(tokens)\n\u001b[1;32m     29\u001b[0m     timestamps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(seq_len_per_sample)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# config = GATConfig(\n",
    "#     L=3,\n",
    "#     n_layer=6,\n",
    "#     n_head=4,\n",
    "#     n_embd=128,\n",
    "#     vocab_size_list=[128, 64, 32],\n",
    "#     device=device,\n",
    "#     _compile=False  # We will compile manually\n",
    "# )\n",
    "\n",
    "# # 2. Create two identical models\n",
    "# eager_model = GAT(config).to(device)\n",
    "# eager_model.eval()\n",
    "\n",
    "# # Use dynamic=False for best performance with fixed-shape inputs\n",
    "# compiled_model = torch.compile(GAT(config), dynamic=False).to(device)\n",
    "# compiled_model.eval()\n",
    "import torch \n",
    "from utils import HierSeq\n",
    "device = \"cpu\"\n",
    "# 3. Create a realistic, random batch of data\n",
    "# Let's create a batch with 8 samples, each having around 256 tokens\n",
    "batch_size = 8\n",
    "seq_len_per_sample = 256\n",
    "samples = []\n",
    "for _ in range(batch_size):\n",
    "    tokens = torch.randint(0, config.vocab_size_list[0], (seq_len_per_sample,))\n",
    "    levels = torch.zeros_like(tokens)\n",
    "    timestamps = torch.arange(seq_len_per_sample)\n",
    "    samples.append(\n",
    "        ([tokens.tolist()] + [[] for _ in range(config.L-1)], None)\n",
    "    )\n",
    "\n",
    "batch_data = HierSeq.from_hierarchical_data(samples, K=config.K, L=config.L, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing drop_traj_tokens ---\n",
      "Sequence after dropping all but last 3 trajectory tokens:\n",
      "Data: [[[7, 8, 9], [10, 11], [12]]]\n",
      "Timestamps: [[[7, 8, 9], [4, 8], [0]]]\n",
      "✅ Memory diminishing test passed!\n",
      "\n",
      "--- Testing add_rhythmic_placeholders ---\n",
      "Padded sequence (full):\n",
      "[[[1, 2, 3, 4, 5, 6, 7, 8, 9], [4, 8], []]]\n",
      "✅ Full padding test passed!\n",
      "\n",
      "Padded sequence (t_search=5):\n",
      "[[[1, 2, 3, 4, 5, 6, 7, 8, 9], [4], []]]\n",
      "✅ Partial padding test passed!\n",
      "\n",
      "--- Testing generate_rollout_data ---\n",
      "✅ Batch size is correct.\n",
      "✅ All placeholders were denoised.\n",
      "✅ Abstract tokens were added at the correct rhythmic intervals.\n",
      "✅ `generate_rollout_data` test passed!\n"
     ]
    }
   ],
   "source": [
    "from sanity import * \n",
    "\n",
    "test_drop_traj_tokens()\n",
    "test_add_rhythmic_placeholders()\n",
    "test_generate_rollout_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing generate_rollout_data ---\n"
     ]
    }
   ],
   "source": [
    "from gat import GATConfig, GAT\n",
    "from sorl import generate_rollout_data\n",
    "from search import repeat_hseq # Assuming repeat_hseq is in search.py\n",
    "from utils import HierSeq\n",
    "\n",
    "print(\"\\n--- Testing generate_rollout_data ---\")\n",
    "\n",
    "# 1. Setup a dummy model and data\n",
    "config = GATConfig(L=2, K=3, vocab_size_list=[10, 5], device='cpu')\n",
    "model = GAT(config)\n",
    "model.eval()\n",
    "\n",
    "h_data = [\n",
    "    ([1, 2, 3, 4, 5], []), # L0 tokens, no abstractions\n",
    "    ([1, 2, 3, 4, 5], [])\n",
    "]\n",
    "batch = HierSeq.from_hierarchical_data([(h_data[0], h_data[1])], K=3, L=2, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing add_spike_placeholders ---\n",
      "Processed batch with spike-based placeholders:\n",
      "Data (Tokens): [[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [5, 5, 5, 5, 5]]]\n",
      "Data (Timestamps): [[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [3, 3, 6, 6, 6]]]\n",
      "✅ Correct total number of tokens added (5).\n",
      "✅ Correct number of tokens at ts=3 (2).\n",
      "✅ Correct number of tokens at ts=6 (3).\n",
      "✅ `add_spike_placeholders` test passed!\n"
     ]
    }
   ],
   "source": [
    "from sanity import test_add_spike_placeholders\n",
    "\n",
    "# (TBD). \"insert_token\" by default 'overwrites' existing token, allowing one abstract token at each timestamp -- we need to change this\n",
    "test_add_spike_placeholders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sorl import add_spike_placeholders\n",
    "from gat import GAT, GATConfig\n",
    "from utils import HierSeq\n",
    "import torch \n",
    "\n",
    "# 1. Setup a dummy model and data\n",
    "config = GATConfig(L=2, K=4, vocab_size_list=[20, 5], device='cpu')\n",
    "model = GAT(config)\n",
    "model.eval()\n",
    "\n",
    "# 2. Create a batch with one long sample\n",
    "h_data = [\n",
    "    (list(range(10)), []), # L0 tokens\n",
    "    (list(range(1, 11)), [])  # L0 timestamps (1-indexed)\n",
    "]\n",
    "batch = HierSeq.from_hierarchical_data([(h_data[0], h_data[1])], K=4, L=2, device='cpu')\n",
    "\n",
    "# 3. Mock the model's forward pass to return a predictable ppt tensor.\n",
    "# The ppt tensor should have N-1 elements for N tokens, representing the\n",
    "# loss of predicting token i+1 given tokens 0...i.\n",
    "# We create a ppt that results in two clear \"spikes\" (dip then rise).\n",
    "mock_ppt = torch.tensor([5.0, 4.0, 8.0, 4.0, 3.0, 10.0, 5.0, 5.0, 5.0], device='cpu')\n",
    "\n",
    "original_forward = model.forward\n",
    "model.forward = lambda b: mock_ppt\n",
    "\n",
    "# 4. Call the function with a budget of 5 tokens for level 1\n",
    "budgets = torch.tensor([0, 5]) \n",
    "\n",
    "# processed_batch = add_spike_placeholders(model, batch.clone(), budgets)\n",
    "abstract_budgets = torch.tensor([0, 5])\n",
    "batch_data = batch.clone()\n",
    "gat = model\n",
    "from sorl import allocate_budget\n",
    "\n",
    "abstract_mask = (batch_data.levels > 0)\n",
    "assert not abstract_mask.any(), \"Cannot add placeholders; abstract tokens already exist.\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    ppt = gat(batch_data)\n",
    "\n",
    "ppt_increase = ppt[1:] - ppt[:-1]\n",
    "\n",
    "for add_level in range(1, batch_data.L):\n",
    "    abstract_budget = abstract_budgets[add_level]\n",
    "    if abstract_budget == 0:\n",
    "        continue\n",
    "\n",
    "    # increase in perplexity within same sample\n",
    "    spike_mask = ppt_increase > 0\n",
    "    same_sample_mask = (batch_data.sample_idx[1:-1] == batch_data.sample_idx[2:])\n",
    "    spike_mask &= same_sample_mask\n",
    "\n",
    "    spike_indices = torch.where(spike_mask)[0]\n",
    "    if spike_indices.numel() == 0:\n",
    "        continue\n",
    "        \n",
    "    spike_weights = ppt_increase[spike_mask]\n",
    "    token_counts = allocate_budget(spike_weights, abstract_budget)\n",
    "\n",
    "    non_zero_mask = token_counts > 0\n",
    "    final_counts = token_counts[non_zero_mask]\n",
    "    final_indices = spike_indices[non_zero_mask]\n",
    "\n",
    "    if final_counts.numel() == 0:\n",
    "        continue\n",
    "\n",
    "    add_timestamps = torch.repeat_interleave(batch_data.timestamps[final_indices + 1], final_counts)\n",
    "    add_sample_idx = torch.repeat_interleave(batch_data.sample_idx[final_indices + 1], final_counts)\n",
    "    mask_token = gat.level_mask_tokens[add_level]\n",
    "    \n",
    "    for idx, ts in zip(add_sample_idx, add_timestamps):\n",
    "        batch_data.insert_tokens(idx, mask_token, add_level, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample_idx\n",
    "# add_sample_idx\n",
    "batch_data.sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add timestamps:  tensor([3, 3, 3, 3, 3]) Add sample idx:  tensor([0, 0, 0, 0, 0]) level:  1 mask token:  tensor(5)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from utils import HierSeq\n",
    "from sorl import allocate_budget \n",
    "\n",
    "# A simpler approach is to just check for ppt increment\n",
    "gat = model\n",
    "ppt_increase = ppt[1:] - ppt[:-1]\n",
    "    \n",
    "for add_level in range(1, batch_data.L):\n",
    "    abstract_budget = abstract_budgets[add_level]\n",
    "    if abstract_budget == 0:\n",
    "        continue\n",
    "\n",
    "    # increase in perplexity within same sample\n",
    "    spike_mask = ppt_increase > 0\n",
    "    same_sample_mask = (batch_data.sample_idx[1:-1] == batch_data.sample_idx[2:])\n",
    "    spike_mask &= same_sample_mask\n",
    "\n",
    "    spike_indices = torch.where(spike_mask)[0]\n",
    "    if spike_indices.numel() == 0:\n",
    "        continue\n",
    "        \n",
    "    spike_weights = ppt_increase[spike_mask]\n",
    "    token_counts = allocate_budget(spike_weights, abstract_budget)\n",
    "\n",
    "    non_zero_mask = token_counts > 0\n",
    "    final_counts = token_counts[non_zero_mask]\n",
    "    final_indices = spike_indices[non_zero_mask]\n",
    "\n",
    "    if final_counts.numel() == 0:\n",
    "        continue\n",
    "\n",
    "    add_timestamps = torch.repeat_interleave(batch_data.timestamps[final_indices + 1], final_counts)\n",
    "    add_sample_idx = torch.repeat_interleave(batch_data.sample_idx[final_indices + 1], final_counts)\n",
    "    mask_token = gat.level_mask_tokens[add_level]\n",
    "\n",
    "    print(\"Add timestamps: \", add_timestamps, \"Add sample idx: \", add_sample_idx, \"level: \", add_level, \"mask token: \", mask_token)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_counts\n",
    "# token_counts\n",
    "spike_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8332, 1.8332, 4.8332, 2.8332])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same_sample_mask\n",
    "# spike_mask\n",
    "# drop_before_mask\n",
    "# increase_mask\n",
    "ppt\n",
    "\n",
    "# batch_data.timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HierSeq(tokens=tensor([1, 2, 3, 4, 5]), levels=tensor([0, 0, 0, 0, 0]), timestamps=tensor([1, 2, 3, 4, 5]), sample_idx=tensor([0, 0, 0, 0, 0]), batch_size=1, K=3, L=2, idx_map=None, device='cpu')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game-Plan: we set the base rythmic abstract placeholder, then use fixed budget to assign abstraction on places where perplexity is high\n",
    "\n",
    "for add_level in range(1, batch_data.L):\n",
    "\n",
    "    abstract_budget = abstract_budgets[add_level-1]\n",
    "    # previous decrease + current increase = \"spike\"\n",
    "    drop_before_mask = torch.cat([torch.tensor([1], device=ppt.device, dtype=torch.bool), ppt_increase[:-1] <= 0])\n",
    "    increase_mask = ppt_increase > 0\n",
    "    spike_mask = drop_before_mask & increase_mask\n",
    "\n",
    "    # (A). Just put abstraction into highest spikes position, no need to handle each sample separately right? simplification ~ \n",
    "    add_sample_idx = batch_data.sample_idx[2:][spike_mask][:abstract_budget]\n",
    "    add_timestamps = batch_data.timestamps[2:][spike_mask][:abstract_budget]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), tensor([4]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_sample_idx, add_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([0., 0., 0.]),\n",
       "indices=tensor([0, 1, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ppt_increase.sort()\n",
    "\n",
    "# Request function: \n",
    "# - when tie exists, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_batch.tokens.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1). Model decide what's the next abstraction\n",
    "# gat.generate(batch_data)\n",
    "import torch \n",
    "\n",
    "# (A). Decode decides \"abstraction level\"\n",
    "vocab_size = 25 \n",
    "logits = torch.randn(batch_data.tokens.size(0)-1, vocab_size)\n",
    "\n",
    "# batch_data\n",
    "self = gat\n",
    "_, last_indices = torch.unique_consecutive(batch_data.sample_idx, return_inverse=True)\n",
    "last_token_positions = torch.where(last_indices[:-1] != last_indices[1:])[0]\n",
    "last_token_positions = torch.cat([last_token_positions, torch.tensor([len(batch_data.sample_idx)-1], device=self.device)])\n",
    "\n",
    "last_hidden_states = x[0, last_token_positions, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 81)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token_positions.size(0), batch_data.indices.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1094"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data.tokens.size(0)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100, loss: 4.9127, abs_loss: 2.0794, ssl_loss: 2.8332\n",
      "Iteration 2/100, loss: 4.7362, abs_loss: 1.9370, ssl_loss: 2.7991\n",
      "Iteration 3/100, loss: 4.4892, abs_loss: 1.7341, ssl_loss: 2.7552\n",
      "Iteration 4/100, loss: 4.2146, abs_loss: 1.5094, ssl_loss: 2.7052\n",
      "Iteration 5/100, loss: 3.9717, abs_loss: 1.3026, ssl_loss: 2.6691\n",
      "Iteration 6/100, loss: 3.7408, abs_loss: 1.1191, ssl_loss: 2.6218\n",
      "Iteration 7/100, loss: 3.5362, abs_loss: 0.9555, ssl_loss: 2.5807\n",
      "Iteration 8/100, loss: 3.3784, abs_loss: 0.8085, ssl_loss: 2.5699\n",
      "Iteration 9/100, loss: 3.2184, abs_loss: 0.6780, ssl_loss: 2.5404\n",
      "Iteration 10/100, loss: 3.0805, abs_loss: 0.5644, ssl_loss: 2.5161\n",
      "Iteration 11/100, loss: 2.9727, abs_loss: 0.4672, ssl_loss: 2.5055\n",
      "Iteration 12/100, loss: 2.8609, abs_loss: 0.3852, ssl_loss: 2.4758\n",
      "Iteration 13/100, loss: 2.7815, abs_loss: 0.3173, ssl_loss: 2.4642\n",
      "Iteration 14/100, loss: 2.7241, abs_loss: 0.2617, ssl_loss: 2.4625\n",
      "Iteration 15/100, loss: 2.6535, abs_loss: 0.2164, ssl_loss: 2.4371\n",
      "Iteration 16/100, loss: 2.6034, abs_loss: 0.1799, ssl_loss: 2.4236\n",
      "Iteration 17/100, loss: 2.5715, abs_loss: 0.1504, ssl_loss: 2.4211\n",
      "Iteration 18/100, loss: 2.5845, abs_loss: 0.1267, ssl_loss: 2.4578\n",
      "Iteration 19/100, loss: 2.5641, abs_loss: 0.1076, ssl_loss: 2.4565\n",
      "Iteration 20/100, loss: 2.5013, abs_loss: 0.0923, ssl_loss: 2.4090\n",
      "Iteration 21/100, loss: 2.5002, abs_loss: 0.0800, ssl_loss: 2.4202\n",
      "Iteration 22/100, loss: 2.4817, abs_loss: 0.0702, ssl_loss: 2.4116\n",
      "Iteration 23/100, loss: 2.4443, abs_loss: 0.0625, ssl_loss: 2.3818\n",
      "Iteration 24/100, loss: 2.4127, abs_loss: 0.0571, ssl_loss: 2.3557\n",
      "Iteration 25/100, loss: 2.3909, abs_loss: 0.0548, ssl_loss: 2.3362\n",
      "Iteration 26/100, loss: 2.3698, abs_loss: 0.0580, ssl_loss: 2.3118\n",
      "Iteration 27/100, loss: 2.3406, abs_loss: 0.0682, ssl_loss: 2.2724\n",
      "Iteration 28/100, loss: 2.3350, abs_loss: 0.0759, ssl_loss: 2.2591\n",
      "Iteration 29/100, loss: 2.3110, abs_loss: 0.0749, ssl_loss: 2.2361\n",
      "Iteration 30/100, loss: 2.2971, abs_loss: 0.0671, ssl_loss: 2.2300\n",
      "Iteration 31/100, loss: 2.2696, abs_loss: 0.0614, ssl_loss: 2.2081\n",
      "Iteration 32/100, loss: 2.2340, abs_loss: 0.0585, ssl_loss: 2.1755\n",
      "Iteration 33/100, loss: 2.1928, abs_loss: 0.0580, ssl_loss: 2.1348\n",
      "Iteration 34/100, loss: 2.1545, abs_loss: 0.0579, ssl_loss: 2.0966\n",
      "Iteration 35/100, loss: 2.1315, abs_loss: 0.0521, ssl_loss: 2.0794\n",
      "Iteration 36/100, loss: 2.1179, abs_loss: 0.0470, ssl_loss: 2.0709\n",
      "Iteration 37/100, loss: 2.1086, abs_loss: 0.0387, ssl_loss: 2.0699\n",
      "Iteration 38/100, loss: 2.0646, abs_loss: 0.0330, ssl_loss: 2.0316\n",
      "Iteration 39/100, loss: 2.0454, abs_loss: 0.0351, ssl_loss: 2.0103\n",
      "Iteration 40/100, loss: 2.0231, abs_loss: 0.0392, ssl_loss: 1.9839\n",
      "Iteration 41/100, loss: 1.9984, abs_loss: 0.0415, ssl_loss: 1.9569\n",
      "Iteration 42/100, loss: 1.9670, abs_loss: 0.0415, ssl_loss: 1.9255\n",
      "Iteration 43/100, loss: 1.9091, abs_loss: 0.0348, ssl_loss: 1.8743\n",
      "Iteration 44/100, loss: 1.9043, abs_loss: 0.0348, ssl_loss: 1.8695\n",
      "Iteration 45/100, loss: 1.8752, abs_loss: 0.0376, ssl_loss: 1.8376\n",
      "Iteration 46/100, loss: 1.8536, abs_loss: 0.0394, ssl_loss: 1.8142\n",
      "Iteration 47/100, loss: 1.8112, abs_loss: 0.0357, ssl_loss: 1.7755\n",
      "Iteration 48/100, loss: 1.8255, abs_loss: 0.0337, ssl_loss: 1.7918\n",
      "Iteration 49/100, loss: 1.7972, abs_loss: 0.0298, ssl_loss: 1.7675\n",
      "Iteration 50/100, loss: 1.7900, abs_loss: 0.0246, ssl_loss: 1.7654\n",
      "Iteration 51/100, loss: 1.7748, abs_loss: 0.0204, ssl_loss: 1.7544\n",
      "Iteration 52/100, loss: 1.7417, abs_loss: 0.0211, ssl_loss: 1.7206\n",
      "Iteration 53/100, loss: 1.7457, abs_loss: 0.0234, ssl_loss: 1.7224\n",
      "Iteration 54/100, loss: 1.7216, abs_loss: 0.0219, ssl_loss: 1.6997\n",
      "Iteration 55/100, loss: 1.7277, abs_loss: 0.0199, ssl_loss: 1.7079\n",
      "Iteration 56/100, loss: 1.7184, abs_loss: 0.0228, ssl_loss: 1.6955\n",
      "Iteration 57/100, loss: 1.6990, abs_loss: 0.0195, ssl_loss: 1.6795\n",
      "Iteration 58/100, loss: 1.6785, abs_loss: 0.0156, ssl_loss: 1.6629\n",
      "Iteration 59/100, loss: 1.6704, abs_loss: 0.0141, ssl_loss: 1.6563\n",
      "Iteration 60/100, loss: 1.6587, abs_loss: 0.0138, ssl_loss: 1.6449\n",
      "Iteration 61/100, loss: 1.6630, abs_loss: 0.0148, ssl_loss: 1.6482\n",
      "Iteration 62/100, loss: 1.6686, abs_loss: 0.0155, ssl_loss: 1.6531\n",
      "Iteration 63/100, loss: 1.6346, abs_loss: 0.0158, ssl_loss: 1.6188\n",
      "Iteration 64/100, loss: 1.6457, abs_loss: 0.0138, ssl_loss: 1.6318\n",
      "Iteration 65/100, loss: 1.6293, abs_loss: 0.0131, ssl_loss: 1.6162\n",
      "Iteration 66/100, loss: 1.5948, abs_loss: 0.0114, ssl_loss: 1.5834\n",
      "Iteration 67/100, loss: 1.6046, abs_loss: 0.0102, ssl_loss: 1.5944\n",
      "Iteration 68/100, loss: 1.5687, abs_loss: 0.0100, ssl_loss: 1.5586\n",
      "Iteration 69/100, loss: 1.5944, abs_loss: 0.0107, ssl_loss: 1.5837\n",
      "Iteration 70/100, loss: 1.5895, abs_loss: 0.0114, ssl_loss: 1.5782\n",
      "Iteration 71/100, loss: 1.5754, abs_loss: 0.0119, ssl_loss: 1.5635\n",
      "Iteration 72/100, loss: 1.5757, abs_loss: 0.0116, ssl_loss: 1.5640\n",
      "Iteration 73/100, loss: 1.5537, abs_loss: 0.0101, ssl_loss: 1.5435\n",
      "Iteration 74/100, loss: 1.5492, abs_loss: 0.0109, ssl_loss: 1.5383\n",
      "Iteration 75/100, loss: 1.5599, abs_loss: 0.0116, ssl_loss: 1.5484\n",
      "Iteration 76/100, loss: 1.5571, abs_loss: 0.0126, ssl_loss: 1.5446\n",
      "Iteration 77/100, loss: 1.5394, abs_loss: 0.0131, ssl_loss: 1.5263\n",
      "Iteration 78/100, loss: 1.5390, abs_loss: 0.0121, ssl_loss: 1.5269\n",
      "Iteration 79/100, loss: 1.5456, abs_loss: 0.0104, ssl_loss: 1.5352\n",
      "Iteration 80/100, loss: 1.5116, abs_loss: 0.0097, ssl_loss: 1.5019\n",
      "Iteration 81/100, loss: 1.5463, abs_loss: 0.0092, ssl_loss: 1.5370\n",
      "Iteration 82/100, loss: 1.5069, abs_loss: 0.0087, ssl_loss: 1.4982\n",
      "Iteration 83/100, loss: 1.5160, abs_loss: 0.0092, ssl_loss: 1.5067\n",
      "Iteration 84/100, loss: 1.4968, abs_loss: 0.0086, ssl_loss: 1.4882\n",
      "Iteration 85/100, loss: 1.5397, abs_loss: 0.0079, ssl_loss: 1.5318\n",
      "Iteration 86/100, loss: 1.5272, abs_loss: 0.0076, ssl_loss: 1.5196\n",
      "Iteration 87/100, loss: 1.5037, abs_loss: 0.0073, ssl_loss: 1.4964\n",
      "Iteration 88/100, loss: 1.5219, abs_loss: 0.0069, ssl_loss: 1.5150\n",
      "Iteration 89/100, loss: 1.5130, abs_loss: 0.0067, ssl_loss: 1.5063\n",
      "Iteration 90/100, loss: 1.5185, abs_loss: 0.0068, ssl_loss: 1.5117\n",
      "Iteration 91/100, loss: 1.4859, abs_loss: 0.0068, ssl_loss: 1.4792\n",
      "Iteration 92/100, loss: 1.5157, abs_loss: 0.0069, ssl_loss: 1.5088\n",
      "Iteration 93/100, loss: 1.5358, abs_loss: 0.0065, ssl_loss: 1.5293\n",
      "Iteration 94/100, loss: 1.5017, abs_loss: 0.0062, ssl_loss: 1.4955\n",
      "Iteration 95/100, loss: 1.4980, abs_loss: 0.0061, ssl_loss: 1.4920\n",
      "Iteration 96/100, loss: 1.5126, abs_loss: 0.0059, ssl_loss: 1.5068\n",
      "Iteration 97/100, loss: 1.5113, abs_loss: 0.0057, ssl_loss: 1.5056\n",
      "Iteration 98/100, loss: 1.5152, abs_loss: 0.0055, ssl_loss: 1.5097\n",
      "Iteration 99/100, loss: 1.4803, abs_loss: 0.0054, ssl_loss: 1.4749\n",
      "Iteration 100/100, loss: 1.4921, abs_loss: 0.0055, ssl_loss: 1.4866\n"
     ]
    }
   ],
   "source": [
    "# Record & Save an annotated dataset\n",
    "\n",
    "# from dataset.base import BaseHierDataset\n",
    "from dataset.arithmetic import ArithmeticHierDataset\n",
    "from nil import annotate_abstraction\n",
    "from nil import supervise_gat \n",
    "\n",
    "record_dataset = ArithmeticHierDataset.from_dataset(dataset)\n",
    "\n",
    "# Greedy Abstraction Annotation (Passing knowledge to the next generation)\n",
    "# ------------------------------------------------------------------------\n",
    "record_dataset = annotate_abstraction(record_dataset, gat)\n",
    "\n",
    "\n",
    "# Reset GAT module\n",
    "# -------------------\n",
    "gat = GAT(gat_config)\n",
    "\n",
    "\n",
    "# Weak Supervision (GAT)\n",
    "# ------------------------------------------------------------------------\n",
    "weak_iterations = 100 # require tuning\n",
    "context_length = 1024\n",
    "\n",
    "supervised_gat = supervise_gat(record_dataset, gat, weak_iterations, context_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep it beautifully simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/200, loss: 3.2189, abs_loss: 0.0000, ssl_loss: 3.2189\n",
      "\n",
      "Improve ppl percentage (train): 0.0000\n",
      "per-sample abstraction switch ratio: 0.0000 | t_search: 1 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 2/200, loss: 3.1475, abs_loss: 0.0000, ssl_loss: 3.1475\n",
      "\n",
      "Improve ppl percentage (train): 0.0376\n",
      "per-sample abstraction switch ratio: 0.0000 | t_search: 2 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 3/200, loss: 6.4414, abs_loss: 3.3758, ssl_loss: 3.0656\n",
      "\n",
      "Improve ppl percentage (train): 0.0855\n",
      "per-sample abstraction switch ratio: 0.4000 | t_search: 3 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 4/200, loss: 6.2989, abs_loss: 3.2920, ssl_loss: 3.0069\n",
      "\n",
      "Improve ppl percentage (train): 0.1368\n",
      "per-sample abstraction switch ratio: 0.2857 | t_search: 4 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 5/200, loss: 6.1441, abs_loss: 3.1884, ssl_loss: 2.9557\n",
      "\n",
      "Improve ppl percentage (train): 0.1847\n",
      "per-sample abstraction switch ratio: 0.0769 | t_search: 5 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 6/200, loss: 6.0763, abs_loss: 3.1510, ssl_loss: 2.9252\n",
      "\n",
      "Improve ppl percentage (train): 0.2377\n",
      "per-sample abstraction switch ratio: 0.3077 | t_search: 6 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 7/200, loss: 5.9064, abs_loss: 3.0279, ssl_loss: 2.8785\n",
      "\n",
      "Improve ppl percentage (train): 0.1850\n",
      "per-sample abstraction switch ratio: 0.4074 | t_search: 7 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 8/200, loss: 5.7194, abs_loss: 2.8910, ssl_loss: 2.8283\n",
      "\n",
      "Improve ppl percentage (train): 0.2204\n",
      "per-sample abstraction switch ratio: 0.2593 | t_search: 8 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 9/200, loss: 5.6300, abs_loss: 2.8148, ssl_loss: 2.8151\n",
      "\n",
      "Improve ppl percentage (train): 0.1675\n",
      "per-sample abstraction switch ratio: 0.3462 | t_search: 9 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 10/200, loss: 5.5702, abs_loss: 2.7745, ssl_loss: 2.7957\n",
      "\n",
      "Improve ppl percentage (train): 0.1649\n",
      "per-sample abstraction switch ratio: 0.4800 | t_search: 10 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 11/200, loss: 5.4499, abs_loss: 2.6736, ssl_loss: 2.7763\n",
      "\n",
      "Improve ppl percentage (train): 0.1193\n",
      "per-sample abstraction switch ratio: 0.4444 | t_search: 11 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 12/200, loss: 5.4817, abs_loss: 2.7093, ssl_loss: 2.7725\n",
      "\n",
      "Improve ppl percentage (train): 0.1402\n",
      "per-sample abstraction switch ratio: 0.6154 | t_search: 12 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 13/200, loss: 5.2844, abs_loss: 2.5247, ssl_loss: 2.7597\n",
      "\n",
      "Improve ppl percentage (train): 0.1271\n",
      "per-sample abstraction switch ratio: 0.4815 | t_search: 13 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 14/200, loss: 5.0293, abs_loss: 2.2947, ssl_loss: 2.7346\n",
      "\n",
      "Improve ppl percentage (train): 0.4029\n",
      "per-sample abstraction switch ratio: 0.4074 | t_search: 14 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 15/200, loss: 5.1436, abs_loss: 2.3688, ssl_loss: 2.7748\n",
      "\n",
      "Improve ppl percentage (train): 0.3120\n",
      "per-sample abstraction switch ratio: 0.5926 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 16/200, loss: 4.8908, abs_loss: 2.1025, ssl_loss: 2.7883\n",
      "\n",
      "Improve ppl percentage (train): 2.9310\n",
      "per-sample abstraction switch ratio: 0.5000 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 17/200, loss: 4.5082, abs_loss: 1.7637, ssl_loss: 2.7445\n",
      "\n",
      "Improve ppl percentage (train): 3.0696\n",
      "per-sample abstraction switch ratio: 0.0741 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 18/200, loss: 4.4024, abs_loss: 1.6435, ssl_loss: 2.7589\n",
      "\n",
      "Improve ppl percentage (train): 0.6573\n",
      "per-sample abstraction switch ratio: 0.1154 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 19/200, loss: 4.5818, abs_loss: 1.7893, ssl_loss: 2.7925\n",
      "\n",
      "Improve ppl percentage (train): 0.0353\n",
      "per-sample abstraction switch ratio: 0.6154 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 20/200, loss: 4.2757, abs_loss: 1.5195, ssl_loss: 2.7562\n",
      "\n",
      "Improve ppl percentage (train): 3.3827\n",
      "per-sample abstraction switch ratio: 0.4074 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 21/200, loss: 3.9752, abs_loss: 1.2379, ssl_loss: 2.7373\n",
      "\n",
      "Improve ppl percentage (train): 5.6334\n",
      "per-sample abstraction switch ratio: 0.0769 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 22/200, loss: 3.9768, abs_loss: 1.1926, ssl_loss: 2.7842\n",
      "\n",
      "Improve ppl percentage (train): 5.5106\n",
      "per-sample abstraction switch ratio: 0.1200 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 23/200, loss: 3.7886, abs_loss: 1.0734, ssl_loss: 2.7152\n",
      "\n",
      "Improve ppl percentage (train): 6.3197\n",
      "per-sample abstraction switch ratio: 0.1111 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 24/200, loss: 3.7240, abs_loss: 1.0617, ssl_loss: 2.6623\n",
      "\n",
      "Improve ppl percentage (train): 8.0264\n",
      "per-sample abstraction switch ratio: 0.2222 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 25/200, loss: 3.7466, abs_loss: 1.1051, ssl_loss: 2.6415\n",
      "\n",
      "Improve ppl percentage (train): 8.0475\n",
      "per-sample abstraction switch ratio: 0.3333 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 26/200, loss: 3.6543, abs_loss: 0.9990, ssl_loss: 2.6553\n",
      "\n",
      "Improve ppl percentage (train): 8.6019\n",
      "per-sample abstraction switch ratio: 0.1923 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 27/200, loss: 3.6542, abs_loss: 0.9697, ssl_loss: 2.6845\n",
      "\n",
      "Improve ppl percentage (train): 8.8123\n",
      "per-sample abstraction switch ratio: 0.5600 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 28/200, loss: 3.7301, abs_loss: 1.2339, ssl_loss: 2.4962\n",
      "\n",
      "Improve ppl percentage (train): 11.4021\n",
      "per-sample abstraction switch ratio: 0.6154 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 29/200, loss: 3.4397, abs_loss: 0.9074, ssl_loss: 2.5323\n",
      "\n",
      "Improve ppl percentage (train): 15.2979\n",
      "per-sample abstraction switch ratio: 0.4815 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 30/200, loss: 3.3764, abs_loss: 0.8569, ssl_loss: 2.5195\n",
      "\n",
      "Improve ppl percentage (train): 13.4940\n",
      "per-sample abstraction switch ratio: 0.4615 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 31/200, loss: 3.3457, abs_loss: 1.0181, ssl_loss: 2.3276\n",
      "\n",
      "Improve ppl percentage (train): 16.3470\n",
      "per-sample abstraction switch ratio: 0.4074 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 32/200, loss: 3.1125, abs_loss: 0.6771, ssl_loss: 2.4354\n",
      "\n",
      "Improve ppl percentage (train): 19.3606\n",
      "per-sample abstraction switch ratio: 0.4615 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 33/200, loss: 3.2900, abs_loss: 0.6904, ssl_loss: 2.5996\n",
      "\n",
      "Improve ppl percentage (train): 19.3003\n",
      "per-sample abstraction switch ratio: 0.6400 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 34/200, loss: 3.0573, abs_loss: 0.5656, ssl_loss: 2.4916\n",
      "\n",
      "Improve ppl percentage (train): 18.4063\n",
      "per-sample abstraction switch ratio: 0.4615 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 35/200, loss: 2.9190, abs_loss: 0.5891, ssl_loss: 2.3299\n",
      "\n",
      "Improve ppl percentage (train): 17.5035\n",
      "per-sample abstraction switch ratio: 0.5926 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 36/200, loss: 2.9606, abs_loss: 0.7283, ssl_loss: 2.2323\n",
      "\n",
      "Improve ppl percentage (train): 17.9639\n",
      "per-sample abstraction switch ratio: 0.2963 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 37/200, loss: 2.7871, abs_loss: 0.5623, ssl_loss: 2.2248\n",
      "\n",
      "Improve ppl percentage (train): 21.6784\n",
      "per-sample abstraction switch ratio: 0.4444 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 38/200, loss: 2.6250, abs_loss: 0.3874, ssl_loss: 2.2376\n",
      "\n",
      "Improve ppl percentage (train): 25.2365\n",
      "per-sample abstraction switch ratio: 0.1538 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 39/200, loss: 2.6238, abs_loss: 0.3681, ssl_loss: 2.2557\n",
      "\n",
      "Improve ppl percentage (train): 28.0668\n",
      "per-sample abstraction switch ratio: 0.4815 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[1;32m     30\u001b[0m     repeat_batch, switch_ratio, rollout_advantages \u001b[38;5;241m=\u001b[39m sorl_search_v2(gat, batch_data, n, temperature, t_search) \u001b[38;5;66;03m# pinned greedy sample ver.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m ppt \u001b[38;5;241m=\u001b[39m gat(repeat_batch)\n\u001b[1;32m     34\u001b[0m ssl_loss \u001b[38;5;241m=\u001b[39m compute_ssl_loss(repeat_batch, ppt)\n\u001b[1;32m     35\u001b[0m abs_loss \u001b[38;5;241m=\u001b[39m compute_abs_ssl_loss(repeat_batch, ppt, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Implementation/abstraction-learning/model.py:259\u001b[0m, in \u001b[0;36mGAT.forward\u001b[0;34m(self, batch_data)\u001b[0m\n\u001b[1;32m    256\u001b[0m v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m--> 259\u001b[0m     x, v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh[i](x, v1, x0, block_mask)\n\u001b[1;32m    261\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_ppt(x, batch_data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Implementation/abstraction-learning/model.py:133\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, v1, x0, block_mask)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, v1, x0, block_mask):\n\u001b[1;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdas[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdas[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m x0\n\u001b[0;32m--> 133\u001b[0m     x1, v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(norm(x), v1, block_mask)\n\u001b[1;32m    134\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x1\n\u001b[1;32m    135\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(norm(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Implementation/abstraction-learning/model.py:97\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, v1, block_mask)\u001b[0m\n\u001b[1;32m     95\u001b[0m q, k \u001b[38;5;241m=\u001b[39m norm(q), norm(k) \u001b[38;5;66;03m# QK norm suggested by @Grad62304977\u001b[39;00m\n\u001b[1;32m     96\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary(q), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary(k)        \n\u001b[0;32m---> 97\u001b[0m y \u001b[38;5;241m=\u001b[39m flex_attention(\n\u001b[1;32m     98\u001b[0m     q\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     99\u001b[0m     k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    100\u001b[0m     v\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    101\u001b[0m     block_mask\u001b[38;5;241m=\u001b[39mblock_mask,\n\u001b[1;32m    102\u001b[0m     kernel_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflex_kernel_options\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    104\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview_as(x)       \n\u001b[1;32m    105\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y)       \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/attention/flex_attention.py:1353\u001b[0m, in \u001b[0;36mflex_attention\u001b[0;34m(query, key, value, score_mod, block_mask, scale, enable_gqa, return_lse, kernel_options)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1352\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1353\u001b[0m out, lse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1354\u001b[0m     _flex_attention_hop_wrapper, backend\u001b[38;5;241m=\u001b[39mbackend, fullgraph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m )(\n\u001b[1;32m   1356\u001b[0m     query,\n\u001b[1;32m   1357\u001b[0m     key,\n\u001b[1;32m   1358\u001b[0m     value,\n\u001b[1;32m   1359\u001b[0m     score_mod,\n\u001b[1;32m   1360\u001b[0m     block_mask\u001b[38;5;241m.\u001b[39mas_tuple(),  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m     scale,\n\u001b[1;32m   1362\u001b[0m     kernel_options,\n\u001b[1;32m   1363\u001b[0m )\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_lse:\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, lse \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:574\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    570\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    578\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    579\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/attention/flex_attention.py:1340\u001b[0m, in \u001b[0;36mflex_attention.<locals>._flex_attention_hop_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebugging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   1335\u001b[0m     make_eager_backend_with_torch_function_mode,\n\u001b[1;32m   1336\u001b[0m )\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;66;03m# Dynamo is expecting a callable with \"__code__\" attribute.\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;66;03m# We cannot directly pass hop to it. So we wrap it in a dummy function.\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flex_attention_hop_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flex_attention_hop(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_compilation_env():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m<eval_with_key>.31:21\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, s1, L_args_0_, s3, L_args_4_12_closure_0_cell_contents, s5, L_args_1_, s7, L_args_2_, L_args_4_0_, L_args_4_1_, s10, L_args_4_2_, s11, s12, L_args_4_3_, s13, L_args_4_4_, s14, s15, L_args_4_5_, s16, L_args_4_6_, s17, s18, L_args_4_7_, s19, L_args_4_8_, s20, s21, L_args_4_9_)\u001b[0m\n\u001b[1;32m     19\u001b[0m score_mod_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_mod_0\n\u001b[1;32m     20\u001b[0m mask_fn_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_fn_0\n\u001b[0;32m---> 21\u001b[0m flex_attention \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mhigher_order\u001b[38;5;241m.\u001b[39mflex_attention(l_args_0_, l_args_1_, l_args_2_, score_mod_0, (l_args_4_0_, l_args_4_1_, l_args_4_2_, l_args_4_3_, l_args_4_4_, l_args_4_5_, l_args_4_6_, l_args_4_7_, l_args_4_8_, l_args_4_9_, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, mask_fn_0), \u001b[38;5;241m0.17677669529663687\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRESCALE_QK\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROWS_GUARANTEED_SAFE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOCKS_ARE_CONTIGUOUS\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOUTPUT_LOGSUMEXP\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}, (), (s3, l_args_4_12_closure_0_cell_contents));  l_args_0_ \u001b[38;5;241m=\u001b[39m l_args_1_ \u001b[38;5;241m=\u001b[39m l_args_2_ \u001b[38;5;241m=\u001b[39m score_mod_0 \u001b[38;5;241m=\u001b[39m l_args_4_0_ \u001b[38;5;241m=\u001b[39m l_args_4_1_ \u001b[38;5;241m=\u001b[39m l_args_4_2_ \u001b[38;5;241m=\u001b[39m l_args_4_3_ \u001b[38;5;241m=\u001b[39m l_args_4_4_ \u001b[38;5;241m=\u001b[39m l_args_4_5_ \u001b[38;5;241m=\u001b[39m l_args_4_6_ \u001b[38;5;241m=\u001b[39m l_args_4_7_ \u001b[38;5;241m=\u001b[39m l_args_4_8_ \u001b[38;5;241m=\u001b[39m l_args_4_9_ \u001b[38;5;241m=\u001b[39m mask_fn_0 \u001b[38;5;241m=\u001b[39m s3 \u001b[38;5;241m=\u001b[39m l_args_4_12_closure_0_cell_contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m getitem \u001b[38;5;241m=\u001b[39m flex_attention[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m getitem_1 \u001b[38;5;241m=\u001b[39m flex_attention[\u001b[38;5;241m1\u001b[39m];  flex_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:90\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[0;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     89\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     91\u001b[0m         query,\n\u001b[1;32m     92\u001b[0m         key,\n\u001b[1;32m     93\u001b[0m         value,\n\u001b[1;32m     94\u001b[0m         score_mod,\n\u001b[1;32m     95\u001b[0m         block_mask,\n\u001b[1;32m     96\u001b[0m         scale,\n\u001b[1;32m     97\u001b[0m         kernel_options,\n\u001b[1;32m     98\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m     99\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:436\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:302\u001b[0m, in \u001b[0;36mHigherOrderOperator.dispatch\u001b[0;34m(self, dispatch_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache[dispatch_key]\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kernel, DispatchKey)\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mFuncTorchDynamicLayerFrontMode:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_functorch(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:731\u001b[0m, in \u001b[0;36mflex_attention_autograd\u001b[0;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m         fw_graph, bw_graph \u001b[38;5;241m=\u001b[39m score_mod, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 731\u001b[0m     out, logsumexp \u001b[38;5;241m=\u001b[39m FlexAttentionAutogradOp\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    732\u001b[0m         query,\n\u001b[1;32m    733\u001b[0m         key,\n\u001b[1;32m    734\u001b[0m         value,\n\u001b[1;32m    735\u001b[0m         fw_graph,\n\u001b[1;32m    736\u001b[0m         bw_graph,\n\u001b[1;32m    737\u001b[0m         block_mask,\n\u001b[1;32m    738\u001b[0m         scale,\n\u001b[1;32m    739\u001b[0m         kernel_options,\n\u001b[1;32m    740\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;241m*\u001b[39mscore_mod_other_buffers,\n\u001b[1;32m    742\u001b[0m     )\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, logsumexp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:597\u001b[0m, in \u001b[0;36mFlexAttentionAutogradOp.forward\u001b[0;34m(ctx, query, key, value, fw_graph, joint_graph, block_mask, scale, kernel_options, mask_mod_other_buffers, *score_mod_other_buffers)\u001b[0m\n\u001b[1;32m    595\u001b[0m ctx\u001b[38;5;241m.\u001b[39m_score_mod_other_buffers_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(score_mod_other_buffers)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_AutoDispatchBelowAutograd():\n\u001b[0;32m--> 597\u001b[0m     out, logsumexp \u001b[38;5;241m=\u001b[39m flex_attention(\n\u001b[1;32m    598\u001b[0m         query,\n\u001b[1;32m    599\u001b[0m         key,\n\u001b[1;32m    600\u001b[0m         value,\n\u001b[1;32m    601\u001b[0m         fw_graph,\n\u001b[1;32m    602\u001b[0m         block_mask,\n\u001b[1;32m    603\u001b[0m         scale,\n\u001b[1;32m    604\u001b[0m         kernel_options,\n\u001b[1;32m    605\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m    606\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[1;32m    609\u001b[0m save_tensors_and_symints_for_backward(\n\u001b[1;32m    610\u001b[0m     ctx,\n\u001b[1;32m    611\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m     ),\n\u001b[1;32m    621\u001b[0m )\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, logsumexp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:90\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[0;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     89\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     91\u001b[0m         query,\n\u001b[1;32m     92\u001b[0m         key,\n\u001b[1;32m     93\u001b[0m         value,\n\u001b[1;32m     94\u001b[0m         score_mod,\n\u001b[1;32m     95\u001b[0m         block_mask,\n\u001b[1;32m     96\u001b[0m         scale,\n\u001b[1;32m     97\u001b[0m         kernel_options,\n\u001b[1;32m     98\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m     99\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:431\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    429\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m _to_flat_tuple(args, kwargs)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function(flat_args):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/overrides.py:1720\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1720\u001b[0m         result \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1722\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py:142\u001b[0m, in \u001b[0;36mTransformGetItemToIndex.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m index_args):\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mod_index(args[\u001b[38;5;241m0\u001b[39m], index_args)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:90\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[0;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     89\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     91\u001b[0m         query,\n\u001b[1;32m     92\u001b[0m         key,\n\u001b[1;32m     93\u001b[0m         value,\n\u001b[1;32m     94\u001b[0m         score_mod,\n\u001b[1;32m     95\u001b[0m         block_mask,\n\u001b[1;32m     96\u001b[0m         scale,\n\u001b[1;32m     97\u001b[0m         kernel_options,\n\u001b[1;32m     98\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m     99\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:436\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:302\u001b[0m, in \u001b[0;36mHigherOrderOperator.dispatch\u001b[0;34m(self, dispatch_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache[dispatch_key]\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kernel, DispatchKey)\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mFuncTorchDynamicLayerFrontMode:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_functorch(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:264\u001b[0m, in \u001b[0;36msdpa_dense\u001b[0;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;129m@flex_attention\u001b[39m\u001b[38;5;241m.\u001b[39mpy_impl(DispatchKey\u001b[38;5;241m.\u001b[39mCompositeExplicitAutograd)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msdpa_dense\u001b[39m(\n\u001b[1;32m    254\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m    263\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 264\u001b[0m     out, lse \u001b[38;5;241m=\u001b[39m math_attention(\n\u001b[1;32m    265\u001b[0m         query,\n\u001b[1;32m    266\u001b[0m         key,\n\u001b[1;32m    267\u001b[0m         value,\n\u001b[1;32m    268\u001b[0m         score_mod,\n\u001b[1;32m    269\u001b[0m         block_mask,\n\u001b[1;32m    270\u001b[0m         scale,\n\u001b[1;32m    271\u001b[0m         kernel_options,\n\u001b[1;32m    272\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m    273\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    275\u001b[0m     out \u001b[38;5;241m=\u001b[39m _permute_strides(out, query\u001b[38;5;241m.\u001b[39mstride())\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, lse\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:247\u001b[0m, in \u001b[0;36mmath_attention\u001b[0;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m    244\u001b[0m masked_rows \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mall(post_mod_scores \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    245\u001b[0m logsumexp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(masked_rows, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), logsumexp)\n\u001b[0;32m--> 247\u001b[0m post_mod_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_safe_softmax(post_mod_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m post_mod_scores\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m@\u001b[39m value, logsumexp \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Benchmark RL & SSL combination strategies \n",
    "# (I). Pick the best & learn it \n",
    "# -------------------------------------------------------------\n",
    "import copy \n",
    "import wandb\n",
    "import torch\n",
    "from search import compute_ssl_loss, get_batch, eval_search_improvement\n",
    "from search import compute_abs_ssl_loss\n",
    "from search import sorl_search_v2\n",
    "from search import compute_curriculum_t_increment, eval_ppl_with_search, curriculum_iter\n",
    "\n",
    "n = 3 \n",
    "temperature = 1.0\n",
    "num_iterations = 200\n",
    "context_length = 1024\n",
    "global_step = 0 \n",
    "\n",
    "optimizer = torch.optim.Adam(gat.parameters(), lr=1e-3)\n",
    "gat.train() \n",
    "\n",
    "# curriculum\n",
    "t_search = 0\n",
    "t_delta, t_max = compute_curriculum_t_increment(num_iterations=num_iterations, context_length=context_length, K=gat.K, max_ts=max(dataset.lengths))\n",
    "\n",
    "while global_step < num_iterations: \n",
    "\n",
    "    batch_data = get_batch(dataset.sequences, dataset.lengths, context_length // n, gat.L, gat.K)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        repeat_batch, switch_ratio, rollout_advantages = sorl_search_v2(gat, batch_data, n, temperature, t_search) # pinned greedy sample ver.\n",
    "\n",
    "    ppt = gat(repeat_batch)\n",
    "\n",
    "    ssl_loss = compute_ssl_loss(repeat_batch, ppt)\n",
    "    abs_loss = compute_abs_ssl_loss(repeat_batch, ppt, level=1)\n",
    "\n",
    "    loss = abs_loss + ssl_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Iteration {global_step+1}/{num_iterations}, loss: {loss.item():.4f}, abs_loss: {abs_loss.item():.4f}, ssl_loss: {ssl_loss.item():.4f}\")\n",
    "\n",
    "    global_step += 1\n",
    "    t_search = min(t_search + t_delta, t_max)\n",
    "    del loss, abs_loss, ssl_loss\n",
    "    \n",
    "    # train data ppl improvement\n",
    "    improve_ppl_train = eval_search_improvement(gat, batch_data, t_search=t_search)\n",
    "    print(f\"\\nImprove ppl percentage (train): {improve_ppl_train:.4f}\")\n",
    "    print(f\"per-sample abstraction switch ratio: {switch_ratio:.4f} | t_search: {t_search} | (How often greedy sample is rejected by other abstraction)\")\n",
    "    # s = observe_abstraction(batch_data, gat, t_search=t_search, temperature=0.0)\n",
    "    # print(s)\n",
    "\n",
    "    # if global_step % 10 == 0: \n",
    "    if False: \n",
    "        val_data = get_batch(id_val_dataset.sequences, id_val_dataset.lengths, context_length, gat.L, gat.K)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            improve_ppl_val = eval_search_improvement(gat, val_data, t_search=t_search)\n",
    "            print(f\"Improve ppl percentage (val): {improve_ppl_val:.4f}\\n\")\n",
    "        \n",
    "            if t_search == t_max:\n",
    "                traj_ppl_val = eval_ppl_with_search(val_data, gat, dataset.answer_token_id, n=6, temperature=1.0)\n",
    "                print(f\"Traj ppl (val): {traj_ppl_val.mean().item():.4f}\\n\")\n",
    "\n",
    "            if not config.t_curriculum: \n",
    "                traj_ppl_val = eval_generate_ppl(gat, val_data, n=1, temperature=0.0, t_search=t_search).mean()\n",
    "                print(f\"Traj ppl (val): {traj_ppl_val.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
