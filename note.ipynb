{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $0^{th}$ level policy model $\\pi^{(0)}(s_{t}|s_{t^{(1)}}^{(1)} \\circ a_{t}^{(1)})$\n",
    "2. $1^{th}$ level policy model $\\pi^{(1)}()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 64,   3,  49,  98,  14,  71,  73,  24,   6,  88,  97,  38,  60, 122,\n",
       "          110,  10, 119,  93,   0],\n",
       "         [112,  67,  71,  41,  35, 112,  19, 123,  57,  72,  74,  73,  95,  39,\n",
       "           71,  56, 118,  63,   0],\n",
       "         [ 91,  57,  48, 122,  71, 109,  95,  44,   9,  54,  15, 108,   1,  14,\n",
       "            2,  59,  86, 124,   0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0]])]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import sandwich_embedding as se \n",
    "import torch \n",
    "from model import GPTConfig\n",
    "\n",
    "config = GPTConfig(vocab_size=128, n_layer=4, n_head=4, n_embd=64, K=3, L=2, device=\"cpu\", _compile=False)\n",
    "B, S, D = 3, 6, config.n_embd \n",
    "K = config.K\n",
    "L = config.L\n",
    "\n",
    "# (I). Sandwich embedding\n",
    "# --------------------------------------------------------------------------------------------------------------------------\n",
    "# Sandwich embedding ensemble (temporal predicted token embeddings ensemble)\n",
    "S, S1, S2 = 18, 6, 2\n",
    "\n",
    "tok_embed = torch.randn(B, S, D)\n",
    "high_embed = torch.randn(B, S2, D)\n",
    "low_embed = torch.randn(B, S1, D)\n",
    "\n",
    "# test 1. (S, S1, S2) = (0, 1, 0) --> 1 (check)\n",
    "# test 2. (S, S1, S2) = (3, 4, 1) --> 4 (check)\n",
    "# test 3. (S, S1, S2) = (18, 6, 5) --> 19 (check)\n",
    "cond_embed = se(low_embed, tok_embed, high_embed, K)\n",
    "\n",
    "\n",
    "# (II). Conditional GPT\n",
    "# --------------------------------------------------------------------------------------------------------------------------\n",
    "from model import CondGPT, GPTConfig\n",
    "\n",
    "condgpt = CondGPT(config)\n",
    "idx = torch.randint(0, 128, (B, S))\n",
    "\n",
    "condgpt.forward(idx, high_embed, low_embed)\n",
    "condgpt.generate(idx, high_embed, low_embed)[1]\n",
    "\n",
    "# (III). GAT \n",
    "# --------------------------------------------------------------------------------------------------------------------------\n",
    "from model import GAT\n",
    "gat = GAT(config)\n",
    "\n",
    "idx = torch.randint(0, 128, (B, S))\n",
    "\n",
    "# (a). one-step generation with GAT\n",
    "gat.generate(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
