{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment (I). Hiearhical Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "Hierarchical Sequence K=2 L=3 (aligned by timestamp):\n",
      "-------------------------------------------------------\n",
      "Level 2:                 [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2]             [2] \n",
      "Level 1:         [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1]     [1] \n",
      "Level 0:     [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] \n",
      "=======================================================\n",
      "Total tokens: 350, Max timestamp: 200\n",
      "\n",
      "Epoch 1/30, Loss: 4.158883571624756\n",
      "Epoch 2/30, Loss: 3.987037420272827\n",
      "Epoch 3/30, Loss: 3.74379563331604\n",
      "Epoch 4/30, Loss: 3.527414321899414\n",
      "Epoch 5/30, Loss: 3.318906784057617\n",
      "Epoch 6/30, Loss: 3.1103715896606445\n",
      "Epoch 7/30, Loss: 2.903388261795044\n",
      "Epoch 8/30, Loss: 2.699125289916992\n",
      "Epoch 9/30, Loss: 2.49737286567688\n",
      "Epoch 10/30, Loss: 2.2964634895324707\n",
      "Epoch 11/30, Loss: 2.096004009246826\n",
      "Epoch 12/30, Loss: 1.8935136795043945\n",
      "Epoch 13/30, Loss: 1.7067910432815552\n",
      "Epoch 14/30, Loss: 1.5114589929580688\n",
      "Epoch 15/30, Loss: 1.3093849420547485\n",
      "Epoch 16/30, Loss: 1.156589150428772\n",
      "Epoch 17/30, Loss: 1.0025535821914673\n",
      "Epoch 18/30, Loss: 0.8656474947929382\n",
      "Epoch 19/30, Loss: 0.7505219578742981\n",
      "Epoch 20/30, Loss: 0.6471614241600037\n",
      "Epoch 21/30, Loss: 0.5491518378257751\n",
      "Epoch 22/30, Loss: 0.46484485268592834\n",
      "Epoch 23/30, Loss: 0.39563462138175964\n",
      "Epoch 24/30, Loss: 0.33726969361305237\n",
      "Epoch 25/30, Loss: 0.28673091530799866\n",
      "Epoch 26/30, Loss: 0.2428400069475174\n",
      "Epoch 27/30, Loss: 0.2045609951019287\n",
      "Epoch 28/30, Loss: 0.17239819467067719\n",
      "Epoch 29/30, Loss: 0.14576207101345062\n",
      "Epoch 30/30, Loss: 0.13522891700267792\n"
     ]
    }
   ],
   "source": [
    "# The issue with this counting sequence is that it has batch size of 1 only \n",
    "# it's fine for now, but clearly a fourier series decomposition is more beautiful\n",
    "\n",
    "# (I). Counting Sequence\n",
    "# --------------------------------------------------------------------------------\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "# (I.1) Generate Counting Sequence\n",
    "# ----------------------------------------------------\n",
    "def generate_level(l: int, seq: list, t: int, L: int, K: int): \n",
    "    if l < L:\n",
    "        seq[l] += str(l)\n",
    "        if t % K == 0: \n",
    "            return generate_level(l+1, seq, t // K, L, K)\n",
    "    return seq\n",
    "\n",
    "def generate_count_seq(L: int, K: int, T: int): \n",
    "    seq = defaultdict(str)\n",
    "    for t in range(1, T+1): \n",
    "        seq = generate_level(0, seq, t, L, K)\n",
    "    return seq\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# (I.2) Tokenizer (basic integer tokenizer)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "class TinyTokenizer: \n",
    "    def __init__(self, vocab: dict):\n",
    "        self.vocab = {str(k): v for k, v in vocab.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "    def __call__(self, seq: str):\n",
    "        return [self.vocab[c] for c in seq]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "# (I.3) Tensor idx sequence preparation \n",
    "# ----------------------------------------------------\n",
    "\n",
    "L = 3\n",
    "K = 2\n",
    "T = 200\n",
    "\n",
    "data = generate_count_seq(L, K, T)\n",
    "tokenizer = TinyTokenizer({str(k): k for k in range(10)})\n",
    "\n",
    "idx = [tokenizer(seq) for seq in data.values()]\n",
    "idx += [[] for _ in range(L - len(idx))]\n",
    "samples = [(idx, None)]\n",
    "\n",
    "from model import GATConfig, GAT, HierSeq\n",
    "from utils import stream_print_hseq\n",
    "from torch.optim import Adam \n",
    "\n",
    "config = GATConfig(K=K, L=L, n_embd=128, n_head=4, device=\"cpu\", _compile=False)\n",
    "gat = GAT(config)\n",
    "\n",
    "# .from_hiearchical_data has error: prepared token is NOT interleaved with correct causal ordering\n",
    "batch_data = HierSeq.from_hierarchical_data(samples, K=gat.K, L=gat.L)\n",
    "stream_print_hseq(batch_data) # sanity check\n",
    "\n",
    "# Batched Forward Propagation\n",
    "epochs = 30\n",
    "# gat.train()\n",
    "\n",
    "\n",
    "# Training Loop : learning just fine -- loss reduces quickly\n",
    "# ----------------------------------------------------\n",
    "optimizer = Adam(gat.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = gat(batch_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "    # break\n",
    "# ----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "Hierarchical Sequence K=2 L=3 (aligned by timestamp):\n",
      "-------------------------------------------------------\n",
      "Level 2:                 [2]             [2]             [2]                 \n",
      "Level 1:         [1]     [1]     [1]     [1]     [1]     [1]     [1]         \n",
      "Level 0:     [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] [0] \n",
      "=======================================================\n",
      "Total tokens: 26, Max timestamp: 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import * \n",
    "\n",
    "\n",
    "test_data = generate_count_seq(L, K, 1)\n",
    "idx = [tokenizer(seq) for seq in test_data.values()]\n",
    "idx += [[] for _ in range(L - len(idx))]\n",
    "test_samples = [(idx, None)]\n",
    "test_batch_data = HierSeq.from_hierarchical_data(test_samples, K=gat.K, L=gat.L)\n",
    "\n",
    "\n",
    "for _ in range(25): \n",
    "    gat.generate(test_batch_data)\n",
    "    stream_print_hseq(test_batch_data)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-Organizing Reinforcement Learning\n",
    "1. A more exciting question is how can I build a GAT module for a Snake game? It'll be the natural test-bed for SoRL -- a snake with inner-monologue. \n",
    "2. Decision Abstractive Transformer (DAT).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Episode 0: Score=0, Reward=-11.0\n",
      "Episode 1: Score=0, Reward=-10.9\n",
      "Episode 2: Score=0, Reward=-10.9\n",
      "Episode 3: Score=1, Reward=-0.7999999999999989\n",
      "Episode 4: Score=0, Reward=-13.200000000000001\n",
      "Sanity check passed: total 153 0-th level tokens (state & action)\n",
      "Sanity check passed: 79 state tokens in data, 79 state tokens in trajectories\n",
      "Sanity check passed: 74 action tokens in data, 74 action tokens in trajectories\n",
      "Sanity check passed: 183 (action/state/abstract) tokens in data\n"
     ]
    }
   ],
   "source": [
    "from snake import SnakeGameEngine, collect_trajectories, RandomAgent\n",
    "from utils import HierTraj, data_sanity_check\n",
    "from constant import PLACE_HOLDER_STATE_TOK, PLACE_HOLDER_ACTION_TOK\n",
    "\n",
    "env = SnakeGameEngine(width=10, height=10)\n",
    "\n",
    "# This is a make-shift data collection pipeline, the abstract tokens are complete false. \n",
    "# The proper way to collect data, per SoRL, is to 'simulate' multiple actions on the environment and store the HierTraj state directly\n",
    "# in that sense, the code snippet in the middle is not useful...\n",
    "\n",
    "# A way to use 'expert trajectory' is to take in the 'trajectories', and have the agent 'learn / explore' on what abstract tokens can be \n",
    "# used to explain the trajectory. This is very similar to the 'intuitive-physics' based 'learning by surprise' or 'active learning from obs' \n",
    "# idea. \n",
    "\n",
    "# For what is worth, the mid-snippet is useless (besides serving as a toy-case on which we test on SSL training of GAT)\n",
    "\n",
    "# Collect trajectories\n",
    "trajectories = collect_trajectories(env, RandomAgent(env), num_episodes=5, device=\"cpu\")\n",
    "\n",
    "samples = []\n",
    "for trajectory in trajectories: \n",
    "\n",
    "    n_state = trajectory[0].size(0)\n",
    "    placeholder_tokens = [PLACE_HOLDER_STATE_TOK if i % 2 == 0 else PLACE_HOLDER_ACTION_TOK for i in range(2*n_state-1)]\n",
    "    sample = ([placeholder_tokens, [3, 9, 4, 2], [19, 14]], None)\n",
    "    samples.append(sample)\n",
    "\n",
    "batch_data = HierTraj.from_hierarchical_data(samples, K=3, L=3)\n",
    "data_sanity_check(batch_data, trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DAT, DATConfig\n",
    "\n",
    "# DAT model \n",
    "\n",
    "config = DATConfig(\n",
    "    n_layer=4,\n",
    "    n_head=2,\n",
    "    n_embd=32,\n",
    "    K=2,\n",
    "    L=3,\n",
    "    vocab_size_list=[64, 32],\n",
    "    device=\"cpu\",\n",
    "    _compile=True,\n",
    ")\n",
    "\n",
    "# Snake specific encoder & decoder for state & action\n",
    "from snake import StateEncoder, StateDecoder, ActionEncoder, ActionDecoder\n",
    "\n",
    "state_encoder = StateEncoder(height=10, width=10, feature_dim=config.n_embd)\n",
    "state_decoder = StateDecoder(height=10, width=10, feature_dim=config.n_embd)\n",
    "action_encoder = ActionEncoder(action_size=4, feature_dim=config.n_embd)\n",
    "action_decoder = ActionDecoder(action_size=4, feature_dim=config.n_embd)\n",
    "\n",
    "dat = DAT(config, state_encoder, state_decoder, action_encoder, action_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute loss\n",
    "loss = dat(batch_data, trajectories)\n",
    "\n",
    "# generate & update \n",
    "new_batch_data, new_trajectories = dat.generate(batch_data, trajectories)\n",
    "\n",
    "# act: produce action tokens (if there already exists action-tokens un-grounded with reward, skip it)\n",
    "pairs = dat.act(batch_data, trajectories) # list of (sample_idx, action_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "Hierarchical Trajectory K=2 L=3 (aligned by timestamp):\n",
      "-------------------------------------------------------\n",
      "Level 2:              [0]     \n",
      "Level 1:      [0]     [0]     \n",
      "L0-State: [s] [s] [s] [s]     \n",
      "L0-Action:    [a] [a] [a] [a] \n",
      "=======================================================\n",
      "Total tokens: 11, Max timestamp: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate on 'order of generation'\n",
    "from utils import test_dat_gen_order\n",
    "\n",
    "# Sanity check-up function (order of generation)\n",
    "test_dat_gen_order(dat, env, L=3, K=2, n_gen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x2 visualization for first 4 samples\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "from vis import visualize_backtrack\n",
    "from utils import * \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create 2x2 subplot figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Backtrack Visualization for First 4 Samples', fontsize=16)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "# Process first 4 samples\n",
    "num_samples = min(4, batch_data.indices.max().item() + 1)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample_idx = i\n",
    "    \n",
    "    # Get sample-level perplexity data\n",
    "    per_sample_ppt, per_sample_timestamps, max_abs_ts = get_sample_level_ppl(batch_data, ppt, level=0)\n",
    "    \n",
    "    # Extract data for current sample\n",
    "    sample_timestamps = per_sample_timestamps[sample_idx][:max_abs_ts + batch_data.K]\n",
    "    sample_ppt = per_sample_ppt[sample_idx][:max_abs_ts + batch_data.K]\n",
    "    \n",
    "    # Select current subplot\n",
    "    ax = axes_flat[i]\n",
    "    \n",
    "    # Plot the data\n",
    "    ax.plot(sample_timestamps.cpu().numpy(), sample_ppt.cpu().numpy(), \n",
    "            'b-', linewidth=1.5, label='Perplexity')\n",
    "    \n",
    "    # Add threshold line\n",
    "    ax.axhline(y=buffer.ppl_thres, color='r', linestyle='--', \n",
    "               linewidth=1, label=f'Threshold ({buffer.ppl_thres:.2f})')\n",
    "    \n",
    "    # Mark critical timestamps\n",
    "    critical_mask = sample_ppt > buffer.ppl_thres\n",
    "    if critical_mask.any():\n",
    "        critical_ts = sample_timestamps[critical_mask]\n",
    "        critical_ppt = sample_ppt[critical_mask]\n",
    "        ax.scatter(critical_ts.cpu().numpy(), critical_ppt.cpu().numpy(), \n",
    "                  color='red', s=50, zorder=5, label='Critical Points')\n",
    "    \n",
    "    # Mark backtrack points (where cts changed for this sample)\n",
    "    if sample_idx < len(cts):\n",
    "        backtrack_ts = cts[sample_idx] + 1\n",
    "        if backtrack_ts > 0:\n",
    "            ax.axvline(x=backtrack_ts, color='green', linestyle=':', \n",
    "                      linewidth=1.5, label=f'Backtrack (t={backtrack_ts})')\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlabel('Timestamp', fontsize=10)\n",
    "    ax.set_ylabel('Perplexity', fontsize=10)\n",
    "    ax.set_title(f'Sample {sample_idx}', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=8, loc='upper right')\n",
    "    \n",
    "    # Set y-axis limits for better visualization\n",
    "    ax.set_ylim([0, max(10, sample_ppt.max().item() * 1.1)])\n",
    "\n",
    "# Hide unused subplots if less than 4 samples\n",
    "for i in range(num_samples, 4):\n",
    "    axes_flat[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Visualized {num_samples} samples in 2x2 grid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new helper function for cleaner multi-sample visualization\n",
    "from vis import visualize_multi_sample_backtrack\n",
    "\n",
    "# Create 2x2 grid visualization for first 4 samples\n",
    "fig, axes = visualize_multi_sample_backtrack(batch_data, ppt, cts, buffer, num_samples=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Does DAT dream of playing snake game?\n",
    "\n",
    "# from agent import HiearchicalAgent\n",
    "from snake import SnakeGameEngine\n",
    "from utils import draw_gif\n",
    "from agent import collect_dat_game_play_frames\n",
    "from agent import HiearchicalAgent\n",
    "\n",
    "# Environment\n",
    "env = SnakeGameEngine(width=10, height=10)\n",
    "\n",
    "# DAT Agent (Snake specific agent)\n",
    "# agent = HiearchicalAgent(dat, env.reset(), \"cpu\")\n",
    "\n",
    "# DAT plays the snake game\n",
    "# --------------------------------------------------------------------------------------------------------------------------\n",
    "epsilon = 0.9\n",
    "frames = collect_dat_game_play_frames(dat, env, epsilon=epsilon)\n",
    "draw_gif(frames, txt=f\"DAT (randomized) play Snake (epsilon={epsilon})\", path=\"./visual/dat_snake.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Algorithm\n",
    "\n",
    "- Learn to Explain (GAT)\n",
    "Trajectory only data --> Sample --> Train\n",
    "\n",
    "- Learn to Explore (DAT)\n",
    "No data --> Sample --> Extend & Environment Interaction --> Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{SoRL (GAT)}$\n",
    "1. Group advantage computation \n",
    "2. Surrogate loss computation\n",
    "\n",
    "The key for learning from experience is learning from failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn-to-explain (GAT) | one batch ver.\n",
    "\n",
    "# (1). Initialize Data Buffer with trajectory-only data \n",
    "\n",
    "from model import GATConfig, GAT, HierSeq\n",
    "from utils import *\n",
    "from torch.optim import Adam \n",
    "import random \n",
    "\n",
    "L, K = 3, 2\n",
    "config = GATConfig(K=K, L=L, n_embd=128, n_head=4, device=\"cpu\", _compile=False)\n",
    "gat = GAT(config)\n",
    "\n",
    "# - samples without abstract tokens, trajectory data only\n",
    "samples = [([[0]*random.randint(4, 10)] + [[] for l in range(1, config.L)],None) for _ in range(2)]\n",
    "batch_data = HierSeq.from_hierarchical_data(samples, K=gat.K, L=gat.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Control Ratio: 0.1666666716337204 perplexity: 4.505456447601318, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 4.505456924438477, critial timestamps: 1\n",
      "Epoch 1/80, Loss: 4.505456924438477\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 4.420316219329834, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 4.421609878540039, critial timestamps: 1\n",
      "Epoch 2/80, Loss: 4.420963287353516\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 4.288652420043945, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 4.290994167327881, critial timestamps: 1\n",
      "Epoch 3/80, Loss: 4.289823532104492\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 4.174693584442139, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 4.176031589508057, critial timestamps: 1\n",
      "Epoch 4/80, Loss: 4.175362586975098\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 4.068854808807373, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 4.0695037841796875, critial timestamps: 1\n",
      "Epoch 5/80, Loss: 4.069179534912109\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.6977458000183105, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.9664268493652344, critial timestamps: 1\n",
      "Epoch 6/80, Loss: 3.8320863246917725\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.433945655822754, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.865361213684082, critial timestamps: 1\n",
      "Epoch 7/80, Loss: 3.649653434753418\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.5916614532470703, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.768338203430176, critial timestamps: 1\n",
      "Epoch 8/80, Loss: 3.679999828338623\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.6742565631866455, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.67498779296875, critial timestamps: 1\n",
      "Epoch 9/80, Loss: 3.674622058868408\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.5850963592529297, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.5853192806243896, critial timestamps: 1\n",
      "Epoch 10/80, Loss: 3.585207939147949\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.500103712081909, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.9414851665496826, critial timestamps: 1\n",
      "Epoch 11/80, Loss: 3.220794439315796\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.013000011444092, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.4199419021606445, critial timestamps: 1\n",
      "Epoch 12/80, Loss: 3.216470956802368\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.259476900100708, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.723605155944824, critial timestamps: 1\n",
      "Epoch 13/80, Loss: 2.9915409088134766\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.2747504711151123, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.5626893043518066, critial timestamps: 1\n",
      "Epoch 14/80, Loss: 2.91871976852417\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.2327616214752197, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.4002022743225098, critial timestamps: 1\n",
      "Epoch 15/80, Loss: 2.3164820671081543\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.6895174980163574, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.2388265132904053, critial timestamps: 1\n",
      "Epoch 16/80, Loss: 2.464171886444092\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.2055866718292236, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.102552652359009, critial timestamps: 1\n",
      "Epoch 17/80, Loss: 2.654069662094116\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.430974245071411, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.06010365486145, critial timestamps: 1\n",
      "Epoch 18/80, Loss: 2.7455389499664307\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.4309585094451904, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.025134325027466, critial timestamps: 1\n",
      "Epoch 19/80, Loss: 2.728046417236328\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.1934690475463867, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.2343392372131348, critial timestamps: 1\n",
      "Epoch 20/80, Loss: 2.2139041423797607\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.6188803911209106, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.110023021697998, critial timestamps: 1\n",
      "Epoch 21/80, Loss: 1.8644516468048096\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.13112473487854, critial timestamps: 1\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.9156615734100342, critial timestamps: 1\n",
      "Epoch 22/80, Loss: 2.023393154144287\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.9602439403533936, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.9602580070495605, critial timestamps: 4\n",
      "Epoch 23/80, Loss: 2.9602508544921875\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.960476875305176, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.666041612625122, critial timestamps: 4\n",
      "Epoch 24/80, Loss: 2.3132591247558594\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3862390518188477, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.9517021179199219, critial timestamps: 4\n",
      "Epoch 25/80, Loss: 1.6689705848693848\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.8160295486450195, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.5891809463500977, critial timestamps: 4\n",
      "Epoch 26/80, Loss: 1.7026052474975586\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3028233051300049, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.2724295854568481, critial timestamps: 4\n",
      "Epoch 27/80, Loss: 1.2876265048980713\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.2603172063827515, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.2603232860565186, critial timestamps: 4\n",
      "Epoch 28/80, Loss: 1.2603201866149902\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.0191502571105957, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.742877721786499, critial timestamps: 4\n",
      "Epoch 29/80, Loss: 1.8810139894485474\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.7080062627792358, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.6444121599197388, critial timestamps: 4\n",
      "Epoch 30/80, Loss: 1.6762092113494873\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.5572309494018555, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.18019437789917, critial timestamps: 4\n",
      "Epoch 31/80, Loss: 1.3687126636505127\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.1072380542755127, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.691112756729126, critial timestamps: 4\n",
      "Epoch 32/80, Loss: 2.3991754055023193\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.5460205078125, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.6906898021697998, critial timestamps: 4\n",
      "Epoch 33/80, Loss: 1.61835515499115\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.1559677124023438, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.524584174156189, critial timestamps: 4\n",
      "Epoch 34/80, Loss: 2.340276002883911\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.0414975881576538, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.4994783401489258, critial timestamps: 4\n",
      "Epoch 35/80, Loss: 1.2704880237579346\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.5243968963623047, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.4613804817199707, critial timestamps: 4\n",
      "Epoch 36/80, Loss: 1.4928886890411377\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.343144416809082, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.1287827491760254, critial timestamps: 4\n",
      "Epoch 37/80, Loss: 1.7359635829925537\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.6458371877670288, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.0020831823349, critial timestamps: 4\n",
      "Epoch 38/80, Loss: 1.3239601850509644\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 0.9865639209747314, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.4468915462493896, critial timestamps: 4\n",
      "Epoch 39/80, Loss: 1.2167277336120605\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.567284107208252, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.540268063545227, critial timestamps: 4\n",
      "Epoch 40/80, Loss: 1.5537760257720947\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.387263536453247, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.5340745449066162, critial timestamps: 4\n",
      "Epoch 41/80, Loss: 1.4606690406799316\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.8511096239089966, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.5845867395401, critial timestamps: 4\n",
      "Epoch 42/80, Loss: 1.7178481817245483\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.6632487773895264, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.5461171865463257, critial timestamps: 4\n",
      "Epoch 43/80, Loss: 1.6046829223632812\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.6082267761230469, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3111814260482788, critial timestamps: 4\n",
      "Epoch 44/80, Loss: 1.4597041606903076\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.2929929494857788, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.6437445878982544, critial timestamps: 4\n",
      "Epoch 45/80, Loss: 1.4683687686920166\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.502960205078125, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.2640317678451538, critial timestamps: 4\n",
      "Epoch 46/80, Loss: 1.8834960460662842\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.2256946563720703, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.5997618436813354, critial timestamps: 4\n",
      "Epoch 47/80, Loss: 1.4127283096313477\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.2138409614562988, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.4388720989227295, critial timestamps: 4\n",
      "Epoch 48/80, Loss: 2.3263564109802246\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.4764502048492432, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.162473201751709, critial timestamps: 4\n",
      "Epoch 49/80, Loss: 1.819461703300476\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.1113710403442383, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.0070784091949463, critial timestamps: 4\n",
      "Epoch 50/80, Loss: 1.0592247247695923\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.7467421293258667, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.07489812374115, critial timestamps: 4\n",
      "Epoch 51/80, Loss: 1.4108201265335083\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.4330426454544067, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.0309478044509888, critial timestamps: 4\n",
      "Epoch 52/80, Loss: 1.2319952249526978\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.4941192865371704, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.0004442930221558, critial timestamps: 4\n",
      "Epoch 53/80, Loss: 1.247281789779663\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.4834457635879517, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.6753393411636353, critial timestamps: 4\n",
      "Epoch 54/80, Loss: 1.5793925523757935\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.5188438892364502, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3807662725448608, critial timestamps: 4\n",
      "Epoch 55/80, Loss: 1.4498050212860107\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3199609518051147, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.8126143217086792, critial timestamps: 4\n",
      "Epoch 56/80, Loss: 1.566287636756897\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.0795778036117554, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.0795767307281494, critial timestamps: 4\n",
      "Epoch 57/80, Loss: 1.0795772075653076\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 0.8861730694770813, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3155009746551514, critial timestamps: 4\n",
      "Epoch 58/80, Loss: 1.100836992263794\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 0.8690042495727539, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.6623649597167969, critial timestamps: 4\n",
      "Epoch 59/80, Loss: 1.2656846046447754\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3935775756835938, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.0745906829833984, critial timestamps: 4\n",
      "Epoch 60/80, Loss: 1.734084129333496\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.352545976638794, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.5280416011810303, critial timestamps: 4\n",
      "Epoch 61/80, Loss: 1.940293788909912\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.4312570095062256, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 0.8221715688705444, critial timestamps: 4\n",
      "Epoch 62/80, Loss: 1.1267142295837402\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.5345380306243896, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.226872444152832, critial timestamps: 4\n",
      "Epoch 63/80, Loss: 1.3807052373886108\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 0.7998297810554504, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.648090124130249, critial timestamps: 4\n",
      "Epoch 64/80, Loss: 2.2239599227905273\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.1924701929092407, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.4931254386901855, critial timestamps: 4\n",
      "Epoch 65/80, Loss: 1.8427977561950684\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3626469373703003, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 3.662709951400757, critial timestamps: 4\n",
      "Epoch 66/80, Loss: 2.512678384780884\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3373538255691528, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.1278258562088013, critial timestamps: 4\n",
      "Epoch 67/80, Loss: 1.232589840888977\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.131333827972412, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 0.7809447050094604, critial timestamps: 4\n",
      "Epoch 68/80, Loss: 0.9561392664909363\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.514779806137085, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.4583516120910645, critial timestamps: 4\n",
      "Epoch 69/80, Loss: 1.9865657091140747\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3429174423217773, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.1210130453109741, critial timestamps: 4\n",
      "Epoch 70/80, Loss: 1.2319653034210205\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.1078330278396606, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.106455683708191, critial timestamps: 4\n",
      "Epoch 71/80, Loss: 1.1071443557739258\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 0.7955368161201477, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.00384783744812, critial timestamps: 4\n",
      "Epoch 72/80, Loss: 1.3996922969818115\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.351332664489746, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 0.7979524731636047, critial timestamps: 4\n",
      "Epoch 73/80, Loss: 1.574642539024353\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.324364185333252, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.4622012376785278, critial timestamps: 4\n",
      "Epoch 74/80, Loss: 1.8932826519012451\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3004932403564453, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.0453964471817017, critial timestamps: 4\n",
      "Epoch 75/80, Loss: 1.1729447841644287\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3184778690338135, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.076338529586792, critial timestamps: 4\n",
      "Epoch 76/80, Loss: 1.1974081993103027\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.0659502744674683, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.0152294635772705, critial timestamps: 4\n",
      "Epoch 77/80, Loss: 1.0405898094177246\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 0.9957109093666077, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 0.8247103095054626, critial timestamps: 4\n",
      "Epoch 78/80, Loss: 0.9102106094360352\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 2.3725085258483887, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.0326581001281738, critial timestamps: 4\n",
      "Epoch 79/80, Loss: 1.7025833129882812\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 0.845552384853363, critial timestamps: 4\n",
      " - Control Ratio: 0.1666666716337204 perplexity: 1.3108924627304077, critial timestamps: 4\n",
      "Epoch 80/80, Loss: 1.078222393989563\n"
     ]
    }
   ],
   "source": [
    "# Abstract learning for GAT (one batch ver.)\n",
    "# --------------------------------------------------------------------------------------------------------------------------\n",
    "from search import eval_hseq\n",
    "\n",
    "optimizer = Adam(gat.parameters(), lr=1e-3)\n",
    "t_search = 2\n",
    "critical_ts = None\n",
    "temperature = 1.0\n",
    "epochs = 80\n",
    "\n",
    "# Issue with training loop: \n",
    "# 1. Control ratio is not 1.0, this might not be possible when temperature remains at 1.0 -- how about dropping temperature gradually? \n",
    "# 2. \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    pad_abstract_tokens(batch_data, critical_ts, t_search)\n",
    "    gat.generate(batch_data, parallel=True, temperature=temperature)\n",
    "    # Error must be here, ts computation logic is off\n",
    "    p_per_sample, critical_ts, cr, ppt = eval_hseq(gat, batch_data, p_thres=0.5)\n",
    "    for b in range(batch_data.batch_size): \n",
    "        print(f\" - Control Ratio: {cr[b]} perplexity: {p_per_sample[b]}, critial timestamps: {critical_ts[b]}\")\n",
    "\n",
    "    loss = gat(batch_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm: \n",
    "1. Naive Search: every time we search from scratch, generate abstraction for certain timesteps and learn. Rely on model's growing capacity to increase the search timestamps nextly. This means no storing, no caching of intermediate results, most similar to GRPO, but with special treatment for incremental searching mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions \n",
    "# ----------------------------------\n",
    "\n",
    "def pad_abstract_tokens(batch_data: HierSeq, t_search: int = 8): \n",
    "    abstract_mask = (batch_data.levels > 0)\n",
    "    assert not abstract_mask.any(), \" - Abstract tokens already exist, 'pad_abstract_tokens' requires no abstract tokens\"\n",
    "\n",
    "    for sample_idx in batch_data.indices: \n",
    "        sample_mask = batch_data.sample_idx == sample_idx\n",
    "        sample_timestamps = batch_data.timestamps[sample_mask]\n",
    "        start_ts, end_ts = sample_timestamps[0], sample_timestamps[-1]\n",
    "\n",
    "        for l in range(1, batch_data.L): \n",
    "            abs_tok_ts = torch.arange(start_ts - 1, end_ts, batch_data.K ** l)\n",
    "            batch_data.insert_tokens(sample_idx, MASK_TOK, l, abs_tok_ts[abs_tok_ts >= start_ts])\n",
    "\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "def repeat_hseq(batch_data: HierSeq, n_copies: int): \n",
    "\n",
    "    original_batch_size = batch_data.batch_size\n",
    "\n",
    "    replicated_tokens = batch_data.tokens.repeat(n_copies)\n",
    "    replicated_levels = batch_data.levels.repeat(n_copies)  \n",
    "    replicated_timestamps = batch_data.timestamps.repeat(n_copies)\n",
    "\n",
    "    tokens_per_sample = torch.bincount(batch_data.sample_idx)[batch_data.indices]  # [num_tokens_sample_0, num_tokens_sample_1, ...]\n",
    "    repeats = tokens_per_sample.repeat(n_copies) \n",
    "    new_sample_idx = torch.repeat_interleave(\n",
    "        torch.arange(original_batch_size * n_copies),\n",
    "        repeats\n",
    "    )\n",
    "\n",
    "    unique_original_indices = batch_data.sample_idx.unique(sorted=True)  # Get actual unique sample indices\n",
    "    idx_map = {}\n",
    "    for rep_idx, orig_idx in enumerate(unique_original_indices.repeat(n_copies)): \n",
    "        idx_map[rep_idx] = orig_idx.item()\n",
    "        \n",
    "    replicated_batch_data = HierSeq(\n",
    "        tokens=replicated_tokens,\n",
    "        levels=replicated_levels,\n",
    "        timestamps=replicated_timestamps,\n",
    "        sample_idx=new_sample_idx,\n",
    "        batch_size=original_batch_size * n_copies,\n",
    "        K=batch_data.K,\n",
    "        L=batch_data.L,\n",
    "        idx_map=idx_map\n",
    "    )\n",
    "\n",
    "    return replicated_batch_data\n",
    "\n",
    "def compute_per_sample_rewards(level_ppt: torch.Tensor, level_idx: torch.Tensor): \n",
    "    \n",
    "    unique_samples, inverse = torch.unique(level_idx, return_inverse=True)\n",
    "\n",
    "    n_unique = len(unique_samples)\n",
    "    sums = torch.zeros(n_unique).scatter_add_(0, inverse, level_ppt)\n",
    "    counts = torch.bincount(inverse, minlength=n_unique).float()\n",
    "    averages = - sums / counts\n",
    "\n",
    "    lookup = {s.item(): avg for s, avg in zip(unique_samples, averages)}\n",
    "    return lookup \n",
    "\n",
    "def compute_hierarchical_rewards(ppt: torch.Tensor, repeat_batch: HierSeq) -> dict: \n",
    "    \"\"\" \n",
    "    abstraction level l is rewarded when it improves the perplexity of level l-1 tokens\n",
    "    \"\"\"\n",
    "    traj_mask = (repeat_batch.levels[1:] == 0) & (repeat_batch.timestamps[1:] > 0)\n",
    "    traj_ppt = ppt[traj_mask]\n",
    "    traj_idx = repeat_batch.sample_idx[1:][traj_mask]\n",
    "\n",
    "    reward_lookups = {}\n",
    "\n",
    "    reward_lookups[0] = None\n",
    "\n",
    "    current_level_ppt = traj_ppt\n",
    "    current_level_idx = traj_idx\n",
    "\n",
    "    for level in range(1, repeat_batch.L):\n",
    "        # Compute per-sample average of current level\n",
    "        sample_rewards = compute_per_sample_rewards(current_level_ppt, current_level_idx)\n",
    "        reward_lookups[level] = sample_rewards\n",
    "        \n",
    "        # Prepare for next level (if exists)\n",
    "        if level < repeat_batch.L - 1:\n",
    "            next_mask = (repeat_batch.levels[1:] == level) & (repeat_batch.timestamps[1:] > 0)\n",
    "            current_level_ppt = ppt[next_mask]\n",
    "            current_level_idx = repeat_batch.sample_idx[1:][next_mask]\n",
    "\n",
    "    return reward_lookups \n",
    "\n",
    "\n",
    "def compute_grouped_advantage(values: torch.Tensor, indices: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Grouped advantage using indices to group values tensor\"\"\"\n",
    "    \n",
    "    unique, inverse = torch.unique(indices, return_inverse=True)\n",
    "    n = len(unique)\n",
    "\n",
    "    means = torch.zeros(n).scatter_add_(0, inverse, values) / torch.bincount(inverse).float()\n",
    "    vars = torch.zeros(n).scatter_add_(0, inverse, values**2) / torch.bincount(inverse).float() - means**2\n",
    "    stds = torch.sqrt(torch.clamp(vars, min=0))\n",
    "\n",
    "    advantages = (values - means[inverse]) / (stds[inverse] + 1e-4)\n",
    "\n",
    "    return advantages\n",
    "\n",
    "def _compute_log_probs(ppt: torch.Tensor, repeat_batch: HierSeq) -> list:\n",
    "    old_log_probs = [[]]\n",
    "    for l in range(1, repeat_batch.levels.max() + 1):\n",
    "        level_ppt_mask = (repeat_batch.levels[1:] == l) & (repeat_batch.timestamps[1:] > 0)\n",
    "        old_level_ppt = ppt[level_ppt_mask] # per-token-log-probability of un-updated model\n",
    "        old_log_probs.append(old_level_ppt)\n",
    "    return old_log_probs\n",
    "\n",
    "\n",
    "def compute_grpo_loss(repeat_batch: HierSeq, ppt: torch.Tensor, old_log_probs: list,\n",
    "                      epsilon: float = 0.2):\n",
    "        \n",
    "    # per-level sample_idx->reward lookup table | detach makes sense?\n",
    "    reward_lookups = compute_hierarchical_rewards(ppt.detach(), repeat_batch)\n",
    "\n",
    "    loss = torch.tensor(0.0, device=repeat_batch.tokens.device)\n",
    "\n",
    "    for l in range(1, repeat_batch.L): \n",
    "        level_ppt_mask = (repeat_batch.levels[1:] == l) & (repeat_batch.timestamps[1:] > 0)\n",
    "        new_level_ppt = ppt[level_ppt_mask] \n",
    "        old_level_ppt = old_log_probs[l] # loaded from rollout data\n",
    "\n",
    "        if old_level_ppt is None: \n",
    "            continue \n",
    "\n",
    "        # Compute level l reward for each sample with abstraction at level l\n",
    "        level_sample_idx = repeat_batch.sample_idx[1:][level_ppt_mask]\n",
    "        sample_with_level_l = repeat_batch.indices[torch.isin(repeat_batch.indices, level_sample_idx)]\n",
    "\n",
    "        sample_level_rewards = torch.tensor([reward_lookups[l][idx.item()] for idx in sample_with_level_l])\n",
    "        orig_idx = torch.tensor([repeat_batch.idx_map[idx.item()] for idx in sample_with_level_l])\n",
    "\n",
    "        advantages = compute_grouped_advantage(sample_level_rewards, orig_idx)\n",
    "\n",
    "        advantages = advantages[level_sample_idx] # broadcast to each abstract token at level l\n",
    "        ratio = torch.exp(new_level_ppt - old_level_ppt)\n",
    "\n",
    "        # Compute surrogate loss\n",
    "        surrogate_loss = torch.min(ratio * advantages, torch.clamp(ratio, 1-epsilon, 1+epsilon) * advantages)\n",
    "\n",
    "        loss_abs_l = - surrogate_loss.mean()\n",
    "        loss += loss_abs_l\n",
    "\n",
    "    return loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surrogate loss computation refernece (GRPO)\n",
    "# - we need reward for each abstract token\n",
    "# - we compute avg reward for each sample (over all its abstract tokens)\n",
    "# - we compute mean / std of rewards for multiple abstractions corresponding to the same original sample\n",
    "# - we calculate advantage term for each repeated sample\n",
    "# - this provides 'abstraction sequence' level advantage\n",
    "# - compute policy gradient loss for each abstraction token\n",
    "\n",
    "# Question: can we compute per-abstract-token advantage? Well we have but one sample in terms of each abstract token \n",
    "#           so advantage can't be approxiamted here. \n",
    "\n",
    "# Note: GRPO requires a reference model to avoid strong deviation\n",
    "\n",
    "# We shall compute a reference free (kl regularization free) loss first in here. \n",
    "\n",
    "# Note: we need to store 'rollout data' and 'old-log-probs' for each sample .... how is that possible with new abstractions?\n",
    "\n",
    "# ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "# rewards = rewards.view(batch_size, num_generations)\n",
    "# avg_reward = rewards.mean().item() \n",
    "# mean_rewards = rewards.mean(dim=1).repeat_interleave(num_generations)\n",
    "# std_rewards = rewards.std(dim=1).repeat_interleave(num_generations)\n",
    "# advantages = ((rewards.view(-1) - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1)\n",
    "# surrogate_loss = torch.min(ratio * advantages, torch.clamp(ratio, 1-epsilon, 1+epsilon) * advantages)\n",
    "# kl = torch.exp(ref_log_probs - new_log_probs) - (ref_log_probs - new_log_probs) - 1\n",
    "# per_token_loss = surrogate_loss - beta * kl\n",
    "# loss = - ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "\n",
    "\n",
    "# ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "# surrogate_loss = torch.min(ratio * advantages, torch.clamp(ratio, 1-epsilon, 1+epsilon) * advantages)\n",
    "# loss = - ((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (I). \"generate_rollout_data\" function\n",
    "#    - generate rollout, compute (detached) old_log_probs for each level's abstraction\n",
    "\n",
    "n = 4 # number of generations for each sample\n",
    "t_search = 4 \n",
    "temperature = 1.0\n",
    "\n",
    "batch_data.sample_idx += 100\n",
    "\n",
    "repeat_batch = repeat_hseq(batch_data, n)\n",
    "\n",
    "pad_abstract_tokens(repeat_batch, t_search) \n",
    "\n",
    "# generate rollout \n",
    "repeat_batch = gat.generate(repeat_batch, parallel=True, temperature=temperature)\n",
    "\n",
    "# compute log_probs\n",
    "rep = gat.propagate(repeat_batch, return_attn=False)\n",
    "ppt = gat._compute_ppt(rep, repeat_batch)\n",
    "old_log_probs = _compute_log_probs(ppt, repeat_batch) # per-level log probs\n",
    "\n",
    "# (TBD). Store rollout data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting difference w.r.t. GRPO is 'reward' is basically ppl for suffix trajectory tokens, model can maximize its reward, besides picking better \n",
    "action etc. Loss for abstraction are loss for action, using surrogate one makes sense, on the other hand, loss for trajectory should be normal CE loss. \n",
    "We do SSL on trajectory token and RL on abstract token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need a 'generate_rollout_data' function that \n",
    "- generate different abstractions & compute 'old_log_probs'\n",
    "\n",
    "\n",
    "and also a 'compute_grpo_loss' function that\n",
    "- compute grpo loss by taking rollout & old_log_probs inside\n",
    "\n",
    "\n",
    "Thought: I don't need completion_mask, each abstract token needs to be trained. Pre-training RL has no reference model, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (II). \"compute_grpo_loss\" function | load rollout data & old_log_probs\n",
    "epsilon = 0.2 \n",
    "\n",
    "rep = gat.propagate(repeat_batch, return_attn=False)\n",
    "ppt = gat._compute_ppt(rep, repeat_batch)\n",
    "\n",
    "# sum over abstraction levels\n",
    "grpo_loss = compute_grpo_loss(repeat_batch, ppt, old_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.nbody import * \n",
    "from utils import * \n",
    "from buffer import * \n",
    "from model import GATConfig, GAT, HierSeq\n",
    "from torch.optim import Adam \n",
    "\n",
    "L, K = 3, 2\n",
    "config = GATConfig(K=K, L=L, n_embd=128, n_head=4, device=\"cpu\", _compile=False)\n",
    "gat = GAT(config)\n",
    "\n",
    "# Build Dataset \n",
    "dataset = create_dataset_with_params(\n",
    "    n_bodies=2,\n",
    "    patterns=['cartesian'],\n",
    "    n_context=3,\n",
    "    stride=1,\n",
    "    T=10,\n",
    "    include_masses=True,\n",
    ")\n",
    "\n",
    "# Initialize Tokenizer \n",
    "tokenizer = TinyTokenizer()\n",
    "\n",
    "# Initialize Buffer\n",
    "samples, timestamps = [], [] \n",
    "for seq in dataset['sequences']:\n",
    "    s = tokenizer(seq)\n",
    "    ts = [[t for t in range(1, len(s)+1)]] + [[] for l in range(1, config.L)]\n",
    "    s = [s] + [[] for l in range(1, config.L)]\n",
    "    samples.append((s))\n",
    "    timestamps.append(ts)\n",
    "\n",
    "# pop_size = 2\n",
    "# write_shard('dataset/nbody/buffer_b.bin', samples[:pop_size], timestamps[:pop_size])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
