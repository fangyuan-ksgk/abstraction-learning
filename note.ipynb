{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{SoRL (GAT)}$\n",
    "1. Group advantage computation \n",
    "2. Surrogate loss computation\n",
    "\n",
    "The key for learning from experience is learning from failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 621700.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 sequences to dataset/multiplication/2K-123.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataset.arithmetic import ArithmeticDataset\n",
    "\n",
    "dataset = ArithmeticDataset(\n",
    "    min_digit=1,\n",
    "    max_digit=3,\n",
    "    num_data=2000,\n",
    "    filepath=\"dataset/multiplication/2K-123.bin\"\n",
    ").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized GAT model\n"
     ]
    }
   ],
   "source": [
    "from src import GATConfig, GAT\n",
    "from src.sorl import SORLConfig\n",
    "import torch \n",
    "from dataset.arithmetic import ArithmeticDataset\n",
    "\n",
    "dataset = ArithmeticDataset.from_file(\"dataset/multiplication/2K-123.bin\")\n",
    "\n",
    "# 1. Setup a dummy model and data\n",
    "config = GATConfig(L=2, K=3, vocab_size_list=[dataset.vocab_size_list[0], 5], device='cpu')\n",
    "model = GAT(config)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "print(f\"Initialized GAT model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import infer_level\n",
    "from copy import deepcopy \n",
    "from src.sorl import prep_denoise\n",
    "from dataset.base import get_batch\n",
    "\n",
    "data = get_batch(dataset, batch_size=10, max_length=1024, pad_token_id=model.level_mask_tokens[0])\n",
    "\n",
    "# forward propagation : compute perplexity per token (traj & abstract)\n",
    "idx = data[:, :-1].contiguous().clone()\n",
    "target = data[:, 1:].contiguous().clone() \n",
    "ppt = model(idx, target)\n",
    "\n",
    "# causal generate return next_token (1 per sample)\n",
    "idx = data.contiguous()\n",
    "next_idx, kv_cache, levels = model.generate(idx, temperature=0.0)\n",
    "next_idx, kv_cache, levels = model.generate(next_idx.unsqueeze(1), temperature=0.0, kv_cache=kv_cache, levels=levels)\n",
    "\n",
    "# parallel denoise (return updated token sequence) \n",
    "levels = infer_level(idx, model.vocab_sizes, model.level_mask_tokens[0])\n",
    "denoise_mask = levels.bool() # toy denoise mask\n",
    "denoise_mask[1, 0] = True \n",
    "denoise_mask[1, 1] = True \n",
    "\n",
    "from src import pad_abstract_tokens\n",
    "# denoise | it conduct in-place update\n",
    "denoise_idx = deepcopy(idx)\n",
    "l = 1\n",
    "denoise_idx = pad_abstract_tokens(denoise_idx, model, l, use_rhythmic_placeholders=True)\n",
    "denoise_mask, denoise_levels = prep_denoise(denoise_idx, model)\n",
    "\n",
    "denoise_idx = model.denoise(denoise_idx, denoise_mask, denoise_levels, temperature=0.0) # denoise return an updated idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# sorl_search(data, model, sorl_config)\n",
    "# from src.sorl import heuristic_rollout, chunk_denoise, infer_timestamp\n",
    "# search_data, search_data_idx = heuristic_rollout(data, model, l=config.l, n=config.n-1, temperature=config.temperature, steps=config.steps, max_t_search=config.max_t_search, start_ts=config.start_ts, end_ts=config.end_ts, use_spike_placeholders=config.use_spike_placeholders, abstract_budget=config.abstract_budget, use_rhythmic_placeholders=config.use_rhythmic_placeholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100, loss: 4.9127, abs_loss: 2.0794, ssl_loss: 2.8332\n",
      "Iteration 2/100, loss: 4.7362, abs_loss: 1.9370, ssl_loss: 2.7991\n",
      "Iteration 3/100, loss: 4.4892, abs_loss: 1.7341, ssl_loss: 2.7552\n",
      "Iteration 4/100, loss: 4.2146, abs_loss: 1.5094, ssl_loss: 2.7052\n",
      "Iteration 5/100, loss: 3.9717, abs_loss: 1.3026, ssl_loss: 2.6691\n",
      "Iteration 6/100, loss: 3.7408, abs_loss: 1.1191, ssl_loss: 2.6218\n",
      "Iteration 7/100, loss: 3.5362, abs_loss: 0.9555, ssl_loss: 2.5807\n",
      "Iteration 8/100, loss: 3.3784, abs_loss: 0.8085, ssl_loss: 2.5699\n",
      "Iteration 9/100, loss: 3.2184, abs_loss: 0.6780, ssl_loss: 2.5404\n",
      "Iteration 10/100, loss: 3.0805, abs_loss: 0.5644, ssl_loss: 2.5161\n",
      "Iteration 11/100, loss: 2.9727, abs_loss: 0.4672, ssl_loss: 2.5055\n",
      "Iteration 12/100, loss: 2.8609, abs_loss: 0.3852, ssl_loss: 2.4758\n",
      "Iteration 13/100, loss: 2.7815, abs_loss: 0.3173, ssl_loss: 2.4642\n",
      "Iteration 14/100, loss: 2.7241, abs_loss: 0.2617, ssl_loss: 2.4625\n",
      "Iteration 15/100, loss: 2.6535, abs_loss: 0.2164, ssl_loss: 2.4371\n",
      "Iteration 16/100, loss: 2.6034, abs_loss: 0.1799, ssl_loss: 2.4236\n",
      "Iteration 17/100, loss: 2.5715, abs_loss: 0.1504, ssl_loss: 2.4211\n",
      "Iteration 18/100, loss: 2.5845, abs_loss: 0.1267, ssl_loss: 2.4578\n",
      "Iteration 19/100, loss: 2.5641, abs_loss: 0.1076, ssl_loss: 2.4565\n",
      "Iteration 20/100, loss: 2.5013, abs_loss: 0.0923, ssl_loss: 2.4090\n",
      "Iteration 21/100, loss: 2.5002, abs_loss: 0.0800, ssl_loss: 2.4202\n",
      "Iteration 22/100, loss: 2.4817, abs_loss: 0.0702, ssl_loss: 2.4116\n",
      "Iteration 23/100, loss: 2.4443, abs_loss: 0.0625, ssl_loss: 2.3818\n",
      "Iteration 24/100, loss: 2.4127, abs_loss: 0.0571, ssl_loss: 2.3557\n",
      "Iteration 25/100, loss: 2.3909, abs_loss: 0.0548, ssl_loss: 2.3362\n",
      "Iteration 26/100, loss: 2.3698, abs_loss: 0.0580, ssl_loss: 2.3118\n",
      "Iteration 27/100, loss: 2.3406, abs_loss: 0.0682, ssl_loss: 2.2724\n",
      "Iteration 28/100, loss: 2.3350, abs_loss: 0.0759, ssl_loss: 2.2591\n",
      "Iteration 29/100, loss: 2.3110, abs_loss: 0.0749, ssl_loss: 2.2361\n",
      "Iteration 30/100, loss: 2.2971, abs_loss: 0.0671, ssl_loss: 2.2300\n",
      "Iteration 31/100, loss: 2.2696, abs_loss: 0.0614, ssl_loss: 2.2081\n",
      "Iteration 32/100, loss: 2.2340, abs_loss: 0.0585, ssl_loss: 2.1755\n",
      "Iteration 33/100, loss: 2.1928, abs_loss: 0.0580, ssl_loss: 2.1348\n",
      "Iteration 34/100, loss: 2.1545, abs_loss: 0.0579, ssl_loss: 2.0966\n",
      "Iteration 35/100, loss: 2.1315, abs_loss: 0.0521, ssl_loss: 2.0794\n",
      "Iteration 36/100, loss: 2.1179, abs_loss: 0.0470, ssl_loss: 2.0709\n",
      "Iteration 37/100, loss: 2.1086, abs_loss: 0.0387, ssl_loss: 2.0699\n",
      "Iteration 38/100, loss: 2.0646, abs_loss: 0.0330, ssl_loss: 2.0316\n",
      "Iteration 39/100, loss: 2.0454, abs_loss: 0.0351, ssl_loss: 2.0103\n",
      "Iteration 40/100, loss: 2.0231, abs_loss: 0.0392, ssl_loss: 1.9839\n",
      "Iteration 41/100, loss: 1.9984, abs_loss: 0.0415, ssl_loss: 1.9569\n",
      "Iteration 42/100, loss: 1.9670, abs_loss: 0.0415, ssl_loss: 1.9255\n",
      "Iteration 43/100, loss: 1.9091, abs_loss: 0.0348, ssl_loss: 1.8743\n",
      "Iteration 44/100, loss: 1.9043, abs_loss: 0.0348, ssl_loss: 1.8695\n",
      "Iteration 45/100, loss: 1.8752, abs_loss: 0.0376, ssl_loss: 1.8376\n",
      "Iteration 46/100, loss: 1.8536, abs_loss: 0.0394, ssl_loss: 1.8142\n",
      "Iteration 47/100, loss: 1.8112, abs_loss: 0.0357, ssl_loss: 1.7755\n",
      "Iteration 48/100, loss: 1.8255, abs_loss: 0.0337, ssl_loss: 1.7918\n",
      "Iteration 49/100, loss: 1.7972, abs_loss: 0.0298, ssl_loss: 1.7675\n",
      "Iteration 50/100, loss: 1.7900, abs_loss: 0.0246, ssl_loss: 1.7654\n",
      "Iteration 51/100, loss: 1.7748, abs_loss: 0.0204, ssl_loss: 1.7544\n",
      "Iteration 52/100, loss: 1.7417, abs_loss: 0.0211, ssl_loss: 1.7206\n",
      "Iteration 53/100, loss: 1.7457, abs_loss: 0.0234, ssl_loss: 1.7224\n",
      "Iteration 54/100, loss: 1.7216, abs_loss: 0.0219, ssl_loss: 1.6997\n",
      "Iteration 55/100, loss: 1.7277, abs_loss: 0.0199, ssl_loss: 1.7079\n",
      "Iteration 56/100, loss: 1.7184, abs_loss: 0.0228, ssl_loss: 1.6955\n",
      "Iteration 57/100, loss: 1.6990, abs_loss: 0.0195, ssl_loss: 1.6795\n",
      "Iteration 58/100, loss: 1.6785, abs_loss: 0.0156, ssl_loss: 1.6629\n",
      "Iteration 59/100, loss: 1.6704, abs_loss: 0.0141, ssl_loss: 1.6563\n",
      "Iteration 60/100, loss: 1.6587, abs_loss: 0.0138, ssl_loss: 1.6449\n",
      "Iteration 61/100, loss: 1.6630, abs_loss: 0.0148, ssl_loss: 1.6482\n",
      "Iteration 62/100, loss: 1.6686, abs_loss: 0.0155, ssl_loss: 1.6531\n",
      "Iteration 63/100, loss: 1.6346, abs_loss: 0.0158, ssl_loss: 1.6188\n",
      "Iteration 64/100, loss: 1.6457, abs_loss: 0.0138, ssl_loss: 1.6318\n",
      "Iteration 65/100, loss: 1.6293, abs_loss: 0.0131, ssl_loss: 1.6162\n",
      "Iteration 66/100, loss: 1.5948, abs_loss: 0.0114, ssl_loss: 1.5834\n",
      "Iteration 67/100, loss: 1.6046, abs_loss: 0.0102, ssl_loss: 1.5944\n",
      "Iteration 68/100, loss: 1.5687, abs_loss: 0.0100, ssl_loss: 1.5586\n",
      "Iteration 69/100, loss: 1.5944, abs_loss: 0.0107, ssl_loss: 1.5837\n",
      "Iteration 70/100, loss: 1.5895, abs_loss: 0.0114, ssl_loss: 1.5782\n",
      "Iteration 71/100, loss: 1.5754, abs_loss: 0.0119, ssl_loss: 1.5635\n",
      "Iteration 72/100, loss: 1.5757, abs_loss: 0.0116, ssl_loss: 1.5640\n",
      "Iteration 73/100, loss: 1.5537, abs_loss: 0.0101, ssl_loss: 1.5435\n",
      "Iteration 74/100, loss: 1.5492, abs_loss: 0.0109, ssl_loss: 1.5383\n",
      "Iteration 75/100, loss: 1.5599, abs_loss: 0.0116, ssl_loss: 1.5484\n",
      "Iteration 76/100, loss: 1.5571, abs_loss: 0.0126, ssl_loss: 1.5446\n",
      "Iteration 77/100, loss: 1.5394, abs_loss: 0.0131, ssl_loss: 1.5263\n",
      "Iteration 78/100, loss: 1.5390, abs_loss: 0.0121, ssl_loss: 1.5269\n",
      "Iteration 79/100, loss: 1.5456, abs_loss: 0.0104, ssl_loss: 1.5352\n",
      "Iteration 80/100, loss: 1.5116, abs_loss: 0.0097, ssl_loss: 1.5019\n",
      "Iteration 81/100, loss: 1.5463, abs_loss: 0.0092, ssl_loss: 1.5370\n",
      "Iteration 82/100, loss: 1.5069, abs_loss: 0.0087, ssl_loss: 1.4982\n",
      "Iteration 83/100, loss: 1.5160, abs_loss: 0.0092, ssl_loss: 1.5067\n",
      "Iteration 84/100, loss: 1.4968, abs_loss: 0.0086, ssl_loss: 1.4882\n",
      "Iteration 85/100, loss: 1.5397, abs_loss: 0.0079, ssl_loss: 1.5318\n",
      "Iteration 86/100, loss: 1.5272, abs_loss: 0.0076, ssl_loss: 1.5196\n",
      "Iteration 87/100, loss: 1.5037, abs_loss: 0.0073, ssl_loss: 1.4964\n",
      "Iteration 88/100, loss: 1.5219, abs_loss: 0.0069, ssl_loss: 1.5150\n",
      "Iteration 89/100, loss: 1.5130, abs_loss: 0.0067, ssl_loss: 1.5063\n",
      "Iteration 90/100, loss: 1.5185, abs_loss: 0.0068, ssl_loss: 1.5117\n",
      "Iteration 91/100, loss: 1.4859, abs_loss: 0.0068, ssl_loss: 1.4792\n",
      "Iteration 92/100, loss: 1.5157, abs_loss: 0.0069, ssl_loss: 1.5088\n",
      "Iteration 93/100, loss: 1.5358, abs_loss: 0.0065, ssl_loss: 1.5293\n",
      "Iteration 94/100, loss: 1.5017, abs_loss: 0.0062, ssl_loss: 1.4955\n",
      "Iteration 95/100, loss: 1.4980, abs_loss: 0.0061, ssl_loss: 1.4920\n",
      "Iteration 96/100, loss: 1.5126, abs_loss: 0.0059, ssl_loss: 1.5068\n",
      "Iteration 97/100, loss: 1.5113, abs_loss: 0.0057, ssl_loss: 1.5056\n",
      "Iteration 98/100, loss: 1.5152, abs_loss: 0.0055, ssl_loss: 1.5097\n",
      "Iteration 99/100, loss: 1.4803, abs_loss: 0.0054, ssl_loss: 1.4749\n",
      "Iteration 100/100, loss: 1.4921, abs_loss: 0.0055, ssl_loss: 1.4866\n"
     ]
    }
   ],
   "source": [
    "# Record & Save an annotated dataset\n",
    "\n",
    "# from dataset.base import BaseHierDataset\n",
    "from dataset.arithmetic import ArithmeticHierDataset\n",
    "from nil import annotate_abstraction\n",
    "from nil import supervise_gat \n",
    "\n",
    "record_dataset = ArithmeticHierDataset.from_dataset(dataset)\n",
    "\n",
    "# Greedy Abstraction Annotation (Passing knowledge to the next generation)\n",
    "# ------------------------------------------------------------------------\n",
    "record_dataset = annotate_abstraction(record_dataset, gat)\n",
    "\n",
    "\n",
    "# Reset GAT module\n",
    "# -------------------\n",
    "gat = GAT(gat_config)\n",
    "\n",
    "\n",
    "# Weak Supervision (GAT)\n",
    "# ------------------------------------------------------------------------\n",
    "weak_iterations = 100 # require tuning\n",
    "context_length = 1024\n",
    "\n",
    "supervised_gat = supervise_gat(record_dataset, gat, weak_iterations, context_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep it beautifully simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import sorl_search, SORLConfig\n",
    "from dataset.arithmetic import ArithmeticDataset\n",
    "from src import GATConfig, GAT\n",
    "\n",
    "sorl_config = SORLConfig(\n",
    "    n = 2,\n",
    "    temperature = 0.75,   \n",
    "    # rollout specific \n",
    "    causal_rollout=False, \n",
    "    \n",
    "    l=1,\n",
    "    steps=1,\n",
    "    use_rhythmic_placeholders=True,\n",
    "    use_spike_placeholders=True,\n",
    "    abstract_budget=5,\n",
    "    max_t_search=5,\n",
    "\n",
    "    # dataset specific\n",
    "    train_dataset_path=\"dataset/multiplication/100K-123.bin\",\n",
    "    val_dataset_path=\"dataset/multiplication/2K-123.bin\",\n",
    "    train_batch_size=24,\n",
    "    val_batch_size=1,\n",
    "    train_iterations=400,\n",
    "    val_iterations=1,\n",
    "    # optimization\n",
    "    learning_rate=1e-3, \n",
    "    log_interval=10\n",
    ")\n",
    "\n",
    "train_dataset = ArithmeticDataset.from_file(sorl_config.train_dataset_path)\n",
    "val_dataset = ArithmeticDataset.from_file(sorl_config.val_dataset_path)\n",
    "traj_vocab_size = train_dataset.vocab_size_list[0]\n",
    "\n",
    "gat_config = GATConfig(K=3, L=2, n_embd=128, n_head=4, n_layer=4, device=\"cpu\", _compile=False,\n",
    "                       vocab_size_list=[traj_vocab_size, 8], t_keep=sorl_config.max_length)\n",
    "\n",
    "gat = GAT(gat_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/400 - loss: 3.1780, abs_loss: 0.0000, ssl_loss: 3.1780, search_ppl: 0.0002, switch_ratio: 0.0000, vocab_utilization_rate: 0.0000, t_search: 0\n",
      "Iteration 2/400 - loss: 3.1360, abs_loss: 0.0000, ssl_loss: 3.1360, search_ppl: -0.0000, switch_ratio: 0.0000, vocab_utilization_rate: 0.0000, t_search: 0\n",
      "Iteration 3/400 - loss: 3.0655, abs_loss: 0.0000, ssl_loss: 3.0655, search_ppl: -0.0002, switch_ratio: 0.0000, vocab_utilization_rate: 0.0000, t_search: 0\n",
      "Iteration 4/400 - loss: 2.9848, abs_loss: 0.0000, ssl_loss: 2.9848, search_ppl: 0.0000, switch_ratio: 0.0000, vocab_utilization_rate: 0.0000, t_search: 0\n",
      "Iteration 5/400 - loss: 2.8915, abs_loss: 0.0000, ssl_loss: 2.8915, search_ppl: -0.0001, switch_ratio: 0.0000, vocab_utilization_rate: 0.0000, t_search: 0\n",
      "Iteration 6/400 - loss: 2.8178, abs_loss: 0.0000, ssl_loss: 2.8178, search_ppl: -0.0002, switch_ratio: 0.0000, vocab_utilization_rate: 0.0000, t_search: 0\n",
      "Iteration 7/400 - loss: 2.7558, abs_loss: 0.0000, ssl_loss: 2.7558, search_ppl: -0.0002, switch_ratio: 0.0000, vocab_utilization_rate: 0.0000, t_search: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m gat\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[0;32m---> 44\u001b[0m     _, improve_ppl_train, _, vocab_utilization_rate \u001b[38;5;241m=\u001b[39m evaluate(data, gat, \u001b[38;5;241m5\u001b[39m, config)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mtrain_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, abs_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabs_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ssl_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mssl_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, search_ppl: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimprove_ppl_train\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, switch_ratio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mswitch_ratio\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, vocab_utilization_rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_utilization_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, t_search: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_search\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Implementation/abstraction-learning/src/sorl.py:302\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(data, model, n, config)\u001b[0m\n\u001b[1;32m    299\u001b[0m     search_data, _ \u001b[38;5;241m=\u001b[39m heuristic_rollout(data, model, l\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39ml, n\u001b[38;5;241m=\u001b[39mn\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.\u001b[39m, steps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39msteps, max_t_search\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_t_search, start_ts\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mstart_ts, end_ts\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mend_ts, use_spike_placeholders\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_spike_placeholders, abstract_budget\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mabstract_budget, use_rhythmic_placeholders\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_rhythmic_placeholders)\n\u001b[1;32m    301\u001b[0m greedy_ppt \u001b[38;5;241m=\u001b[39m model(greedy_data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous(), greedy_data[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[0;32m--> 302\u001b[0m search_ppt \u001b[38;5;241m=\u001b[39m model(search_data[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous(), search_data[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[1;32m    304\u001b[0m greedy_ppl \u001b[38;5;241m=\u001b[39m compute_loss(greedy_data, model, greedy_ppt)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    305\u001b[0m search_ppl \u001b[38;5;241m=\u001b[39m compute_loss(search_data, model, search_ppt)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Implementation/abstraction-learning/src/gat.py:74\u001b[0m, in \u001b[0;36mGAT.forward\u001b[0;34m(self, idx, target)\u001b[0m\n\u001b[1;32m     71\u001b[0m v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 74\u001b[0m     x, v1, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh[i](x, v1, x0, block_mask)\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m     77\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Implementation/abstraction-learning/src/model.py:132\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, v1, x0, block_mask, cache, cache_offset)\u001b[0m\n\u001b[1;32m    130\u001b[0m x_norm \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Pass the cache to the attention layer\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m x1, v1, new_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x_norm, v1, block_mask, cache\u001b[38;5;241m=\u001b[39mcache, cache_offset\u001b[38;5;241m=\u001b[39mcache_offset)\n\u001b[1;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdas[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdas[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m x0\n\u001b[1;32m    135\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x1\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Implementation/abstraction-learning/src/model.py:94\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, v1, block_mask, cache, cache_offset)\u001b[0m\n\u001b[1;32m     92\u001b[0m q, k \u001b[38;5;241m=\u001b[39m norm(q), norm(k) \u001b[38;5;66;03m# QK norm suggested by @Grad62304977\u001b[39;00m\n\u001b[1;32m     93\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary(q), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary(k)        \n\u001b[0;32m---> 94\u001b[0m y \u001b[38;5;241m=\u001b[39m flex_attention(\n\u001b[1;32m     95\u001b[0m     q\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     96\u001b[0m     k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     97\u001b[0m     v\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     98\u001b[0m     block_mask\u001b[38;5;241m=\u001b[39mblock_mask,\n\u001b[1;32m     99\u001b[0m     kernel_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflex_kernel_options\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    101\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview_as(x)       \n\u001b[1;32m    102\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y)       \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/attention/flex_attention.py:1353\u001b[0m, in \u001b[0;36mflex_attention\u001b[0;34m(query, key, value, score_mod, block_mask, scale, enable_gqa, return_lse, kernel_options)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1352\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1353\u001b[0m out, lse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1354\u001b[0m     _flex_attention_hop_wrapper, backend\u001b[38;5;241m=\u001b[39mbackend, fullgraph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m )(\n\u001b[1;32m   1356\u001b[0m     query,\n\u001b[1;32m   1357\u001b[0m     key,\n\u001b[1;32m   1358\u001b[0m     value,\n\u001b[1;32m   1359\u001b[0m     score_mod,\n\u001b[1;32m   1360\u001b[0m     block_mask\u001b[38;5;241m.\u001b[39mas_tuple(),  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m     scale,\n\u001b[1;32m   1362\u001b[0m     kernel_options,\n\u001b[1;32m   1363\u001b[0m )\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_lse:\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, lse \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:574\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    570\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    578\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    579\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/attention/flex_attention.py:1340\u001b[0m, in \u001b[0;36mflex_attention.<locals>._flex_attention_hop_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebugging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   1335\u001b[0m     make_eager_backend_with_torch_function_mode,\n\u001b[1;32m   1336\u001b[0m )\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;66;03m# Dynamo is expecting a callable with \"__code__\" attribute.\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;66;03m# We cannot directly pass hop to it. So we wrap it in a dummy function.\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flex_attention_hop_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flex_attention_hop(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_compilation_env():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m<eval_with_key>.19:21\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, s2, s3, L_args_0_, s4, s5, L_args_4_12_closure_0_cell_contents, s8, s9, L_args_1_, s12, s13, L_args_2_, L_args_4_0_, L_args_4_1_, L_args_4_2_, L_args_4_3_, L_args_4_4_, L_args_4_5_, L_args_4_6_, L_args_4_7_, L_args_4_8_, L_args_4_9_)\u001b[0m\n\u001b[1;32m     19\u001b[0m score_mod_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_mod_0\n\u001b[1;32m     20\u001b[0m mask_fn_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_fn_0\n\u001b[0;32m---> 21\u001b[0m flex_attention \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mhigher_order\u001b[38;5;241m.\u001b[39mflex_attention(l_args_0_, l_args_1_, l_args_2_, score_mod_0, (l_args_4_0_, l_args_4_1_, l_args_4_2_, l_args_4_3_, l_args_4_4_, l_args_4_5_, l_args_4_6_, l_args_4_7_, l_args_4_8_, l_args_4_9_, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, mask_fn_0), \u001b[38;5;241m0.17677669529663687\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRESCALE_QK\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROWS_GUARANTEED_SAFE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOCKS_ARE_CONTIGUOUS\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOUTPUT_LOGSUMEXP\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}, (), (s4, s5, l_args_4_12_closure_0_cell_contents));  l_args_0_ \u001b[38;5;241m=\u001b[39m l_args_1_ \u001b[38;5;241m=\u001b[39m l_args_2_ \u001b[38;5;241m=\u001b[39m score_mod_0 \u001b[38;5;241m=\u001b[39m l_args_4_0_ \u001b[38;5;241m=\u001b[39m l_args_4_1_ \u001b[38;5;241m=\u001b[39m l_args_4_2_ \u001b[38;5;241m=\u001b[39m l_args_4_3_ \u001b[38;5;241m=\u001b[39m l_args_4_4_ \u001b[38;5;241m=\u001b[39m l_args_4_5_ \u001b[38;5;241m=\u001b[39m l_args_4_6_ \u001b[38;5;241m=\u001b[39m l_args_4_7_ \u001b[38;5;241m=\u001b[39m l_args_4_8_ \u001b[38;5;241m=\u001b[39m l_args_4_9_ \u001b[38;5;241m=\u001b[39m mask_fn_0 \u001b[38;5;241m=\u001b[39m s4 \u001b[38;5;241m=\u001b[39m s5 \u001b[38;5;241m=\u001b[39m l_args_4_12_closure_0_cell_contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m getitem \u001b[38;5;241m=\u001b[39m flex_attention[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m getitem_1 \u001b[38;5;241m=\u001b[39m flex_attention[\u001b[38;5;241m1\u001b[39m];  flex_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:90\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[0;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     89\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     91\u001b[0m         query,\n\u001b[1;32m     92\u001b[0m         key,\n\u001b[1;32m     93\u001b[0m         value,\n\u001b[1;32m     94\u001b[0m         score_mod,\n\u001b[1;32m     95\u001b[0m         block_mask,\n\u001b[1;32m     96\u001b[0m         scale,\n\u001b[1;32m     97\u001b[0m         kernel_options,\n\u001b[1;32m     98\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m     99\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:436\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:302\u001b[0m, in \u001b[0;36mHigherOrderOperator.dispatch\u001b[0;34m(self, dispatch_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache[dispatch_key]\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kernel, DispatchKey)\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mFuncTorchDynamicLayerFrontMode:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_functorch(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:731\u001b[0m, in \u001b[0;36mflex_attention_autograd\u001b[0;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m         fw_graph, bw_graph \u001b[38;5;241m=\u001b[39m score_mod, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 731\u001b[0m     out, logsumexp \u001b[38;5;241m=\u001b[39m FlexAttentionAutogradOp\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    732\u001b[0m         query,\n\u001b[1;32m    733\u001b[0m         key,\n\u001b[1;32m    734\u001b[0m         value,\n\u001b[1;32m    735\u001b[0m         fw_graph,\n\u001b[1;32m    736\u001b[0m         bw_graph,\n\u001b[1;32m    737\u001b[0m         block_mask,\n\u001b[1;32m    738\u001b[0m         scale,\n\u001b[1;32m    739\u001b[0m         kernel_options,\n\u001b[1;32m    740\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;241m*\u001b[39mscore_mod_other_buffers,\n\u001b[1;32m    742\u001b[0m     )\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, logsumexp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:597\u001b[0m, in \u001b[0;36mFlexAttentionAutogradOp.forward\u001b[0;34m(ctx, query, key, value, fw_graph, joint_graph, block_mask, scale, kernel_options, mask_mod_other_buffers, *score_mod_other_buffers)\u001b[0m\n\u001b[1;32m    595\u001b[0m ctx\u001b[38;5;241m.\u001b[39m_score_mod_other_buffers_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(score_mod_other_buffers)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_AutoDispatchBelowAutograd():\n\u001b[0;32m--> 597\u001b[0m     out, logsumexp \u001b[38;5;241m=\u001b[39m flex_attention(\n\u001b[1;32m    598\u001b[0m         query,\n\u001b[1;32m    599\u001b[0m         key,\n\u001b[1;32m    600\u001b[0m         value,\n\u001b[1;32m    601\u001b[0m         fw_graph,\n\u001b[1;32m    602\u001b[0m         block_mask,\n\u001b[1;32m    603\u001b[0m         scale,\n\u001b[1;32m    604\u001b[0m         kernel_options,\n\u001b[1;32m    605\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m    606\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[1;32m    609\u001b[0m save_tensors_and_symints_for_backward(\n\u001b[1;32m    610\u001b[0m     ctx,\n\u001b[1;32m    611\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m     ),\n\u001b[1;32m    621\u001b[0m )\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, logsumexp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:90\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[0;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     89\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     91\u001b[0m         query,\n\u001b[1;32m     92\u001b[0m         key,\n\u001b[1;32m     93\u001b[0m         value,\n\u001b[1;32m     94\u001b[0m         score_mod,\n\u001b[1;32m     95\u001b[0m         block_mask,\n\u001b[1;32m     96\u001b[0m         scale,\n\u001b[1;32m     97\u001b[0m         kernel_options,\n\u001b[1;32m     98\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m     99\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:431\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    429\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m _to_flat_tuple(args, kwargs)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function(flat_args):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/overrides.py:1720\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1720\u001b[0m         result \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1722\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py:142\u001b[0m, in \u001b[0;36mTransformGetItemToIndex.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m index_args):\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mod_index(args[\u001b[38;5;241m0\u001b[39m], index_args)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:90\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[0;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     89\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     91\u001b[0m         query,\n\u001b[1;32m     92\u001b[0m         key,\n\u001b[1;32m     93\u001b[0m         value,\n\u001b[1;32m     94\u001b[0m         score_mod,\n\u001b[1;32m     95\u001b[0m         block_mask,\n\u001b[1;32m     96\u001b[0m         scale,\n\u001b[1;32m     97\u001b[0m         kernel_options,\n\u001b[1;32m     98\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m     99\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:436\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:302\u001b[0m, in \u001b[0;36mHigherOrderOperator.dispatch\u001b[0;34m(self, dispatch_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache[dispatch_key]\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kernel, DispatchKey)\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mFuncTorchDynamicLayerFrontMode:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_functorch(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:264\u001b[0m, in \u001b[0;36msdpa_dense\u001b[0;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;129m@flex_attention\u001b[39m\u001b[38;5;241m.\u001b[39mpy_impl(DispatchKey\u001b[38;5;241m.\u001b[39mCompositeExplicitAutograd)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msdpa_dense\u001b[39m(\n\u001b[1;32m    254\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m    263\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 264\u001b[0m     out, lse \u001b[38;5;241m=\u001b[39m math_attention(\n\u001b[1;32m    265\u001b[0m         query,\n\u001b[1;32m    266\u001b[0m         key,\n\u001b[1;32m    267\u001b[0m         value,\n\u001b[1;32m    268\u001b[0m         score_mod,\n\u001b[1;32m    269\u001b[0m         block_mask,\n\u001b[1;32m    270\u001b[0m         scale,\n\u001b[1;32m    271\u001b[0m         kernel_options,\n\u001b[1;32m    272\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m    273\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    275\u001b[0m     out \u001b[38;5;241m=\u001b[39m _permute_strides(out, query\u001b[38;5;241m.\u001b[39mstride())\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, lse\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:230\u001b[0m, in \u001b[0;36mmath_attention\u001b[0;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m    227\u001b[0m key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mexpand((Bq, \u001b[38;5;241m*\u001b[39mkey\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m:]))\n\u001b[1;32m    228\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mexpand((Bq, \u001b[38;5;241m*\u001b[39mvalue\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m:]))\n\u001b[0;32m--> 230\u001b[0m _, post_mod_scores \u001b[38;5;241m=\u001b[39m _math_attention_inner(\n\u001b[1;32m    231\u001b[0m     query,\n\u001b[1;32m    232\u001b[0m     key,\n\u001b[1;32m    233\u001b[0m     value,\n\u001b[1;32m    234\u001b[0m     score_mod,\n\u001b[1;32m    235\u001b[0m     block_mask,\n\u001b[1;32m    236\u001b[0m     scale,\n\u001b[1;32m    237\u001b[0m     kernel_options,\n\u001b[1;32m    238\u001b[0m     score_mod_other_buffers,\n\u001b[1;32m    239\u001b[0m     mask_mod_other_buffers,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# Set fully masked rows' sumexp to 0.0\u001b[39;00m\n\u001b[1;32m    243\u001b[0m logsumexp \u001b[38;5;241m=\u001b[39m post_mod_scores\u001b[38;5;241m.\u001b[39mlogsumexp(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:186\u001b[0m, in \u001b[0;36m_math_attention_inner\u001b[0;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TransformGetItemToIndex():\n\u001b[1;32m    184\u001b[0m     scores \u001b[38;5;241m=\u001b[39m (scores \u001b[38;5;241m*\u001b[39m scale)\u001b[38;5;241m.\u001b[39mto(working_precision)\n\u001b[1;32m    185\u001b[0m     post_mod_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(\n\u001b[0;32m--> 186\u001b[0m         mask_mod(b, h, m, n, \u001b[38;5;241m*\u001b[39mmask_mod_other_buffers),\n\u001b[1;32m    187\u001b[0m         score_mod(scores, b, h, m, n, \u001b[38;5;241m*\u001b[39mscore_mod_other_buffers),\n\u001b[1;32m    188\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mworking_precision, device\u001b[38;5;241m=\u001b[39mscores\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores, post_mod_scores\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmap_impl(\n\u001b[1;32m    204\u001b[0m         func, in_dims, out_dims, randomness, chunk_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    205\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    332\u001b[0m     func,\n\u001b[1;32m    333\u001b[0m     batch_size,\n\u001b[1;32m    334\u001b[0m     flat_in_dims,\n\u001b[1;32m    335\u001b[0m     flat_args,\n\u001b[1;32m    336\u001b[0m     args_spec,\n\u001b[1;32m    337\u001b[0m     out_dims,\n\u001b[1;32m    338\u001b[0m     randomness,\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    340\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mbatched_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmap_impl(\n\u001b[1;32m    204\u001b[0m         func, in_dims, out_dims, randomness, chunk_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    205\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    332\u001b[0m     func,\n\u001b[1;32m    333\u001b[0m     batch_size,\n\u001b[1;32m    334\u001b[0m     flat_in_dims,\n\u001b[1;32m    335\u001b[0m     flat_args,\n\u001b[1;32m    336\u001b[0m     args_spec,\n\u001b[1;32m    337\u001b[0m     out_dims,\n\u001b[1;32m    338\u001b[0m     randomness,\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    340\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mbatched_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "    \u001b[0;31m[... skipping similar frames: _flat_vmap at line 479 (1 times), vmap_impl at line 331 (1 times), vmap.<locals>.wrapped at line 203 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmap_impl(\n\u001b[1;32m    204\u001b[0m         func, in_dims, out_dims, randomness, chunk_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    205\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    332\u001b[0m     func,\n\u001b[1;32m    333\u001b[0m     batch_size,\n\u001b[1;32m    334\u001b[0m     flat_in_dims,\n\u001b[1;32m    335\u001b[0m     flat_args,\n\u001b[1;32m    336\u001b[0m     args_spec,\n\u001b[1;32m    337\u001b[0m     out_dims,\n\u001b[1;32m    338\u001b[0m     randomness,\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    340\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mbatched_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/fx/graph_module.py:822\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/fx/graph_module.py:387\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls, obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m<eval_with_key>.17:6\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, child, child_1, child_2, child_3, s4, s5, l_args_4_12_closure_0_cell_contents)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, child : torch\u001b[38;5;241m.\u001b[39mTensor, child_1 : torch\u001b[38;5;241m.\u001b[39mTensor, child_2 : torch\u001b[38;5;241m.\u001b[39mTensor, child_3 : torch\u001b[38;5;241m.\u001b[39mTensor, s4 : torch\u001b[38;5;241m.\u001b[39mSymInt, s5 : torch\u001b[38;5;241m.\u001b[39mSymInt, l_args_4_12_closure_0_cell_contents : torch\u001b[38;5;241m.\u001b[39m_subclasses\u001b[38;5;241m.\u001b[39mfake_tensor\u001b[38;5;241m.\u001b[39mFakeTensor):\n\u001b[1;32m      5\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m child_2 \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m child_3\n\u001b[0;32m----> 6\u001b[0m     getitem \u001b[38;5;241m=\u001b[39m l_args_4_12_closure_0_cell_contents[(child, child_3)];  l_args_4_12_closure_0_cell_contents \u001b[38;5;241m=\u001b[39m child \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     is_higher_level \u001b[38;5;241m=\u001b[39m getitem \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m;  getitem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     sub \u001b[38;5;241m=\u001b[39m child_2 \u001b[38;5;241m-\u001b[39m child_3;  child_2 \u001b[38;5;241m=\u001b[39m child_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py:141\u001b[0m, in \u001b[0;36mTransformGetItemToIndex.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     index_args \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_leaves(args[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m index_args):\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mod_index(args[\u001b[38;5;241m0\u001b[39m], index_args)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/function.py:585\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n\u001b[0;32m--> 585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m custom_function_call(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/autograd_function.py:49\u001b[0m, in \u001b[0;36mCustomFunctionHigherOrderOperator.__call__\u001b[0;34m(self, autograd_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, autograd_function, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# When custom_function_call is done dispatching through functorch,\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# it should just invoke the autograd.Function. This is consistent\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# (because autograd.Function happens before the Python dispatch key)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# and only traces the forward pass.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(autograd_function, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m autograd_function\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:436\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:305\u001b[0m, in \u001b[0;36mHigherOrderOperator.dispatch\u001b[0;34m(self, dispatch_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mFuncTorchDynamicLayerFrontMode:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_functorch(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mPython:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# Keep the following 1:1 with handle_torch_function_no_python_arg_parser\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# in torch/csrc/utils/python_arg_parser.cpp\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     overloaded_args_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/pyfunctorch.py:294\u001b[0m, in \u001b[0;36mdispatch_functorch\u001b[0;34m(op, args, kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# In traditional PyTorch operators, DispatchKey::FuncTorchTensorWrapper's\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# unwrap_dead_tensors fallback handles unwrapping dead tensor wrappers.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# PyDispatcher sidesteps the PyTorch dispatcher when dealing with functorch\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# transforms, so we manually unwrap the dead tensors here.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# This logic won't need to exist when we have mode-only functorch.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m    292\u001b[0m     torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39munwrap_if_dead, (args, kwargs)\n\u001b[1;32m    293\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mprocess(op, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/pyfunctorch.py:130\u001b[0m, in \u001b[0;36mVmapInterpreter.process\u001b[0;34m(self, op, args, kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, op, args, kwargs):\n\u001b[1;32m    129\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mfunctorch_table[TransformType\u001b[38;5;241m.\u001b[39mVmap]\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/autograd_function.py:300\u001b[0m, in \u001b[0;36mcustom_function_call_vmap\u001b[0;34m(interpreter, autograd_function, *operands, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_overriden_vmap_rule(autograd_function):\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;66;03m# TODO: Update link to stable once that's out\u001b[39;00m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/92029\u001b[39;00m\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to vmap over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mautograd_function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit has both generate_vmap_rule=True and an overriden vmap \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m         )\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m custom_function_call_vmap_generate_rule(\n\u001b[1;32m    301\u001b[0m         interpreter, autograd_function, \u001b[38;5;241m*\u001b[39moperands\n\u001b[1;32m    302\u001b[0m     )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_overriden_vmap_rule(autograd_function):\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# TODO: Update link to stable once that's out\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/92029\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to vmap over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mautograd_function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not have vmap support. Please override and implement the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/autograd_function.py:376\u001b[0m, in \u001b[0;36mcustom_function_call_vmap_generate_rule\u001b[0;34m(interpreter, autograd_function, *operands)\u001b[0m\n\u001b[1;32m    371\u001b[0m vmapped_function, get_out_dims \u001b[38;5;241m=\u001b[39m vmapify_autograd_function(\n\u001b[1;32m    372\u001b[0m     autograd_function, in_dims, interpreter\u001b[38;5;241m.\u001b[39mbatch_size(), interpreter\u001b[38;5;241m.\u001b[39mrandomness()\n\u001b[1;32m    373\u001b[0m )\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m--> 376\u001b[0m     output \u001b[38;5;241m=\u001b[39m custom_function_call(vmapped_function, \u001b[38;5;241m*\u001b[39munwrapped_operands)\n\u001b[1;32m    378\u001b[0m out_dims \u001b[38;5;241m=\u001b[39m get_out_dims()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_batched(output, out_dims, interpreter\u001b[38;5;241m.\u001b[39mlevel())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/autograd_function.py:49\u001b[0m, in \u001b[0;36mCustomFunctionHigherOrderOperator.__call__\u001b[0;34m(self, autograd_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, autograd_function, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# When custom_function_call is done dispatching through functorch,\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# it should just invoke the autograd.Function. This is consistent\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# (because autograd.Function happens before the Python dispatch key)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# and only traces the forward pass.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(autograd_function, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m autograd_function\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:436\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:305\u001b[0m, in \u001b[0;36mHigherOrderOperator.dispatch\u001b[0;34m(self, dispatch_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mFuncTorchDynamicLayerFrontMode:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_functorch(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mPython:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# Keep the following 1:1 with handle_torch_function_no_python_arg_parser\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# in torch/csrc/utils/python_arg_parser.cpp\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     overloaded_args_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/pyfunctorch.py:294\u001b[0m, in \u001b[0;36mdispatch_functorch\u001b[0;34m(op, args, kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# In traditional PyTorch operators, DispatchKey::FuncTorchTensorWrapper's\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# unwrap_dead_tensors fallback handles unwrapping dead tensor wrappers.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# PyDispatcher sidesteps the PyTorch dispatcher when dealing with functorch\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# transforms, so we manually unwrap the dead tensors here.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# This logic won't need to exist when we have mode-only functorch.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m    292\u001b[0m     torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39munwrap_if_dead, (args, kwargs)\n\u001b[1;32m    293\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mprocess(op, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/pyfunctorch.py:130\u001b[0m, in \u001b[0;36mVmapInterpreter.process\u001b[0;34m(self, op, args, kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, op, args, kwargs):\n\u001b[1;32m    129\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mfunctorch_table[TransformType\u001b[38;5;241m.\u001b[39mVmap]\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/autograd_function.py:300\u001b[0m, in \u001b[0;36mcustom_function_call_vmap\u001b[0;34m(interpreter, autograd_function, *operands, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_overriden_vmap_rule(autograd_function):\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;66;03m# TODO: Update link to stable once that's out\u001b[39;00m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/92029\u001b[39;00m\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to vmap over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mautograd_function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit has both generate_vmap_rule=True and an overriden vmap \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m         )\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m custom_function_call_vmap_generate_rule(\n\u001b[1;32m    301\u001b[0m         interpreter, autograd_function, \u001b[38;5;241m*\u001b[39moperands\n\u001b[1;32m    302\u001b[0m     )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_overriden_vmap_rule(autograd_function):\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# TODO: Update link to stable once that's out\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/92029\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to vmap over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mautograd_function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not have vmap support. Please override and implement the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/autograd_function.py:376\u001b[0m, in \u001b[0;36mcustom_function_call_vmap_generate_rule\u001b[0;34m(interpreter, autograd_function, *operands)\u001b[0m\n\u001b[1;32m    371\u001b[0m vmapped_function, get_out_dims \u001b[38;5;241m=\u001b[39m vmapify_autograd_function(\n\u001b[1;32m    372\u001b[0m     autograd_function, in_dims, interpreter\u001b[38;5;241m.\u001b[39mbatch_size(), interpreter\u001b[38;5;241m.\u001b[39mrandomness()\n\u001b[1;32m    373\u001b[0m )\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m--> 376\u001b[0m     output \u001b[38;5;241m=\u001b[39m custom_function_call(vmapped_function, \u001b[38;5;241m*\u001b[39munwrapped_operands)\n\u001b[1;32m    378\u001b[0m out_dims \u001b[38;5;241m=\u001b[39m get_out_dims()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_batched(output, out_dims, interpreter\u001b[38;5;241m.\u001b[39mlevel())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/autograd_function.py:49\u001b[0m, in \u001b[0;36mCustomFunctionHigherOrderOperator.__call__\u001b[0;34m(self, autograd_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, autograd_function, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# When custom_function_call is done dispatching through functorch,\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# it should just invoke the autograd.Function. This is consistent\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# (because autograd.Function happens before the Python dispatch key)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# and only traces the forward pass.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(autograd_function, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m autograd_function\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:436\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:305\u001b[0m, in \u001b[0;36mHigherOrderOperator.dispatch\u001b[0;34m(self, dispatch_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mFuncTorchDynamicLayerFrontMode:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_functorch(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mPython:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# Keep the following 1:1 with handle_torch_function_no_python_arg_parser\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# in torch/csrc/utils/python_arg_parser.cpp\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     overloaded_args_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/pyfunctorch.py:294\u001b[0m, in \u001b[0;36mdispatch_functorch\u001b[0;34m(op, args, kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# In traditional PyTorch operators, DispatchKey::FuncTorchTensorWrapper's\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# unwrap_dead_tensors fallback handles unwrapping dead tensor wrappers.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# PyDispatcher sidesteps the PyTorch dispatcher when dealing with functorch\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# transforms, so we manually unwrap the dead tensors here.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# This logic won't need to exist when we have mode-only functorch.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m    292\u001b[0m     torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39munwrap_if_dead, (args, kwargs)\n\u001b[1;32m    293\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mprocess(op, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/pyfunctorch.py:130\u001b[0m, in \u001b[0;36mVmapInterpreter.process\u001b[0;34m(self, op, args, kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, op, args, kwargs):\n\u001b[1;32m    129\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mfunctorch_table[TransformType\u001b[38;5;241m.\u001b[39mVmap]\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/autograd_function.py:300\u001b[0m, in \u001b[0;36mcustom_function_call_vmap\u001b[0;34m(interpreter, autograd_function, *operands, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_overriden_vmap_rule(autograd_function):\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;66;03m# TODO: Update link to stable once that's out\u001b[39;00m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/92029\u001b[39;00m\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to vmap over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mautograd_function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit has both generate_vmap_rule=True and an overriden vmap \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m         )\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m custom_function_call_vmap_generate_rule(\n\u001b[1;32m    301\u001b[0m         interpreter, autograd_function, \u001b[38;5;241m*\u001b[39moperands\n\u001b[1;32m    302\u001b[0m     )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_overriden_vmap_rule(autograd_function):\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# TODO: Update link to stable once that's out\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/92029\u001b[39;00m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to vmap over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mautograd_function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not have vmap support. Please override and implement the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/autograd_function.py:376\u001b[0m, in \u001b[0;36mcustom_function_call_vmap_generate_rule\u001b[0;34m(interpreter, autograd_function, *operands)\u001b[0m\n\u001b[1;32m    371\u001b[0m vmapped_function, get_out_dims \u001b[38;5;241m=\u001b[39m vmapify_autograd_function(\n\u001b[1;32m    372\u001b[0m     autograd_function, in_dims, interpreter\u001b[38;5;241m.\u001b[39mbatch_size(), interpreter\u001b[38;5;241m.\u001b[39mrandomness()\n\u001b[1;32m    373\u001b[0m )\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m--> 376\u001b[0m     output \u001b[38;5;241m=\u001b[39m custom_function_call(vmapped_function, \u001b[38;5;241m*\u001b[39munwrapped_operands)\n\u001b[1;32m    378\u001b[0m out_dims \u001b[38;5;241m=\u001b[39m get_out_dims()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_batched(output, out_dims, interpreter\u001b[38;5;241m.\u001b[39mlevel())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_functorch/autograd_function.py:49\u001b[0m, in \u001b[0;36mCustomFunctionHigherOrderOperator.__call__\u001b[0;34m(self, autograd_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, autograd_function, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# When custom_function_call is done dispatching through functorch,\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# it should just invoke the autograd.Function. This is consistent\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# (because autograd.Function happens before the Python dispatch key)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# and only traces the forward pass.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(autograd_function, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m autograd_function\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:421\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kernel, DispatchKey)\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;129m@abc\u001b[39m\u001b[38;5;241m.\u001b[39mabstractmethod\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# Dynamo already traces the body of HigherOrderOp beforehand when it\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;66;03m# so no need to trace into it.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;129m@disable\u001b[39m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.sorl import SearchScheduler, sorl_search, compute_loss, evaluate\n",
    "import torch \n",
    "from dataset.base import get_batch\n",
    "\n",
    "start_step = 0 \n",
    "config = sorl_config\n",
    "\n",
    "optimizer = torch.optim.Adam(gat.parameters(), lr=config.learning_rate)\n",
    "scheduler = SearchScheduler(config, gat.K, curriculum_ratio=0.5)\n",
    "\n",
    "for i in range(config.train_iterations):\n",
    "    # config.temperature = 0.0 if i % 2 == 0 else 1.0\n",
    "    global_step = start_step + i\n",
    "    gat.train() \n",
    "\n",
    "    t_search = scheduler.step()\n",
    "    config.max_t_search = t_search\n",
    "\n",
    "    data = get_batch(train_dataset, config.train_batch_size, config.max_length, gat.level_mask_tokens[0], device=gat.device)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "\n",
    "        search_data, switch_ratio = sorl_search(data, gat, config)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    ppt = gat(search_data[:, :-1].contiguous(), target=search_data[:, 1:].contiguous())\n",
    "\n",
    "    ssl_loss, abs_loss = compute_loss(search_data, gat, ppt)\n",
    "    loss = abs_loss + ssl_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Validation needs to be more rigorous : more samples\n",
    "    gat.eval()\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        _, improve_ppl_train, _, vocab_utilization_rate = evaluate(data, gat, 5, config)\n",
    "\n",
    "    print(f\"Iteration {i+1}/{config.train_iterations} \"\n",
    "                    f\"- loss: {loss.item():.4f}, abs_loss: {abs_loss.item():.4f}, ssl_loss: {ssl_loss.item():.4f}, search_ppl: {improve_ppl_train.item():.4f}, switch_ratio: {switch_ratio:.4f}, vocab_utilization_rate: {vocab_utilization_rate:.4f}, t_search: {t_search}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A). no curriculum | temperature flip |  ppl_improve: 15. | abs loss ~ 0.0\n",
    "# (B). curriculum | temperature flip | ppl_improve: 26. | abs loss ~ 0.0\n",
    "# Conclusion: curriculum helps improve the search ability of the model. \n",
    "\n",
    "# (C). curriculum | no temperature flip, temperature=1.0 | ppl_improve: 0.15 | abs loss: 1.33\n",
    "# Conclusion: temperature flip stabilizes abstraction (we just need more push towards convergence)\n",
    "\n",
    "# (D). curriculum | disallow spike placeholders | ppl_improve: 9.6 | abs_loss: 0.55 | vocab_utilization_rate: 0.22\n",
    "# (E). curriculum | allow spike placeholders | ppl_improve: 13.8 | abs_loss: 0.6 | vocab_utilization_rate: 0.22\n",
    "# Conclusion: it's hard to tell the effect of spike-placeholders here, but vocab_utilization is an issue\n",
    "\n",
    "# (F). curriculum | temperature=0.5 | ppl_improve: 30.6 | abs_loss: ~0.0 | vocab utilization 0.2 (flucturate a bit)\n",
    "# (G). curriculum | temperature=0.75 | ppl_improve: 15.9 | abs_loss: 0.46 | vocab utilization 0.33 ~ 0.44 \n",
    "\n",
    "# Q. what about the perplexity-placeholder? Does it help? \n",
    "# Q. how about memory fading? Can it work? \n",
    "# Q. can we measure 'vocabulary utilization rate'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# compute abstract utilization rate\n",
    "# search_data <=\n",
    "si, ei = gat.vocab_sizes.cumsum(dim=0) # begin_idx, end_idx\n",
    "\n",
    "vocab_utilization_rate = search_data[(search_data >= si) & (search_data < ei)].unique().size(0) / (ei - si).item()\n",
    "\n",
    "\n",
    "vocab_utilization_rate = data[(data >= si) & (data < ei)].unique().size(0) / (ei - si).item()\n",
    "\n",
    "vocab_utilization_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
