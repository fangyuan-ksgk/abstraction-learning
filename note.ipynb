{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{SoRL (GAT)}$\n",
    "1. Group advantage computation \n",
    "2. Surrogate loss computation\n",
    "\n",
    "The key for learning from experience is learning from failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 1085763.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 sequences to dataset/multiplication/2K-123.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataset.arithmetic import ArithmeticDataset\n",
    "\n",
    "\n",
    "dataset = ArithmeticDataset(\n",
    "    min_digit=1,\n",
    "    max_digit=3,\n",
    "    L=2,\n",
    "    K=3,\n",
    "    num_data=2000,\n",
    "    filepath=\"dataset/multiplication/2K-123.bin\"\n",
    ").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn-to-explain (GAT) | one batch ver.\n",
    "\n",
    "# (1). Initialize Data Buffer with trajectory-only data \n",
    "\n",
    "from model import GATConfig, GAT\n",
    "from dataset.arithmetic import ArithmeticDataset\n",
    "\n",
    "from dataclasses import asdict\n",
    "from search import SORLConfig \n",
    "import wandb\n",
    "\n",
    "gat_config = GATConfig(K=3, L=2, n_embd=128, n_head=4, n_layer=4, device=\"cpu\", _compile=False,\n",
    "                       vocab_size_list=[17, 8])\n",
    "gat = GAT(gat_config)\n",
    "\n",
    "\n",
    "config = SORLConfig(gat_config=gat_config, \n",
    "           n_generations=4, temperature=1.0, num_iterations=2, \n",
    "           joint_steps=10, context_length=1024, learning_rate=1e-3,\n",
    "           dataset_name=\"100K-123\", \n",
    "           dataset_path=\"dataset/multiplication/100K-123.bin\",\n",
    "           id_validate_dataset_path=\"dataset/multiplication/2k-123.bin\",\n",
    "           ood_validate_dataset_path=\"dataset/multiplication/2k-123.bin\")\n",
    "\n",
    "# load dataset\n",
    "dataset = ArithmeticDataset.from_file(config.id_validate_dataset_path)\n",
    "# id_val_dataset = ArithmeticDataset.from_file(config.id_validate_dataset_path)\n",
    "\n",
    "\n",
    "# gat.load_checkpoint(\"experiment/nbody/SoRL-GRPO-per-token-alternate-nbody.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing generate_rollout_data ---\n"
     ]
    }
   ],
   "source": [
    "from gat import GATConfig, reGAT\n",
    "from sorl import generate_rollout_data\n",
    "from search import repeat_hseq # Assuming repeat_hseq is in search.py\n",
    "from utils import HierSeq\n",
    "import torch \n",
    "\n",
    "print(\"\\n--- Testing generate_rollout_data ---\")\n",
    "\n",
    "# 1. Setup a dummy model and data\n",
    "config = GATConfig(L=2, K=3, vocab_size_list=[10, 5], device='cpu')\n",
    "model = reGAT(config)\n",
    "model.eval()\n",
    "\n",
    "h_data = [\n",
    "    ([1, 2, 3, 4, 5, 6, 7], []), # L0 tokens, no abstractions\n",
    "    ([1, 2, 3, 4, 5, 6, 7], [])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refactor: What if we put different sample in different batch dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util functions \n",
    "# ------------------------------------------------------------------------\n",
    "def infer_level(indices: torch.Tensor, vocab_sizes: torch.Tensor, pad_token: int):\n",
    "    indices_expanded = indices.unsqueeze(-1)  # [batch_size, seq_len, 1]\n",
    "    levels = (indices_expanded < vocab_sizes.cumsum(dim=0)).int().argmax(dim=-1)\n",
    "\n",
    "    padding_mask = (indices == pad_token)\n",
    "    final_levels = torch.where(padding_mask, -1, levels.long())\n",
    "    return final_levels\n",
    "\n",
    "# this produces 'abstract timestamps'\n",
    "def infer_timestamp(levels: torch.Tensor, K: int, l: int = 1) -> torch.Tensor:\n",
    "    is_level = (levels == l-1).long()  \n",
    "    cumulative_counts = torch.cumsum(is_level, dim=-1)\n",
    "    timestamps = (cumulative_counts - 1) // K\n",
    "    timestamps.clamp_(min=0) # this assings the correct timestamp \n",
    "    return timestamps\n",
    "\n",
    "# Rhythmic insertion mask calculation\n",
    "def infer_rythmic_insertion_mask(levels: torch.Tensor, timestamps: torch.Tensor, K: int, l: int): \n",
    "\n",
    "    within_level_mask = (levels <= l)\n",
    "    timestamps[~within_level_mask] = False \n",
    "\n",
    "    B = timestamps.size(0)\n",
    "    is_end_of_groups = torch.cat([\n",
    "        (timestamps[:, :-1] != timestamps[:, 1:]),\n",
    "        torch.full((B, 1), True, device=timestamps.device)\n",
    "    ], dim=1)\n",
    "\n",
    "    is_valid_elements = [] \n",
    "    for timestamp in timestamps: \n",
    "        group_counts = torch.bincount(timestamp) # count consecutive value group size\n",
    "        is_valid_group = group_counts >= K \n",
    "        is_valid_element = is_valid_group[timestamp] # timestamp starts from 0 makes this valid\n",
    "        is_valid_elements.append(is_valid_element)\n",
    "    is_valid_elements = torch.stack(is_valid_elements, dim=0)\n",
    "\n",
    "    insert_mask = is_end_of_groups & is_valid_elements\n",
    "    return insert_mask # insert after 'True' position suffices\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def insert_tokens(\n",
    "    tokens: torch.Tensor,\n",
    "    insert_masks: torch.Tensor,\n",
    "    placeholder_token: int,\n",
    "    pad_token: int\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    B, S_orig = tokens.shape\n",
    "    device = tokens.device\n",
    "\n",
    "    n_insertions_per_sample = insert_masks.sum(dim=1)\n",
    "    max_n_insertions = n_insertions_per_sample.max().item()\n",
    "\n",
    "    if max_n_insertions == 0:\n",
    "        return tokens\n",
    "\n",
    "    S_new = S_orig + max_n_insertions\n",
    "    \n",
    "    new_tokens = torch.full((B, S_new), pad_token, dtype=tokens.dtype, device=device)\n",
    "\n",
    "    padded_masks = F.pad(insert_masks, (1, 0), value=0)[:, :-1].long()\n",
    "    shifts = torch.cumsum(padded_masks, dim=1)\n",
    "\n",
    "    original_indices_seq = torch.arange(S_orig, device=device).expand(B, -1)\n",
    "    original_dest_indices = original_indices_seq + shifts\n",
    "\n",
    "    new_tokens.scatter_(1, original_dest_indices, tokens)\n",
    "\n",
    "    ph_rows, ph_cols = insert_masks.nonzero(as_tuple=True)\n",
    "    ph_shifts = shifts[ph_rows, ph_cols]\n",
    "    ph_dest_cols = ph_cols + 1 + ph_shifts\n",
    "    new_tokens[ph_rows, ph_dest_cols] = placeholder_token\n",
    "\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just need to record 'level' besides 'token idx'\n",
    "# - in fact, given our per-level vocabulary size parameter, we can just record 'token_idx' as level can be inferred directly from it\n",
    "\n",
    "# convert h_data into idx (batched ver, no flattening at all)\n",
    "\n",
    "# hierarchical data (ordered) -- it mixes abstract & non-abstract tokens\n",
    "data = torch.tensor([[1,2,3,12,4,5,6,7], [1,2,3,5,4,5,6,7]]) \n",
    "\n",
    "# infer level from 'idx' (that optionally contains 'abstract' token)\n",
    "# levels = infer_level_from_idx(idx, model.vocab_sizes)\n",
    "\n",
    "# forward propagation through GAT module\n",
    "# (Option 1. remove the level-embedding part -- does not help in \"search advantage\")\n",
    "# (Option 2. keep the level-embedding)\n",
    "idx = data[:, :-1].contiguous()\n",
    "target = data[:, 1:].contiguous() # .continuous() is important to avoid error for '.view()'\n",
    "\n",
    "# forward propagation : compute perplexity per token (traj & abstract)\n",
    "ppt = model(idx, target)\n",
    "\n",
    "# generate return next_token (1 per sample)\n",
    "next_idx, kv_cache = model.generate(idx, temperature=0.0)\n",
    "next_idx, kv_cache = model.generate(next_idx.unsqueeze(1), temperature=0.0, kv_cache=kv_cache)\n",
    "\n",
    "# denoise return updated idx, variable number of tokens updated per sample\n",
    "levels = infer_level(idx, model.vocab_sizes, model.level_mask_tokens[0])\n",
    "denoise_mask = levels.bool()\n",
    "denoise_mask[1, 0] = True \n",
    "denoise_mask[1, 1] = True \n",
    "\n",
    "# denoise\n",
    "updated_idx = model.denoise(idx, denoise_mask, temperature=0.0) # denoise return an updated idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes = model.vocab_sizes\n",
    "K = model.K \n",
    "tokens = idx\n",
    "l = 1\n",
    "\n",
    "# rythmic placeholder insertion\n",
    "\n",
    "levels = infer_level(tokens, vocab_sizes, model.level_mask_tokens[0])\n",
    "timestamps = infer_timestamp(levels, K, l)\n",
    "\n",
    "insert_masks = infer_rythmic_insertion_mask(levels, timestamps, K, l)\n",
    "\n",
    "# we'd use last token in level-0 as pad token\n",
    "new_tokens = insert_tokens(tokens, insert_masks, model.level_mask_tokens[l], model.level_mask_tokens[0])\n",
    "\n",
    "# Similarly, we can perform perplexity-based placeholder insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100, loss: 4.9127, abs_loss: 2.0794, ssl_loss: 2.8332\n",
      "Iteration 2/100, loss: 4.7362, abs_loss: 1.9370, ssl_loss: 2.7991\n",
      "Iteration 3/100, loss: 4.4892, abs_loss: 1.7341, ssl_loss: 2.7552\n",
      "Iteration 4/100, loss: 4.2146, abs_loss: 1.5094, ssl_loss: 2.7052\n",
      "Iteration 5/100, loss: 3.9717, abs_loss: 1.3026, ssl_loss: 2.6691\n",
      "Iteration 6/100, loss: 3.7408, abs_loss: 1.1191, ssl_loss: 2.6218\n",
      "Iteration 7/100, loss: 3.5362, abs_loss: 0.9555, ssl_loss: 2.5807\n",
      "Iteration 8/100, loss: 3.3784, abs_loss: 0.8085, ssl_loss: 2.5699\n",
      "Iteration 9/100, loss: 3.2184, abs_loss: 0.6780, ssl_loss: 2.5404\n",
      "Iteration 10/100, loss: 3.0805, abs_loss: 0.5644, ssl_loss: 2.5161\n",
      "Iteration 11/100, loss: 2.9727, abs_loss: 0.4672, ssl_loss: 2.5055\n",
      "Iteration 12/100, loss: 2.8609, abs_loss: 0.3852, ssl_loss: 2.4758\n",
      "Iteration 13/100, loss: 2.7815, abs_loss: 0.3173, ssl_loss: 2.4642\n",
      "Iteration 14/100, loss: 2.7241, abs_loss: 0.2617, ssl_loss: 2.4625\n",
      "Iteration 15/100, loss: 2.6535, abs_loss: 0.2164, ssl_loss: 2.4371\n",
      "Iteration 16/100, loss: 2.6034, abs_loss: 0.1799, ssl_loss: 2.4236\n",
      "Iteration 17/100, loss: 2.5715, abs_loss: 0.1504, ssl_loss: 2.4211\n",
      "Iteration 18/100, loss: 2.5845, abs_loss: 0.1267, ssl_loss: 2.4578\n",
      "Iteration 19/100, loss: 2.5641, abs_loss: 0.1076, ssl_loss: 2.4565\n",
      "Iteration 20/100, loss: 2.5013, abs_loss: 0.0923, ssl_loss: 2.4090\n",
      "Iteration 21/100, loss: 2.5002, abs_loss: 0.0800, ssl_loss: 2.4202\n",
      "Iteration 22/100, loss: 2.4817, abs_loss: 0.0702, ssl_loss: 2.4116\n",
      "Iteration 23/100, loss: 2.4443, abs_loss: 0.0625, ssl_loss: 2.3818\n",
      "Iteration 24/100, loss: 2.4127, abs_loss: 0.0571, ssl_loss: 2.3557\n",
      "Iteration 25/100, loss: 2.3909, abs_loss: 0.0548, ssl_loss: 2.3362\n",
      "Iteration 26/100, loss: 2.3698, abs_loss: 0.0580, ssl_loss: 2.3118\n",
      "Iteration 27/100, loss: 2.3406, abs_loss: 0.0682, ssl_loss: 2.2724\n",
      "Iteration 28/100, loss: 2.3350, abs_loss: 0.0759, ssl_loss: 2.2591\n",
      "Iteration 29/100, loss: 2.3110, abs_loss: 0.0749, ssl_loss: 2.2361\n",
      "Iteration 30/100, loss: 2.2971, abs_loss: 0.0671, ssl_loss: 2.2300\n",
      "Iteration 31/100, loss: 2.2696, abs_loss: 0.0614, ssl_loss: 2.2081\n",
      "Iteration 32/100, loss: 2.2340, abs_loss: 0.0585, ssl_loss: 2.1755\n",
      "Iteration 33/100, loss: 2.1928, abs_loss: 0.0580, ssl_loss: 2.1348\n",
      "Iteration 34/100, loss: 2.1545, abs_loss: 0.0579, ssl_loss: 2.0966\n",
      "Iteration 35/100, loss: 2.1315, abs_loss: 0.0521, ssl_loss: 2.0794\n",
      "Iteration 36/100, loss: 2.1179, abs_loss: 0.0470, ssl_loss: 2.0709\n",
      "Iteration 37/100, loss: 2.1086, abs_loss: 0.0387, ssl_loss: 2.0699\n",
      "Iteration 38/100, loss: 2.0646, abs_loss: 0.0330, ssl_loss: 2.0316\n",
      "Iteration 39/100, loss: 2.0454, abs_loss: 0.0351, ssl_loss: 2.0103\n",
      "Iteration 40/100, loss: 2.0231, abs_loss: 0.0392, ssl_loss: 1.9839\n",
      "Iteration 41/100, loss: 1.9984, abs_loss: 0.0415, ssl_loss: 1.9569\n",
      "Iteration 42/100, loss: 1.9670, abs_loss: 0.0415, ssl_loss: 1.9255\n",
      "Iteration 43/100, loss: 1.9091, abs_loss: 0.0348, ssl_loss: 1.8743\n",
      "Iteration 44/100, loss: 1.9043, abs_loss: 0.0348, ssl_loss: 1.8695\n",
      "Iteration 45/100, loss: 1.8752, abs_loss: 0.0376, ssl_loss: 1.8376\n",
      "Iteration 46/100, loss: 1.8536, abs_loss: 0.0394, ssl_loss: 1.8142\n",
      "Iteration 47/100, loss: 1.8112, abs_loss: 0.0357, ssl_loss: 1.7755\n",
      "Iteration 48/100, loss: 1.8255, abs_loss: 0.0337, ssl_loss: 1.7918\n",
      "Iteration 49/100, loss: 1.7972, abs_loss: 0.0298, ssl_loss: 1.7675\n",
      "Iteration 50/100, loss: 1.7900, abs_loss: 0.0246, ssl_loss: 1.7654\n",
      "Iteration 51/100, loss: 1.7748, abs_loss: 0.0204, ssl_loss: 1.7544\n",
      "Iteration 52/100, loss: 1.7417, abs_loss: 0.0211, ssl_loss: 1.7206\n",
      "Iteration 53/100, loss: 1.7457, abs_loss: 0.0234, ssl_loss: 1.7224\n",
      "Iteration 54/100, loss: 1.7216, abs_loss: 0.0219, ssl_loss: 1.6997\n",
      "Iteration 55/100, loss: 1.7277, abs_loss: 0.0199, ssl_loss: 1.7079\n",
      "Iteration 56/100, loss: 1.7184, abs_loss: 0.0228, ssl_loss: 1.6955\n",
      "Iteration 57/100, loss: 1.6990, abs_loss: 0.0195, ssl_loss: 1.6795\n",
      "Iteration 58/100, loss: 1.6785, abs_loss: 0.0156, ssl_loss: 1.6629\n",
      "Iteration 59/100, loss: 1.6704, abs_loss: 0.0141, ssl_loss: 1.6563\n",
      "Iteration 60/100, loss: 1.6587, abs_loss: 0.0138, ssl_loss: 1.6449\n",
      "Iteration 61/100, loss: 1.6630, abs_loss: 0.0148, ssl_loss: 1.6482\n",
      "Iteration 62/100, loss: 1.6686, abs_loss: 0.0155, ssl_loss: 1.6531\n",
      "Iteration 63/100, loss: 1.6346, abs_loss: 0.0158, ssl_loss: 1.6188\n",
      "Iteration 64/100, loss: 1.6457, abs_loss: 0.0138, ssl_loss: 1.6318\n",
      "Iteration 65/100, loss: 1.6293, abs_loss: 0.0131, ssl_loss: 1.6162\n",
      "Iteration 66/100, loss: 1.5948, abs_loss: 0.0114, ssl_loss: 1.5834\n",
      "Iteration 67/100, loss: 1.6046, abs_loss: 0.0102, ssl_loss: 1.5944\n",
      "Iteration 68/100, loss: 1.5687, abs_loss: 0.0100, ssl_loss: 1.5586\n",
      "Iteration 69/100, loss: 1.5944, abs_loss: 0.0107, ssl_loss: 1.5837\n",
      "Iteration 70/100, loss: 1.5895, abs_loss: 0.0114, ssl_loss: 1.5782\n",
      "Iteration 71/100, loss: 1.5754, abs_loss: 0.0119, ssl_loss: 1.5635\n",
      "Iteration 72/100, loss: 1.5757, abs_loss: 0.0116, ssl_loss: 1.5640\n",
      "Iteration 73/100, loss: 1.5537, abs_loss: 0.0101, ssl_loss: 1.5435\n",
      "Iteration 74/100, loss: 1.5492, abs_loss: 0.0109, ssl_loss: 1.5383\n",
      "Iteration 75/100, loss: 1.5599, abs_loss: 0.0116, ssl_loss: 1.5484\n",
      "Iteration 76/100, loss: 1.5571, abs_loss: 0.0126, ssl_loss: 1.5446\n",
      "Iteration 77/100, loss: 1.5394, abs_loss: 0.0131, ssl_loss: 1.5263\n",
      "Iteration 78/100, loss: 1.5390, abs_loss: 0.0121, ssl_loss: 1.5269\n",
      "Iteration 79/100, loss: 1.5456, abs_loss: 0.0104, ssl_loss: 1.5352\n",
      "Iteration 80/100, loss: 1.5116, abs_loss: 0.0097, ssl_loss: 1.5019\n",
      "Iteration 81/100, loss: 1.5463, abs_loss: 0.0092, ssl_loss: 1.5370\n",
      "Iteration 82/100, loss: 1.5069, abs_loss: 0.0087, ssl_loss: 1.4982\n",
      "Iteration 83/100, loss: 1.5160, abs_loss: 0.0092, ssl_loss: 1.5067\n",
      "Iteration 84/100, loss: 1.4968, abs_loss: 0.0086, ssl_loss: 1.4882\n",
      "Iteration 85/100, loss: 1.5397, abs_loss: 0.0079, ssl_loss: 1.5318\n",
      "Iteration 86/100, loss: 1.5272, abs_loss: 0.0076, ssl_loss: 1.5196\n",
      "Iteration 87/100, loss: 1.5037, abs_loss: 0.0073, ssl_loss: 1.4964\n",
      "Iteration 88/100, loss: 1.5219, abs_loss: 0.0069, ssl_loss: 1.5150\n",
      "Iteration 89/100, loss: 1.5130, abs_loss: 0.0067, ssl_loss: 1.5063\n",
      "Iteration 90/100, loss: 1.5185, abs_loss: 0.0068, ssl_loss: 1.5117\n",
      "Iteration 91/100, loss: 1.4859, abs_loss: 0.0068, ssl_loss: 1.4792\n",
      "Iteration 92/100, loss: 1.5157, abs_loss: 0.0069, ssl_loss: 1.5088\n",
      "Iteration 93/100, loss: 1.5358, abs_loss: 0.0065, ssl_loss: 1.5293\n",
      "Iteration 94/100, loss: 1.5017, abs_loss: 0.0062, ssl_loss: 1.4955\n",
      "Iteration 95/100, loss: 1.4980, abs_loss: 0.0061, ssl_loss: 1.4920\n",
      "Iteration 96/100, loss: 1.5126, abs_loss: 0.0059, ssl_loss: 1.5068\n",
      "Iteration 97/100, loss: 1.5113, abs_loss: 0.0057, ssl_loss: 1.5056\n",
      "Iteration 98/100, loss: 1.5152, abs_loss: 0.0055, ssl_loss: 1.5097\n",
      "Iteration 99/100, loss: 1.4803, abs_loss: 0.0054, ssl_loss: 1.4749\n",
      "Iteration 100/100, loss: 1.4921, abs_loss: 0.0055, ssl_loss: 1.4866\n"
     ]
    }
   ],
   "source": [
    "# Record & Save an annotated dataset\n",
    "\n",
    "# from dataset.base import BaseHierDataset\n",
    "from dataset.arithmetic import ArithmeticHierDataset\n",
    "from nil import annotate_abstraction\n",
    "from nil import supervise_gat \n",
    "\n",
    "record_dataset = ArithmeticHierDataset.from_dataset(dataset)\n",
    "\n",
    "# Greedy Abstraction Annotation (Passing knowledge to the next generation)\n",
    "# ------------------------------------------------------------------------\n",
    "record_dataset = annotate_abstraction(record_dataset, gat)\n",
    "\n",
    "\n",
    "# Reset GAT module\n",
    "# -------------------\n",
    "gat = GAT(gat_config)\n",
    "\n",
    "\n",
    "# Weak Supervision (GAT)\n",
    "# ------------------------------------------------------------------------\n",
    "weak_iterations = 100 # require tuning\n",
    "context_length = 1024\n",
    "\n",
    "supervised_gat = supervise_gat(record_dataset, gat, weak_iterations, context_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep it beautifully simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/200, loss: 3.2189, abs_loss: 0.0000, ssl_loss: 3.2189\n",
      "\n",
      "Improve ppl percentage (train): 0.0000\n",
      "per-sample abstraction switch ratio: 0.0000 | t_search: 1 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 2/200, loss: 3.1475, abs_loss: 0.0000, ssl_loss: 3.1475\n",
      "\n",
      "Improve ppl percentage (train): 0.0376\n",
      "per-sample abstraction switch ratio: 0.0000 | t_search: 2 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 3/200, loss: 6.4414, abs_loss: 3.3758, ssl_loss: 3.0656\n",
      "\n",
      "Improve ppl percentage (train): 0.0855\n",
      "per-sample abstraction switch ratio: 0.4000 | t_search: 3 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 4/200, loss: 6.2989, abs_loss: 3.2920, ssl_loss: 3.0069\n",
      "\n",
      "Improve ppl percentage (train): 0.1368\n",
      "per-sample abstraction switch ratio: 0.2857 | t_search: 4 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 5/200, loss: 6.1441, abs_loss: 3.1884, ssl_loss: 2.9557\n",
      "\n",
      "Improve ppl percentage (train): 0.1847\n",
      "per-sample abstraction switch ratio: 0.0769 | t_search: 5 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 6/200, loss: 6.0763, abs_loss: 3.1510, ssl_loss: 2.9252\n",
      "\n",
      "Improve ppl percentage (train): 0.2377\n",
      "per-sample abstraction switch ratio: 0.3077 | t_search: 6 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 7/200, loss: 5.9064, abs_loss: 3.0279, ssl_loss: 2.8785\n",
      "\n",
      "Improve ppl percentage (train): 0.1850\n",
      "per-sample abstraction switch ratio: 0.4074 | t_search: 7 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 8/200, loss: 5.7194, abs_loss: 2.8910, ssl_loss: 2.8283\n",
      "\n",
      "Improve ppl percentage (train): 0.2204\n",
      "per-sample abstraction switch ratio: 0.2593 | t_search: 8 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 9/200, loss: 5.6300, abs_loss: 2.8148, ssl_loss: 2.8151\n",
      "\n",
      "Improve ppl percentage (train): 0.1675\n",
      "per-sample abstraction switch ratio: 0.3462 | t_search: 9 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 10/200, loss: 5.5702, abs_loss: 2.7745, ssl_loss: 2.7957\n",
      "\n",
      "Improve ppl percentage (train): 0.1649\n",
      "per-sample abstraction switch ratio: 0.4800 | t_search: 10 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 11/200, loss: 5.4499, abs_loss: 2.6736, ssl_loss: 2.7763\n",
      "\n",
      "Improve ppl percentage (train): 0.1193\n",
      "per-sample abstraction switch ratio: 0.4444 | t_search: 11 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 12/200, loss: 5.4817, abs_loss: 2.7093, ssl_loss: 2.7725\n",
      "\n",
      "Improve ppl percentage (train): 0.1402\n",
      "per-sample abstraction switch ratio: 0.6154 | t_search: 12 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 13/200, loss: 5.2844, abs_loss: 2.5247, ssl_loss: 2.7597\n",
      "\n",
      "Improve ppl percentage (train): 0.1271\n",
      "per-sample abstraction switch ratio: 0.4815 | t_search: 13 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 14/200, loss: 5.0293, abs_loss: 2.2947, ssl_loss: 2.7346\n",
      "\n",
      "Improve ppl percentage (train): 0.4029\n",
      "per-sample abstraction switch ratio: 0.4074 | t_search: 14 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 15/200, loss: 5.1436, abs_loss: 2.3688, ssl_loss: 2.7748\n",
      "\n",
      "Improve ppl percentage (train): 0.3120\n",
      "per-sample abstraction switch ratio: 0.5926 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 16/200, loss: 4.8908, abs_loss: 2.1025, ssl_loss: 2.7883\n",
      "\n",
      "Improve ppl percentage (train): 2.9310\n",
      "per-sample abstraction switch ratio: 0.5000 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 17/200, loss: 4.5082, abs_loss: 1.7637, ssl_loss: 2.7445\n",
      "\n",
      "Improve ppl percentage (train): 3.0696\n",
      "per-sample abstraction switch ratio: 0.0741 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 18/200, loss: 4.4024, abs_loss: 1.6435, ssl_loss: 2.7589\n",
      "\n",
      "Improve ppl percentage (train): 0.6573\n",
      "per-sample abstraction switch ratio: 0.1154 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 19/200, loss: 4.5818, abs_loss: 1.7893, ssl_loss: 2.7925\n",
      "\n",
      "Improve ppl percentage (train): 0.0353\n",
      "per-sample abstraction switch ratio: 0.6154 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 20/200, loss: 4.2757, abs_loss: 1.5195, ssl_loss: 2.7562\n",
      "\n",
      "Improve ppl percentage (train): 3.3827\n",
      "per-sample abstraction switch ratio: 0.4074 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 21/200, loss: 3.9752, abs_loss: 1.2379, ssl_loss: 2.7373\n",
      "\n",
      "Improve ppl percentage (train): 5.6334\n",
      "per-sample abstraction switch ratio: 0.0769 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 22/200, loss: 3.9768, abs_loss: 1.1926, ssl_loss: 2.7842\n",
      "\n",
      "Improve ppl percentage (train): 5.5106\n",
      "per-sample abstraction switch ratio: 0.1200 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 23/200, loss: 3.7886, abs_loss: 1.0734, ssl_loss: 2.7152\n",
      "\n",
      "Improve ppl percentage (train): 6.3197\n",
      "per-sample abstraction switch ratio: 0.1111 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 24/200, loss: 3.7240, abs_loss: 1.0617, ssl_loss: 2.6623\n",
      "\n",
      "Improve ppl percentage (train): 8.0264\n",
      "per-sample abstraction switch ratio: 0.2222 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 25/200, loss: 3.7466, abs_loss: 1.1051, ssl_loss: 2.6415\n",
      "\n",
      "Improve ppl percentage (train): 8.0475\n",
      "per-sample abstraction switch ratio: 0.3333 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 26/200, loss: 3.6543, abs_loss: 0.9990, ssl_loss: 2.6553\n",
      "\n",
      "Improve ppl percentage (train): 8.6019\n",
      "per-sample abstraction switch ratio: 0.1923 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 27/200, loss: 3.6542, abs_loss: 0.9697, ssl_loss: 2.6845\n",
      "\n",
      "Improve ppl percentage (train): 8.8123\n",
      "per-sample abstraction switch ratio: 0.5600 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 28/200, loss: 3.7301, abs_loss: 1.2339, ssl_loss: 2.4962\n",
      "\n",
      "Improve ppl percentage (train): 11.4021\n",
      "per-sample abstraction switch ratio: 0.6154 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 29/200, loss: 3.4397, abs_loss: 0.9074, ssl_loss: 2.5323\n",
      "\n",
      "Improve ppl percentage (train): 15.2979\n",
      "per-sample abstraction switch ratio: 0.4815 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 30/200, loss: 3.3764, abs_loss: 0.8569, ssl_loss: 2.5195\n",
      "\n",
      "Improve ppl percentage (train): 13.4940\n",
      "per-sample abstraction switch ratio: 0.4615 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 31/200, loss: 3.3457, abs_loss: 1.0181, ssl_loss: 2.3276\n",
      "\n",
      "Improve ppl percentage (train): 16.3470\n",
      "per-sample abstraction switch ratio: 0.4074 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 32/200, loss: 3.1125, abs_loss: 0.6771, ssl_loss: 2.4354\n",
      "\n",
      "Improve ppl percentage (train): 19.3606\n",
      "per-sample abstraction switch ratio: 0.4615 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 33/200, loss: 3.2900, abs_loss: 0.6904, ssl_loss: 2.5996\n",
      "\n",
      "Improve ppl percentage (train): 19.3003\n",
      "per-sample abstraction switch ratio: 0.6400 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 34/200, loss: 3.0573, abs_loss: 0.5656, ssl_loss: 2.4916\n",
      "\n",
      "Improve ppl percentage (train): 18.4063\n",
      "per-sample abstraction switch ratio: 0.4615 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 35/200, loss: 2.9190, abs_loss: 0.5891, ssl_loss: 2.3299\n",
      "\n",
      "Improve ppl percentage (train): 17.5035\n",
      "per-sample abstraction switch ratio: 0.5926 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 36/200, loss: 2.9606, abs_loss: 0.7283, ssl_loss: 2.2323\n",
      "\n",
      "Improve ppl percentage (train): 17.9639\n",
      "per-sample abstraction switch ratio: 0.2963 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 37/200, loss: 2.7871, abs_loss: 0.5623, ssl_loss: 2.2248\n",
      "\n",
      "Improve ppl percentage (train): 21.6784\n",
      "per-sample abstraction switch ratio: 0.4444 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 38/200, loss: 2.6250, abs_loss: 0.3874, ssl_loss: 2.2376\n",
      "\n",
      "Improve ppl percentage (train): 25.2365\n",
      "per-sample abstraction switch ratio: 0.1538 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n",
      "Iteration 39/200, loss: 2.6238, abs_loss: 0.3681, ssl_loss: 2.2557\n",
      "\n",
      "Improve ppl percentage (train): 28.0668\n",
      "per-sample abstraction switch ratio: 0.4815 | t_search: 15 | (How often greedy sample is rejected by other abstraction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[1;32m     30\u001b[0m     repeat_batch, switch_ratio, rollout_advantages \u001b[38;5;241m=\u001b[39m sorl_search_v2(gat, batch_data, n, temperature, t_search) \u001b[38;5;66;03m# pinned greedy sample ver.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m ppt \u001b[38;5;241m=\u001b[39m gat(repeat_batch)\n\u001b[1;32m     34\u001b[0m ssl_loss \u001b[38;5;241m=\u001b[39m compute_ssl_loss(repeat_batch, ppt)\n\u001b[1;32m     35\u001b[0m abs_loss \u001b[38;5;241m=\u001b[39m compute_abs_ssl_loss(repeat_batch, ppt, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Implementation/abstraction-learning/model.py:259\u001b[0m, in \u001b[0;36mGAT.forward\u001b[0;34m(self, batch_data)\u001b[0m\n\u001b[1;32m    256\u001b[0m v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m--> 259\u001b[0m     x, v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh[i](x, v1, x0, block_mask)\n\u001b[1;32m    261\u001b[0m x \u001b[38;5;241m=\u001b[39m norm(x)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_ppt(x, batch_data)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Implementation/abstraction-learning/model.py:133\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, v1, x0, block_mask)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, v1, x0, block_mask):\n\u001b[1;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdas[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambdas[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m x0\n\u001b[0;32m--> 133\u001b[0m     x1, v1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(norm(x), v1, block_mask)\n\u001b[1;32m    134\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x1\n\u001b[1;32m    135\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(norm(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Implementation/abstraction-learning/model.py:97\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x, v1, block_mask)\u001b[0m\n\u001b[1;32m     95\u001b[0m q, k \u001b[38;5;241m=\u001b[39m norm(q), norm(k) \u001b[38;5;66;03m# QK norm suggested by @Grad62304977\u001b[39;00m\n\u001b[1;32m     96\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary(q), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary(k)        \n\u001b[0;32m---> 97\u001b[0m y \u001b[38;5;241m=\u001b[39m flex_attention(\n\u001b[1;32m     98\u001b[0m     q\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     99\u001b[0m     k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    100\u001b[0m     v\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m    101\u001b[0m     block_mask\u001b[38;5;241m=\u001b[39mblock_mask,\n\u001b[1;32m    102\u001b[0m     kernel_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflex_kernel_options\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    104\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview_as(x)       \n\u001b[1;32m    105\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y)       \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/attention/flex_attention.py:1353\u001b[0m, in \u001b[0;36mflex_attention\u001b[0;34m(query, key, value, score_mod, block_mask, scale, enable_gqa, return_lse, kernel_options)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1352\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1353\u001b[0m out, lse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1354\u001b[0m     _flex_attention_hop_wrapper, backend\u001b[38;5;241m=\u001b[39mbackend, fullgraph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m )(\n\u001b[1;32m   1356\u001b[0m     query,\n\u001b[1;32m   1357\u001b[0m     key,\n\u001b[1;32m   1358\u001b[0m     value,\n\u001b[1;32m   1359\u001b[0m     score_mod,\n\u001b[1;32m   1360\u001b[0m     block_mask\u001b[38;5;241m.\u001b[39mas_tuple(),  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1361\u001b[0m     scale,\n\u001b[1;32m   1362\u001b[0m     kernel_options,\n\u001b[1;32m   1363\u001b[0m )\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_lse:\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, lse \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:574\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    570\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    578\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    579\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/attention/flex_attention.py:1340\u001b[0m, in \u001b[0;36mflex_attention.<locals>._flex_attention_hop_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebugging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   1335\u001b[0m     make_eager_backend_with_torch_function_mode,\n\u001b[1;32m   1336\u001b[0m )\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;66;03m# Dynamo is expecting a callable with \"__code__\" attribute.\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;66;03m# We cannot directly pass hop to it. So we wrap it in a dummy function.\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flex_attention_hop_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flex_attention_hop(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_compilation_env():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m<eval_with_key>.31:21\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, s1, L_args_0_, s3, L_args_4_12_closure_0_cell_contents, s5, L_args_1_, s7, L_args_2_, L_args_4_0_, L_args_4_1_, s10, L_args_4_2_, s11, s12, L_args_4_3_, s13, L_args_4_4_, s14, s15, L_args_4_5_, s16, L_args_4_6_, s17, s18, L_args_4_7_, s19, L_args_4_8_, s20, s21, L_args_4_9_)\u001b[0m\n\u001b[1;32m     19\u001b[0m score_mod_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_mod_0\n\u001b[1;32m     20\u001b[0m mask_fn_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_fn_0\n\u001b[0;32m---> 21\u001b[0m flex_attention \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mhigher_order\u001b[38;5;241m.\u001b[39mflex_attention(l_args_0_, l_args_1_, l_args_2_, score_mod_0, (l_args_4_0_, l_args_4_1_, l_args_4_2_, l_args_4_3_, l_args_4_4_, l_args_4_5_, l_args_4_6_, l_args_4_7_, l_args_4_8_, l_args_4_9_, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, mask_fn_0), \u001b[38;5;241m0.17677669529663687\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPRESCALE_QK\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROWS_GUARANTEED_SAFE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOCKS_ARE_CONTIGUOUS\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOUTPUT_LOGSUMEXP\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}, (), (s3, l_args_4_12_closure_0_cell_contents));  l_args_0_ \u001b[38;5;241m=\u001b[39m l_args_1_ \u001b[38;5;241m=\u001b[39m l_args_2_ \u001b[38;5;241m=\u001b[39m score_mod_0 \u001b[38;5;241m=\u001b[39m l_args_4_0_ \u001b[38;5;241m=\u001b[39m l_args_4_1_ \u001b[38;5;241m=\u001b[39m l_args_4_2_ \u001b[38;5;241m=\u001b[39m l_args_4_3_ \u001b[38;5;241m=\u001b[39m l_args_4_4_ \u001b[38;5;241m=\u001b[39m l_args_4_5_ \u001b[38;5;241m=\u001b[39m l_args_4_6_ \u001b[38;5;241m=\u001b[39m l_args_4_7_ \u001b[38;5;241m=\u001b[39m l_args_4_8_ \u001b[38;5;241m=\u001b[39m l_args_4_9_ \u001b[38;5;241m=\u001b[39m mask_fn_0 \u001b[38;5;241m=\u001b[39m s3 \u001b[38;5;241m=\u001b[39m l_args_4_12_closure_0_cell_contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m getitem \u001b[38;5;241m=\u001b[39m flex_attention[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m getitem_1 \u001b[38;5;241m=\u001b[39m flex_attention[\u001b[38;5;241m1\u001b[39m];  flex_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:90\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[0;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     89\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     91\u001b[0m         query,\n\u001b[1;32m     92\u001b[0m         key,\n\u001b[1;32m     93\u001b[0m         value,\n\u001b[1;32m     94\u001b[0m         score_mod,\n\u001b[1;32m     95\u001b[0m         block_mask,\n\u001b[1;32m     96\u001b[0m         scale,\n\u001b[1;32m     97\u001b[0m         kernel_options,\n\u001b[1;32m     98\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m     99\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:436\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:302\u001b[0m, in \u001b[0;36mHigherOrderOperator.dispatch\u001b[0;34m(self, dispatch_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache[dispatch_key]\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kernel, DispatchKey)\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mFuncTorchDynamicLayerFrontMode:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_functorch(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:731\u001b[0m, in \u001b[0;36mflex_attention_autograd\u001b[0;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m         fw_graph, bw_graph \u001b[38;5;241m=\u001b[39m score_mod, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 731\u001b[0m     out, logsumexp \u001b[38;5;241m=\u001b[39m FlexAttentionAutogradOp\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    732\u001b[0m         query,\n\u001b[1;32m    733\u001b[0m         key,\n\u001b[1;32m    734\u001b[0m         value,\n\u001b[1;32m    735\u001b[0m         fw_graph,\n\u001b[1;32m    736\u001b[0m         bw_graph,\n\u001b[1;32m    737\u001b[0m         block_mask,\n\u001b[1;32m    738\u001b[0m         scale,\n\u001b[1;32m    739\u001b[0m         kernel_options,\n\u001b[1;32m    740\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;241m*\u001b[39mscore_mod_other_buffers,\n\u001b[1;32m    742\u001b[0m     )\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, logsumexp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:597\u001b[0m, in \u001b[0;36mFlexAttentionAutogradOp.forward\u001b[0;34m(ctx, query, key, value, fw_graph, joint_graph, block_mask, scale, kernel_options, mask_mod_other_buffers, *score_mod_other_buffers)\u001b[0m\n\u001b[1;32m    595\u001b[0m ctx\u001b[38;5;241m.\u001b[39m_score_mod_other_buffers_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(score_mod_other_buffers)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_AutoDispatchBelowAutograd():\n\u001b[0;32m--> 597\u001b[0m     out, logsumexp \u001b[38;5;241m=\u001b[39m flex_attention(\n\u001b[1;32m    598\u001b[0m         query,\n\u001b[1;32m    599\u001b[0m         key,\n\u001b[1;32m    600\u001b[0m         value,\n\u001b[1;32m    601\u001b[0m         fw_graph,\n\u001b[1;32m    602\u001b[0m         block_mask,\n\u001b[1;32m    603\u001b[0m         scale,\n\u001b[1;32m    604\u001b[0m         kernel_options,\n\u001b[1;32m    605\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m    606\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[1;32m    609\u001b[0m save_tensors_and_symints_for_backward(\n\u001b[1;32m    610\u001b[0m     ctx,\n\u001b[1;32m    611\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m     ),\n\u001b[1;32m    621\u001b[0m )\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, logsumexp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:90\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[0;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     89\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     91\u001b[0m         query,\n\u001b[1;32m     92\u001b[0m         key,\n\u001b[1;32m     93\u001b[0m         value,\n\u001b[1;32m     94\u001b[0m         score_mod,\n\u001b[1;32m     95\u001b[0m         block_mask,\n\u001b[1;32m     96\u001b[0m         scale,\n\u001b[1;32m     97\u001b[0m         kernel_options,\n\u001b[1;32m     98\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m     99\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:431\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    429\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m _to_flat_tuple(args, kwargs)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function(flat_args):\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/overrides.py:1720\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1720\u001b[0m         result \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1722\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/_trace_wrapped_higher_order_op.py:142\u001b[0m, in \u001b[0;36mTransformGetItemToIndex.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m index_args):\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mod_index(args[\u001b[38;5;241m0\u001b[39m], index_args)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:90\u001b[0m, in \u001b[0;36mFlexAttentionHOP.__call__\u001b[0;34m(self, query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m     88\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m     89\u001b[0m     validate_subgraph_args_types(score_mod_other_buffers \u001b[38;5;241m+\u001b[39m mask_mod_other_buffers)\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     91\u001b[0m         query,\n\u001b[1;32m     92\u001b[0m         key,\n\u001b[1;32m     93\u001b[0m         value,\n\u001b[1;32m     94\u001b[0m         score_mod,\n\u001b[1;32m     95\u001b[0m         block_mask,\n\u001b[1;32m     96\u001b[0m         scale,\n\u001b[1;32m     97\u001b[0m         kernel_options,\n\u001b[1;32m     98\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m     99\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:440\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m         dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m     )\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:436\u001b[0m, in \u001b[0;36mHigherOrderOperator.__call__.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;28mself\u001b[39m, flat_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    435\u001b[0m dispatch_key_set \u001b[38;5;241m=\u001b[39m _compute_keyset(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_fallthrough_keys)\n\u001b[0;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[1;32m    437\u001b[0m     dispatch_key_set\u001b[38;5;241m.\u001b[39mhighestPriorityTypeId(), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    438\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_ops.py:302\u001b[0m, in \u001b[0;36mHigherOrderOperator.dispatch\u001b[0;34m(self, dispatch_key, *args, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache[dispatch_key]\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kernel, DispatchKey)\n\u001b[0;32m--> 302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m kernel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;241m==\u001b[39m DispatchKey\u001b[38;5;241m.\u001b[39mFuncTorchDynamicLayerFrontMode:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_functorch(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:264\u001b[0m, in \u001b[0;36msdpa_dense\u001b[0;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;129m@flex_attention\u001b[39m\u001b[38;5;241m.\u001b[39mpy_impl(DispatchKey\u001b[38;5;241m.\u001b[39mCompositeExplicitAutograd)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msdpa_dense\u001b[39m(\n\u001b[1;32m    254\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m     mask_mod_other_buffers: Tuple \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m    263\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 264\u001b[0m     out, lse \u001b[38;5;241m=\u001b[39m math_attention(\n\u001b[1;32m    265\u001b[0m         query,\n\u001b[1;32m    266\u001b[0m         key,\n\u001b[1;32m    267\u001b[0m         value,\n\u001b[1;32m    268\u001b[0m         score_mod,\n\u001b[1;32m    269\u001b[0m         block_mask,\n\u001b[1;32m    270\u001b[0m         scale,\n\u001b[1;32m    271\u001b[0m         kernel_options,\n\u001b[1;32m    272\u001b[0m         score_mod_other_buffers,\n\u001b[1;32m    273\u001b[0m         mask_mod_other_buffers,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    275\u001b[0m     out \u001b[38;5;241m=\u001b[39m _permute_strides(out, query\u001b[38;5;241m.\u001b[39mstride())\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, lse\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_higher_order_ops/flex_attention.py:247\u001b[0m, in \u001b[0;36mmath_attention\u001b[0;34m(query, key, value, score_mod, block_mask, scale, kernel_options, score_mod_other_buffers, mask_mod_other_buffers)\u001b[0m\n\u001b[1;32m    244\u001b[0m masked_rows \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mall(post_mod_scores \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    245\u001b[0m logsumexp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(masked_rows, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m), logsumexp)\n\u001b[0;32m--> 247\u001b[0m post_mod_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_safe_softmax(post_mod_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m post_mod_scores\u001b[38;5;241m.\u001b[39mto(query\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m@\u001b[39m value, logsumexp \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Benchmark RL & SSL combination strategies \n",
    "# (I). Pick the best & learn it \n",
    "# -------------------------------------------------------------\n",
    "import copy \n",
    "import wandb\n",
    "import torch\n",
    "from search import compute_ssl_loss, get_batch, eval_search_improvement\n",
    "from search import compute_abs_ssl_loss\n",
    "from search import sorl_search_v2\n",
    "from search import compute_curriculum_t_increment, eval_ppl_with_search, curriculum_iter\n",
    "\n",
    "n = 3 \n",
    "temperature = 1.0\n",
    "num_iterations = 200\n",
    "context_length = 1024\n",
    "global_step = 0 \n",
    "\n",
    "optimizer = torch.optim.Adam(gat.parameters(), lr=1e-3)\n",
    "gat.train() \n",
    "\n",
    "# curriculum\n",
    "t_search = 0\n",
    "t_delta, t_max = compute_curriculum_t_increment(num_iterations=num_iterations, context_length=context_length, K=gat.K, max_ts=max(dataset.lengths))\n",
    "\n",
    "while global_step < num_iterations: \n",
    "\n",
    "    batch_data = get_batch(dataset.sequences, dataset.lengths, context_length // n, gat.L, gat.K)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        repeat_batch, switch_ratio, rollout_advantages = sorl_search_v2(gat, batch_data, n, temperature, t_search) # pinned greedy sample ver.\n",
    "\n",
    "    ppt = gat(repeat_batch)\n",
    "\n",
    "    ssl_loss = compute_ssl_loss(repeat_batch, ppt)\n",
    "    abs_loss = compute_abs_ssl_loss(repeat_batch, ppt, level=1)\n",
    "\n",
    "    loss = abs_loss + ssl_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Iteration {global_step+1}/{num_iterations}, loss: {loss.item():.4f}, abs_loss: {abs_loss.item():.4f}, ssl_loss: {ssl_loss.item():.4f}\")\n",
    "\n",
    "    global_step += 1\n",
    "    t_search = min(t_search + t_delta, t_max)\n",
    "    del loss, abs_loss, ssl_loss\n",
    "    \n",
    "    # train data ppl improvement\n",
    "    improve_ppl_train = eval_search_improvement(gat, batch_data, t_search=t_search)\n",
    "    print(f\"\\nImprove ppl percentage (train): {improve_ppl_train:.4f}\")\n",
    "    print(f\"per-sample abstraction switch ratio: {switch_ratio:.4f} | t_search: {t_search} | (How often greedy sample is rejected by other abstraction)\")\n",
    "    # s = observe_abstraction(batch_data, gat, t_search=t_search, temperature=0.0)\n",
    "    # print(s)\n",
    "\n",
    "    # if global_step % 10 == 0: \n",
    "    if False: \n",
    "        val_data = get_batch(id_val_dataset.sequences, id_val_dataset.lengths, context_length, gat.L, gat.K)\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            improve_ppl_val = eval_search_improvement(gat, val_data, t_search=t_search)\n",
    "            print(f\"Improve ppl percentage (val): {improve_ppl_val:.4f}\\n\")\n",
    "        \n",
    "            if t_search == t_max:\n",
    "                traj_ppl_val = eval_ppl_with_search(val_data, gat, dataset.answer_token_id, n=6, temperature=1.0)\n",
    "                print(f\"Traj ppl (val): {traj_ppl_val.mean().item():.4f}\\n\")\n",
    "\n",
    "            if not config.t_curriculum: \n",
    "                traj_ppl_val = eval_generate_ppl(gat, val_data, n=1, temperature=0.0, t_search=t_search).mean()\n",
    "                print(f\"Traj ppl (val): {traj_ppl_val.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
