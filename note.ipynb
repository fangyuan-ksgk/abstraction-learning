{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $0^{th}$ level policy model $\\pi^{(0)}(s_{t}|s_{t^{(1)}}^{(1)} \\circ a_{t}^{(1)})$\n",
    "2. $1^{th}$ level policy model $\\pi^{(1)}()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import sandwich_embedding as se \n",
    "import torch \n",
    "from model import GPTConfig\n",
    "\n",
    "config = GPTConfig(vocab_size=50304, n_layer=12, n_head=6, n_embd=768, K=3, L=2, device=\"cpu\", _compile=False)\n",
    "B, S, D = 3, 6, config.n_embd \n",
    "K = config.K\n",
    "L = config.L\n",
    "\n",
    "# Sandwich embedding ensemble (temporal predicted token embeddings ensemble)\n",
    "\n",
    "token_embeddings = torch.randn(B, S, D)\n",
    "high_level_embeddings = torch.randn(B, S//K, D)\n",
    "low_level_embeddings = torch.randn(B, S*K, D)\n",
    "\n",
    "# (I). Embedding Ensemble\n",
    "# v1. pure additive ensemble across abstraction levels\n",
    "se(low_level_embeddings, token_embeddings, high_level_embeddings, K)\n",
    "\n",
    "# (II). Conditional GPT\n",
    "from model import CondGPT, GPTConfig\n",
    "\n",
    "condgpt = CondGPT(config)\n",
    "idx = torch.randint(0, 50304, (B, S))\n",
    "condgpt.forward(idx, high_level_embeddings, low_level_embeddings)\n",
    "condgpt.generate(idx, high_level_embeddings, low_level_embeddings)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = token_embeddings.shape[1]\n",
    "L2 = low_level_embeddings.shape[1]\n",
    "assert seq_len*K <= L2, f\"Planning without grounding is not allowed: {seq_len} > {L2}\"\n",
    "token_embeddings += low_level_embeddings[:, 0:seq_len*K:K]\n",
    "\n",
    "if high_level_embeddings is not None: \n",
    "    L1 = high_level_embeddings.shape[1]\n",
    "    assert L1 * K <= seq_len < (L1 + 1) * K, f\"Execution without purpose or planning without grounding is not allowed: {L1 * K} < {seq_len} <= {(L1 + 1) * K}\"\n",
    "    cond_embeddings = high_level_embeddings.repeat_interleave(K, dim=1)\n",
    "    token_embeddings[:, :L1 * K] += cond_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Better to put a $<begin>$ token at each abstract level, just to avoid forward propagation without token at abstract level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step t string:  0\n",
      "Step t string:  001\n",
      "Step t string:  0010\n",
      "Step t string:  0010012\n",
      "Step t string:  00100120\n",
      "Step t string:  0010012001\n",
      "Step t string:  00100120010\n",
      "Step t string:  001001200100123\n",
      "Step t string:  0010012001001230\n",
      "Step t string:  001001200100123001\n",
      "Step t string:  0010012001001230010\n",
      "Step t string:  0010012001001230010012\n",
      "Step t string:  00100120010012300100120\n",
      "Step t string:  0010012001001230010012001\n",
      "Step t string:  00100120010012300100120010\n",
      "Step t string:  0010012001001230010012001001234\n"
     ]
    }
   ],
   "source": [
    "Lmax = 4 # maxiaml abstraction level\n",
    "K = 2  # abstraction ratio\n",
    "\t\n",
    "\t\n",
    "# Version 2. Focus on 'generating all tokens for current time' t, realisticly speaking, having the model go on forever is not a great idea\n",
    "#            better control at least \"how many steps it'll run\", or at least having it wait for the real world ...\n",
    "\n",
    "def generate_level(l: int, curr: str, t: int): \n",
    "    if l <= Lmax:\n",
    "        curr += str(l)\n",
    "        if t % K == 0: \n",
    "            return generate_level(l+1, curr, t // K)\n",
    "    return curr\n",
    "\t\t\n",
    "# bugs for t=2\n",
    "total_str = \"\"\n",
    "for t in range(1, 2**4+1):\n",
    "\tt_str = generate_level(0, \"\", t)\n",
    "\ttotal_str += t_str\n",
    "\tprint(\"Step t string: \", total_str) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 5], [0], [0], [0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lmax = 4\n",
    "K = 2\n",
    "BOS_TOKEN_ID = 0 \n",
    "\n",
    "def decorate_sequences(idx: list, Lmax: int): \n",
    "\tif not isinstance(idx[0], list): \n",
    "\t\tidx = [idx] + [[BOS_TOKEN_ID] for l in range(1, Lmax)]\n",
    "\telse:\n",
    "\t  assert len(idx) == Lmax, f\"Missing sequence for {Lmax} abstraction levels, currently only got {len(idx)}.\"\n",
    "\t  idx = [seq if (len(seq)>0 and isistance(seq[0], int)) else [BOS_TOKEN_ID] for seq in idx]\n",
    "\treturn idx\n",
    "\n",
    "\n",
    "idx = [3, 4, 5]\n",
    "decorate_sequences(idx, Lmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
